<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-02T00:00:00Z">2025-01-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Cryptography and Security <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detection and classification of DDoS flooding attacks by machine
  learning method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmytro Tymoshchuk, Oleh Yasniy, Mykola Mytnyk, Nataliya Zagorodna, Vitaliy Tymoshchuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on a method for detecting and classifying distributed
denial of service (DDoS) attacks, such as SYN Flooding, ACK Flooding, HTTP
Flooding, and UDP Flooding, using neural networks. Machine learning,
particularly neural networks, is highly effective in detecting malicious
traffic. A dataset containing normal traffic and various DDoS attacks was used
to train a neural network model with a 24-106-5 architecture. The model
achieved high Accuracy (99.35%), Precision (99.32%), Recall (99.54%), and
F-score (0.99) in the classification task. All major attack types were
correctly identified. The model was also further tested in the lab using
virtual infrastructures to generate normal and DDoS traffic. The results showed
that the model can accurately classify attacks under near-real-world
conditions, demonstrating 95.05% accuracy and balanced F-score scores for all
attack types. This confirms that neural networks are an effective tool for
detecting DDoS attacks in modern information security systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper Submitted to BAIT 2024 CEUR-WS, see
  https://ceur-ws.org/Vol-3842/paper11.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Security Attacks on LLM-based Code Completion Tools <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11006v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11006v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Cheng, Ke Sun, Xinyu Zhang, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has significantly
advanced code completion capabilities, giving rise to a new generation of
LLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these
tools possess unique workflows, integrating multiple information sources as
input and prioritizing code suggestions over natural language interaction,
which introduces distinct security challenges. Additionally, LCCTs often rely
on proprietary code datasets for training, raising concerns about the potential
exposure of sensitive data. This paper exploits these distinct characteristics
of LCCTs to develop targeted attack methodologies on two critical security
risks: jailbreaking and training data extraction attacks. Our experimental
results expose significant vulnerabilities within LCCTs, including a 99.4%
success rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate
on Amazon Q. Furthermore, We successfully extracted sensitive user data from
GitHub Copilot, including 54 real email addresses and 314 physical addresses
associated with GitHub usernames. Our study also demonstrates that these
code-based attack methods are effective against general-purpose LLMs, such as
the GPT series, highlighting a broader security misalignment in the handling of
code by modern LLMs. These findings underscore critical security challenges
associated with LCCTs and suggest essential directions for strengthening their
security frameworks. The example code and attack samples from our research are
provided at https://github.com/Sensente/Security-Attacks-on-LCCTs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Hardware Verification with Graph Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raghul Saravanan, Sreenitha Kasarapu, Sai Manoj Pudukotai Dinakarrao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity of modern processor and IP designs presents
significant challenges in identifying and mitigating hardware flaws early in
the IC design cycle. Traditional hardware fuzzing techniques, inspired by
software testing, have shown promise but face scalability issues, especially at
the gate-level netlist where bugs introduced during synthesis are often missed
by RTL-level verification due to longer simulation times.
  To address this, we introduce GraphFuzz, a graph-based hardware fuzzer
designed for gate-level netlist verification. In this approach, hardware
designs are modeled as graph nodes, with gate behaviors encoded as features. By
leveraging graph learning algorithms, GraphFuzz efficiently detects hardware
vulnerabilities by analyzing node patterns. Our evaluation across benchmark
circuits and open-source processors demonstrates an average prediction accuracy
of 80% and bug detection accuracy of 70%, highlighting the potential of
graph-based methods for enhancing hardware verification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Financial Bots on the Ethereum <span class="highlight-title">Blockchain</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Niedermayer, Pietro Saggese, Bernhard Haslhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of bots in Distributed Ledger Technologies (DLTs) fosters
efficiency and automation. However, their use is also associated with predatory
trading and market manipulation, and can pose threats to system integrity. It
is therefore essential to understand the extent of bot deployment in DLTs;
despite this, current detection systems are predominantly rule-based and lack
flexibility. In this study, we present a novel approach that utilizes machine
learning for the detection of financial bots on the Ethereum platform. First,
we systematize existing scientific literature and collect anecdotal evidence to
establish a taxonomy for financial bots, comprising 7 categories and 24
subcategories. Next, we create a ground-truth dataset consisting of 133 human
and 137 bot addresses. Third, we employ both unsupervised and supervised
machine learning algorithms to detect bots deployed on Ethereum. The
highest-performing clustering algorithm is a Gaussian Mixture Model with an
average cluster purity of 82.6%, while the highest-performing model for binary
classification is a Random Forest with an accuracy of 83%. Our machine
learning-based detection mechanism contributes to understanding the Ethereum
ecosystem dynamics by providing additional insights into the current bot
landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Heavy Hitter Analytics with Local Differential Privacy <span class="chip">SIGMOD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuemin Zhang, Qingqing Ye, Haibo Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated heavy hitter analytics enables service providers to better
understand the preferences of cross-party users by analyzing the most frequent
items. As with federated learning, it faces challenges of privacy concerns,
statistical heterogeneity, and expensive communication. Local differential
privacy (LDP), as the de facto standard for privacy-preserving data collection,
solves the privacy challenge by letting each user perturb her data locally and
report the sanitized version. However, in federated settings, applying LDP
complicates the other two challenges, due to the deteriorated utility by the
injected LDP noise or increasing communication/computation costs by
perturbation mechanism. To tackle these problems, we propose a novel
target-aligning prefix tree mechanism satisfying $\epsilon$-LDP, for federated
heavy hitter analytics. In particular, we propose an adaptive extension
strategy to address the inconsistencies between covering necessary prefixes and
estimating heavy hitters within a party to enhance the utility. We also present
a consensus-based pruning strategy that utilizes noisy prior knowledge from
other parties to further align the inconsistency between finding heavy hitters
in each party and providing reasonable frequency information to identify the
global ones. To the best of our knowledge, our study is the first solution to
the federated heavy hitter analytics in a cross-party setting while satisfying
the stringent $\epsilon$-LDP. Comprehensive experiments on both real-world and
synthetic datasets confirm the effectiveness of our proposed mechanism.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGMOD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Region-Guided Attack on the Segment Anything Model (SAM) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoliang Liu, Furao Shen, Jian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) is a cornerstone of image segmentation,
demonstrating exceptional performance across various applications, particularly
in autonomous driving and medical imaging, where precise segmentation is
crucial. However, SAM is vulnerable to adversarial attacks that can
significantly impair its functionality through minor input perturbations.
Traditional techniques, such as FGSM and PGD, are often ineffective in
segmentation tasks due to their reliance on global perturbations that overlook
spatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address
these challenges, but they frequently depend on external cues and do not fully
leverage the structural interdependencies within segmentation processes. This
limitation underscores the need for a novel adversarial strategy that exploits
the unique characteristics of segmentation tasks. In response, we introduce the
Region-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a
Region-Guided Map (RGM) to manipulate segmented regions, enabling targeted
perturbations that fragment large segments and expand smaller ones, resulting
in erroneous outputs from SAM. Our experiments demonstrate that RGA achieves
high success rates in both white-box and black-box scenarios, emphasizing the
need for robust defenses against such sophisticated attacks. RGA not only
reveals SAM's vulnerabilities but also lays the groundwork for developing more
resilient defenses against adversarial threats in image segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contention-Aware Microservice Deployment in Collaborative Mobile Edge
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlei Ge, Yang Li, Xing Zhang, Yukun Sun, Yunji Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an emerging computing paradigm, mobile edge computing (MEC) provides
processing capabilities at the network edge, aiming to reduce latency and
improve user experience. Meanwhile, the advancement of containerization
technology facilitates the deployment of microservice-based applications via
edge node collaboration, ensuring highly efficient service delivery. However,
existing research overlooks the resource contention among microservices in MEC.
This neglect potentially results in inadequate resources for microservices
constituting latency-sensitive applications, leading to increased response time
and ultimately compromising quality of service (QoS). To solve this problem, we
propose the Contention-Aware Multi-Application Microservice Deployment (CAMD)
algorithm for collaborative MEC, balancing rapid response for applications with
low-latency requirements and overall processing efficiency. The CAMD algorithm
decomposes the overall deployment problem into manageable sub-problems, each
focusing on a single microservice, then employs a heuristic approach to
optimize these sub-problems, and ultimately arrives at an optimized deployment
scheme through an iterative process. Finally, the superiority of the proposed
algorithm is evidenced through intensive experiments and comparison with
baseline algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Large Language Model Acceleration based on KV Cache
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19442v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19442v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized a wide range of domains such
as natural language processing, computer vision, and multi-modal tasks due to
their ability to comprehend context and perform logical reasoning. However, the
computational and memory demands of LLMs, particularly during inference, pose
significant challenges when scaling them to real-world, long-context, and
real-time applications. Key-Value (KV) cache management has emerged as a
critical optimization technique for accelerating LLM inference by reducing
redundant computations and improving memory utilization. This survey provides a
comprehensive overview of KV cache management strategies for LLM acceleration,
categorizing them into token-level, model-level, and system-level
optimizations. Token-level strategies include KV cache selection, budget
allocation, merging, quantization, and low-rank decomposition, while
model-level optimizations focus on architectural innovations and attention
mechanisms to enhance KV reuse. System-level approaches address memory
management, scheduling, and hardware-aware designs to improve efficiency across
diverse computing environments. Additionally, the survey provides an overview
of both text and multimodal datasets and benchmarks used to evaluate these
strategies. By presenting detailed taxonomies and comparative analyses, this
work aims to offer useful insights for researchers and practitioners to support
the development of efficient and scalable KV cache management techniques,
contributing to the practical deployment of LLMs in real-world applications.
The curated paper list for KV cache management is in:
\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Performance Analysis for Configurable Software Systems: A
  Case Study from a Fitness Landscape Perspective <span class="chip">ISSTA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16888v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16888v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Huang, Peili Mao, Ke Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern software systems are often highly configurable to tailor varied
requirements from diverse stakeholders. Understanding the mapping between
configurations and the desired performance attributes plays a fundamental role
in advancing the controllability and tuning of the underlying system, yet has
long been a dark hole of knowledge due to its black-box nature. While there
have been previous efforts in performance analysis for these systems, they
analyze the configurations as isolated data points without considering their
inherent spatial relationships. This renders them incapable of interrogating
many important aspects of the configuration space like local optima. In this
work, we advocate a novel perspective to rethink performance analysis --
modeling the configuration space as a structured ``landscape''. To support this
proposition, we designed \our, an open-source, graph data mining empowered
fitness landscape analysis (FLA) framework. By applying this framework to $86$M
benchmarked configurations from $32$ running workloads of $3$ real-world
systems, we arrived at $6$ main findings, which together constitute a holistic
picture of the landscape topography, with thorough discussions about their
implications on both configuration tuning and performance modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures, accepted as a conference paper at ISSTA 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimization of Excitation Waveforms for Maximum Instantaneous Field
  Intensity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Liska, Lukas Jelinek, Miloslav Capek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a computational approach to identify performance
constraints in the time-domain based on optimizing the excitation waveform. The
method builds on an optimization algorithm that has been employed for decades
to establish fundamental limits in the frequency domain and this paper
showcases its first comprehensive application to time-domain pulses. The method
is applied to arbitrarily polarized multiport antennas and arrays. The
demonstration performed is based on finding an antenna's maximum peak radiation
intensity in a given direction and time with limited total input energy
available. To highlight the generality of the approach, an analysis on finding
optimal illumination for antiferromagnetic memory switching is conducted.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 16 figures, IEEE TAP paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Protocol Design for Irregular Repetition Slotted ALOHA With Energy
  Harvesting to Maintain Information Freshness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khac-Hoang Ngo, Diep N. Nguyen, Thai-Mai Dinh Thi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate an internet-of-things system where energy-harvesting devices
send status updates to a common receiver using the irregular repetition slotted
ALOHA (IRSA) protocol. Energy shortages in these devices may lead to
transmission failures that are unknown to the receiver, disrupting the decoding
process. To address this issue, we propose a method for the receiver to
perfectly identify such failures. Furthermore, we optimize the degree
distribution of the protocol to enhance the freshness of the status updates.
Our optimized degree distribution mitigates the adverse effects of potential
transmission failures. Numerical results demonstrate that, despite
energy-harvesting constraints, IRSA can achieve a level of information
freshness comparable to systems with unlimited energy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to IEEE WCNC 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transfer Learning and Double U-Net Empowered Wave Propagation Model in
  Complex Indoor Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Fu, Swagato Mukherjee, Michael T. Lanagan, Prasenjit Mitra, Tarun Chawla, Ram M. Narayanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Machine Learning (ML) network based on transfer learning and transformer
networks is applied to wave propagation models for complex indoor settings.
This network is designed to predict signal propagation in environments with a
variety of objects, effectively simulating the diverse range of furniture
typically found in indoor spaces. We propose Attention U-Net with Efficient
Networks as the backbone, to process images encoded with the essential
information of the indoor environment. The indoor environment is defined by its
fundamental structure, such as the arrangement of walls, windows, and doorways,
alongside varying configurations of furniture placement. An innovative
algorithm is introduced to generate a 3D environment from a 2D floorplan, which
is crucial for efficient collection of data for training. The model is
evaluated by comparing the predicted signal coverage map with ray tracing (RT)
simulations. The prediction results show a root mean square error of less than
6 dB across all tested scenarios, with significant improvements observed when
using a Double U-Net structure compared to a single U-Net model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RIS-Augmented Millimeter-Wave MIMO Systems for Passive Drone Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiguang He, Aymen Fakhreddine, George C. Alexandropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past decade, the number of amateur drones is increasing, and this
trend is expected to continue in the future. The security issues brought by
abuse and misconduct of drones become more and more severe and may incur a
negative impact to the society. In this paper, we leverage existing cellular
multiple-input multiple-output (MIMO) base station (BS) infrastructure,
operating at millimeter wave (mmWave) frequency bands, for drone detection in a
device-free manner with the aid of one reconfigurable intelligent surface
(RIS), deployed in the proximity of the BS. We theoretically examine the
feasibility of drone detection with the aid of the generalized likelihood ratio
test (GLRT) and validate via simulations that, the optimized deployment of an
RIS can bring added benefits compared to RIS-free systems. In addition, the
effect of RIS training beams, training overhead, and radar cross section, is
investigated in order to offer theoretical design guidance for the proposed
cellular RIS-based passive drone detection system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, accepted by IEEE PIMRC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Device-Free 3D Drone Localization in RIS-Assisted mmWave MIMO Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14879v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14879v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiguang He, Charles Vanwynsberghe, Hui Chen, Chongwen Huang, Aymen Fakhreddine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the potential of reconfigurable intelligent
surfaces (RISs) in facilitating passive/device-free three-dimensional (3D)
drone localization within existing cellular infrastructure operating at
millimeter-wave (mmWave) frequencies and employing multiple antennas at the
transceivers. The developed localization system operates in the bi-static mode
without requiring direct communication between the drone and the base station.
We analyze the theoretical performance limits via Fisher information analysis
and Cram\'er Rao lower bounds (CRLBs). Furthermore, we develop a low-complexity
yet effective drone localization algorithm based on coordinate gradient descent
and examine the impact of factors such as radar cross section (RCS) of the
drone and training overhead on system performance. It is demonstrated that
integrating RIS yields significant benefits over its RIS-free counterpart, as
evidenced by both theoretical analyses and numerical simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, accepted by IEEE GLOBECOM 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-01-01T00:00:00Z">2025-01-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Freeze-Tag Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19706v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19706v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharareh Alipour, Kajal Baghestani, Mahdis Mirzaei, Soroush Sahraei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the Freeze-Tag Problem (FTP), introduced by Arkin et al. (SODA'02),
where the objective is to activate a group of n robots, starting from a single
initially active robot. Robots are positioned in $\mathbb{R}^d$, and once
activated, they move at a constant speed to wake up others. The goal is to
minimize the time required to activate the last robot, known as the makespan.
We establish new upper bounds for the makespan under the $l_1$ and $l_2$ norms
in $\mathbb{R}^2$ and $\mathbb{R}^3$. Specifically, we improve the previous
upper bound for $(\mathbb{R}^2, l_2)$ from $7.07r$ (Bonichon et al., DISC'24)
to $5.064r$. For $(\mathbb{R}^3, l_1)$, we derive a makespan bound of $13r$,
which translates to $22.52r$ for $(\mathbb{R}^3, l_2)$. Here, $r$ denotes the
maximum distance of any robot from the initially active robot under the given
norm. To our knowledge, these are the first makespan bounds for FTP in
$\mathbb{R}^3$. Additionally, we show that the maximum makespan for $n$ robots
is not necessarily achieved when robots are equally distributed along the
boundary in $(\mathbb{R}^2, l_2)$. We further investigate FTP in
$(\mathbb{R}^3, l_2)$ for specific configurations where robots lie on a
boundary, providing insights into practical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Cryptography and Security <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Graph Sparsification in the Continual Release Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Epasto, Quanquan C. Liu, Tamalika Mukherjee, Felix Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The graph continual release model of differential privacy seeks to produce
differentially private solutions to graph problems under a stream of edge
updates where new private solutions are released after each update. Thus far,
previously known edge-differentially private algorithms for most graph problems
including densest subgraph and matchings in the continual release setting only
output real-value estimates (not vertex subset solutions) and do not use
sublinear space. Instead, they rely on computing exact graph statistics on the
input [FHO21,SLMVC18]. In this paper, we leverage sparsification to address the
above shortcomings for edge-insertion streams. Our edge-differentially private
algorithms use sublinear space with respect to the number of edges in the graph
while some also achieve sublinear space in the number of vertices in the graph.
In addition, for the densest subgraph problem, we also output
edge-differentially private vertex subset solutions; no previous graph
algorithms in the continual release model output such subsets.
  We make novel use of assorted sparsification techniques from the non-private
streaming and static graph algorithms literature to achieve new results in the
sublinear space, continual release setting. This includes algorithms for
densest subgraph, maximum matching, as well as the first continual release
$k$-core decomposition algorithm. We conclude with polynomial additive error
lower bounds for edge-privacy in the fully dynamic setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SECOMP: Formally Secure Compilation of Compartmentalized C Programs <span class="chip">CCS'24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16277v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16277v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérémy Thibault, Roberto Blanco, Dongjae Lee, Sven Argo, Arthur Azevedo de Amorim, Aïna Linn Georges, Catalin Hritcu, Andrew Tolmach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Undefined behavior in C often causes devastating security vulnerabilities.
One practical mitigation is compartmentalization, which allows developers to
structure large programs into mutually distrustful compartments with clearly
specified privileges and interactions. In this paper we introduce SECOMP, a
compiler for compartmentalized C code that comes with machine-checked proofs
guaranteeing that the scope of undefined behavior is restricted to the
compartments that encounter it and become dynamically compromised. These
guarantees are formalized as the preservation of safety properties against
adversarial contexts, a secure compilation criterion similar to full
abstraction, and this is the first time such a strong criterion is proven for a
mainstream programming language. To achieve this we extend the languages of the
CompCert verified C compiler with isolated compartments that can only interact
via procedure calls and returns, as specified by cross-compartment interfaces.
We adapt the passes and optimizations of CompCert as well as their correctness
proofs to this compartment-aware setting. We then use compiler correctness as
an ingredient in a larger secure compilation proof that involves several proof
engineering novelties, needed to scale formally secure compilation up to a C
compiler.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CCS'24 version, slightly updated and extended with appendices and a
  few more references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modelling the Impact of <span class="highlight-title">Quantum</span> Circuit Imperfections on Networks and
  Computer Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00062v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00062v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Savo Glisic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post Quantum and Quantum Cryptography schemes are feasible quantum computer
applications for 7G networks. These schemes could possibly replace existing
schemes. These algorithms have been compromised by advances in quantum search
algorithms run on quantum computers like Shor algorithm. Shor algorithm is a
quantum algorithm for finding the prime factors of an integer which is the
basis of existing algorithm. This has become an available quantum computer
application putting the use of ESA algorithm at risk. Our recent paper provides
a detailed survey of the work on post quantum and quantum cryptography
algorithms with focus on their applicability in 7G networks.
  Since the paper focuses on the cryptography algorithms as a follow up, in
this paper, we provide a new framework for quantum network optimization and
survey in detail the work on enabling technologies (quantum hardware) for the
practical implementation of these algorithms including the most important
segments of quantum hardware in 7G. As always in engineering practice practical
solutions are a compromise between the performance and complexity of the
implementation. For this reason, as the main contribution, the paper presents a
network and computer applications optimization framework that includes
implementation imperfections. The tools should be useful in optimizing future
generation practical computer system design. After that a comprehensive survey
of the existing work on quantum hardware is presented pointing out the sources
of these imperfections. This enables us to make a fair assessment of how much
investment into quantum hardware improvements contributes to the performance
enhancement of the overall system. In this way a decision can be made on proper
partitioning between the investment in hardware and system level complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Characterization of Semi-Involutory MDS Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tapas Chatterjee, Ayantika Laha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In symmetric cryptography, maximum distance separable (MDS) matrices with
computationally simple inverses have wide applications. Many block ciphers like
AES, SQUARE, SHARK, and hash functions like PHOTON use an MDS matrix in the
diffusion layer. In this article, we first characterize all $3 \times 3$
irreducible semi-involutory matrices over the finite field of characteristic
$2$. Using this matrix characterization, we provide a necessary and sufficient
condition to construct MDS semi-involutory matrices using only their diagonal
entries and the entries of an associated diagonal matrix. Finally, we count the
number of $3 \times 3$ semi-involutory MDS matrices over any finite field of
characteristic $2$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Simultaneous Information and Energy Transmission through <span class="highlight-title">Quantum</span>
  Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13691v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13691v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishal Kumar Das, Lav R. Varshney, Vaibhav Madhok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The optimal rate at which information can be sent through a quantum channel
when the transmitted signal must simultaneously carry some minimum amount of
energy is characterized. To do so, we introduce the quantum-classical analogue
of the capacity-power function and generalize results in classical information
theory for transmitting classical information through noisy channels. We show
that the capacity-power function for a classical-quantum channel, for both
unassisted and private protocol, is concave and also prove additivity for
unentangled and uncorrelated ensembles of input signals for such channels. This
implies we do not need regularized formulas for calculation. We show these
properties also hold for all noiseless channels when we restrict the set of
input states to be pure quantum states. For general channels, we find that the
capacity-power function is piece-wise concave. We give an elegant visual proof
for this supported by numerical simulations. We connect channel capacity and
properties of random quantum states. In particular, we obtain analytical
expressions for the capacity-power function for the case of noiseless channels
using properties of random quantum states under an energy constraint and
concentration phenomena in large Hilbert spaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Network-Based IRS Assisted NLoS DoA Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18306v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18306v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Azhdari, Mahmoud Farhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direction-of-Arrival (DoA) estimation in challenging Non-Line-of-Sight (NLoS)
environments is crucial for various wireless applications. This paper presents
a novel neural network architecture to address this challenge using an
Intelligent Reflecting Surface (IRS). The key innovation is the introduction of
a dedicated, learnable Neural Network (NN)-based IRS layer integrated within a
carefully designed network structure. Unlike conventional neural network
layers, this specialized layer incorporates sinusoidal weight constraints,
where the phase arguments of these sinusoids are learned during training to
directly emulate the phase shifts of the IRS elements. This allows the network
to autonomously optimize the IRS configuration for enhanced DoA estimation,
eliminating the need for separate IRS control algorithms. We adapt standard
backpropagation to accommodate these constraints, ensuring effective training.
Numerical simulations, conducted under various conditions and noise levels,
demonstrate the superior performance of the proposed approach compared to
traditional methods, highlighting its potential for significantly improved DoA
estimation accuracy in complex IRS-assisted wireless systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-31T00:00:00Z">2024-12-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Cryptography and Security <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Tale of Two Imperatives: Privacy and Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Supriya Manna, Niladri Sett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning's preponderance across scientific domains has reshaped
high-stakes decision-making, making it essential to follow rigorous operational
frameworks that include both Right-to-Privacy (RTP) and Right-to-Explanation
(RTE). This paper examines the complexities of combining these two
requirements. For RTP, we focus on `Differential privacy' (DP), which is
considered the current \textit{gold standard} for privacy-preserving machine
learning due to its strong quantitative guarantee of privacy. For RTE, we focus
on post-hoc explainers: they are the \textit{go-to} option for model auditing
as they operate independently of model training. We formally investigate DP
models and various commonly-used post-hoc explainers: how to evaluate these
explainers subject to RTP, and analyze the intrinsic interactions between DP
models and these explainers. Furthermore, our work throws light on how RTP and
RTE can be effectively combined in high-stakes applications. Our study
concludes by outlining an industrial software pipeline, with the example of a
wildly used use-case, that respects both RTP and RTE requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for
  LLMs in Cybersecurity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Jing, Mengyun Tang, Xiaorong Shi, Xing Zheng, Sen Nie, Shi Wu, Yong Yang, Xiapu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Large Language Models (LLMs) is crucial for understanding their
capabilities and limitations across various applications, including natural
language processing and code generation. Existing benchmarks like MMLU, C-Eval,
and HumanEval assess general LLM performance but lack focus on specific expert
domains such as cybersecurity. Previous attempts to create cybersecurity
datasets have faced limitations, including insufficient data volume and a
reliance on multiple-choice questions (MCQs). To address these gaps, we propose
SecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in
the cybersecurity domain. SecBench includes questions in various formats (MCQs
and short-answer questions (SAQs)), at different capability levels (Knowledge
Retention and Logical Reasoning), in multiple languages (Chinese and English),
and across various sub-domains. The dataset was constructed by collecting
high-quality data from open sources and organizing a Cybersecurity Question
Design Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used
the powerful while cost-effective LLMs to (1). label the data and (2).
constructing a grading agent for automatic evaluation of SAQs. Benchmarking
results on 13 SOTA LLMs demonstrate the usability of SecBench, which is
arguably the largest and most comprehensive benchmark dataset for LLMs in
cybersecurity. More information about SecBench can be found at our website, and
the dataset can be accessed via the artifact link.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cool, But What About Oracles? An Oracle-Based Perspective on <span class="highlight-title">Blockchain</span>
  Integration in the Accounting Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20447v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20447v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Caldarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bitcoin Network is a sophisticated accounting system that allows its
underlying cryptocurrency to be trusted even in the absence of a reliable
financial authority. Given its undeniable success, the technology, generally
referred to as blockchain, has also been proposed as a means to improve legacy
accounting systems. Accounting for real-world data, however, requires the
intervention of a third party known as an Oracle, which, having not the same
characteristics as a blockchain, could potentially reduce the expected
integration benefit. Through a systematic review of the literature, this study
aims to investigate whether the papers concerning blockchain integration in
accounting consider and address the limitations posed by oracles. A broad
overview of the limitations that emerged in the literature is provided and
distinguished according to the specific accounting integration. Results support
the view that although research on the subject counts numerous articles, actual
studies considering oracle limitations are lacking. Interestingly, despite the
scarce production of papers addressing oracles in various accounting sectors,
reporting for ESG already shows interesting workarounds for oracle limitations,
with permissioned chains envisioned as a valid support for the safe storage of
sustainability data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is not Proofread. Some tables and figures, as well as
  paragraph content may be subject to change in the journal version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Sands to Mansions: Simulating Full Attack Chain with LLM-Organized
  Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16928v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16928v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingzhi Wang, Zhenyuan Li, Zonghan Guo, Yi Jiang, Kyle Jung, Kedar Thiagarajan, Jiahui Wang, Zhengkai Wang, Emily Wei, Xiangmin Shen, Yan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial dynamics are intrinsic to the nature of offense and defense in
cyberspace, with both attackers and defenders continuously evolving their
technologies. Given the wide array of security products available, users often
face challenges in selecting the most effective solutions. Furthermore,
traditional benchmarks based on single-point attacks are increasingly
inadequate, failing to accurately reflect the full range of attacker
capabilities and falling short in properly evaluating the effectiveness of
defense products. Automated multi-stage attack simulations offer a promising
approach to enhance system evaluation efficiency and aid in analyzing the
effectiveness of detection systems. However, simulating a full attack chain is
complex and requires significant time and expertise from security
professionals, facing several challenges, including limited coverage of attack
techniques, a high level of required expertise, and a lack of execution detail.
In this paper, we model automatic attack simulation as a planning problem. By
using the Planning Domain Definition Language (PDDL) to formally describe the
attack simulation problem, and combining domain knowledge of both the problem
and the domain space, we enable the planning of attack paths through
standardized, domain-independent planning algorithms. We explore the potential
of Large Language Models (LLMs) to summarize and analyze knowledge from
existing attack documentation and reports, facilitating automated attack
planning. We introduce Aurora, a system that autonomously simulates full attack
chains based on external attack tools and threat intelligence reports.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Steganography Beyond the Constraints of Modality <span class="chip">ECCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05496v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05496v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sojeong Song, Seoyun Yang, Chang D. Yoo, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-modal steganography is committed to hiding secret information of one
modality in another modality. Despite the advancement in the field of
steganography by the introduction of deep learning, cross-modal steganography
still remains to be a challenge to the field. The incompatibility between
different modalities not only complicate the hiding process but also results in
increased vulnerability to detection. To rectify these limitations, we present
INRSteg, an innovative cross-modal steganography framework based on Implicit
Neural Representations (INRs). We introduce a novel network allocating
framework with a masked parameter update which facilitates hiding multiple data
and enables cross modality across image, audio, video and 3D shape. Moreover,
we eliminate the necessity of training a deep neural network and therefore
substantially reduce the memory and computational cost and avoid domain
adaptation issues. To the best of our knowledge, in the field of steganography,
this is the first to introduce diverse modalities to both the secret and cover
data. Detailed experiments in extreme modality settings demonstrate the
flexibility, security, and robustness of INRSteg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, Accepted at European Conference on Computer Vision (ECCV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Securing Cloud-Based Internet of Things: Challenges and Mitigations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00356v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00356v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nivedita Singh, Rajkumar Buyya, Hyoungshich Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Internet of Things (IoT) has seen remarkable advancements in recent
years, leading to a paradigm shift in the digital landscape. However, these
technological strides have introduced new challenges, particularly in
cybersecurity. IoT devices, inherently connected to the internet, are
susceptible to various forms of attacks. Moreover, IoT services often handle
sensitive user data, which could be exploited by malicious actors or
unauthorized service providers. As IoT ecosystems expand, the convergence of
traditional and cloud-based systems presents unique security threats in the
absence of uniform regulations. Cloud-based IoT systems, enabled by
Platform-as-a-Service (PaaS) and Infrastructure-as-a-Service (IaaS) models,
offer flexibility and scalability but also pose additional security risks. The
intricate interaction between these systems and traditional IoT devices demands
comprehensive strategies to protect data integrity and user privacy. This paper
highlights the pressing security concerns associated with the widespread
adoption of IoT devices and services. We propose viable solutions to bridge the
existing security gaps while anticipating and preparing for future challenges.
This paper provides a detailed survey of the key security challenges that IoT
services are currently facing. We also suggest proactive strategies to mitigate
these risks, thereby strengthening the overall security of IoT devices and
services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11441v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11441v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuning Han, Bingyin Zhao, Rui Chu, Feng Luo, Biplab Sikdar, Yingjie Lao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show that diffusion models (DMs) are vulnerable to backdoor
attacks. Existing backdoor attacks impose unconcealed triggers (e.g., a gray
box and eyeglasses) that contain evident patterns, rendering remarkable attack
effects yet easy detection upon human inspection and defensive algorithms.
While it is possible to improve stealthiness by reducing the strength of the
backdoor, doing so can significantly compromise its generality and
effectiveness. In this paper, we propose UIBDiffusion, the universal
imperceptible backdoor attack for diffusion models, which allows us to achieve
superior attack and generation performance while evading state-of-the-art
defenses. We propose a novel trigger generation approach based on universal
adversarial perturbations (UAPs) and reveal that such perturbations, which are
initially devised for fooling pre-trained discriminative models, can be adapted
as potent imperceptible backdoor triggers for DMs. We evaluate UIBDiffusion on
multiple types of DMs with different kinds of samplers across various datasets
and targets. Experimental results demonstrate that UIBDiffusion brings three
advantages: 1) Universality, the imperceptible trigger is universal (i.e.,
image and model agnostic) where a single trigger is effective to any images and
all diffusion models with different samplers; 2) Utility, it achieves
comparable generation quality (e.g., FID) and even better attack success rate
(i.e., ASR) at low poison rates compared to the prior works; and 3)
Undetectability, UIBDiffusion is plausible to human perception and can bypass
Elijah and TERD, the SOTA defenses against backdoors for DMs. We will release
our backdoor triggers and code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Quantum</span> One-Time Protection of any Randomized Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03305v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03305v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Gunn, Ramis Movassagh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The meteoric rise in power and popularity of machine learning models
dependent on valuable training data has reignited a basic tension between the
power of running a program locally and the risk of exposing details of that
program to the user. At the same time, fundamental properties of quantum states
offer new solutions to data and program security that can require strikingly
few quantum resources to exploit, and offer advantages outside of mere
computational run time. In this work, we demonstrate such a solution with
quantum one-time tokens.
  A quantum one-time token is a quantum state that permits a certain program to
be evaluated exactly once. One-time security guarantees, roughly, that the
token cannot be used to evaluate the program more than once. We propose a
scheme for building quantum one-time tokens for any randomized classical
program, which include generative AI models. We prove that the scheme satisfies
an interesting definition of one-time security as long as outputs of the
classical algorithm have high enough min-entropy, in a black box model.
  Importantly, the classical program being protected does not need to be
implemented coherently on a quantum computer. In fact, the size and complexity
of the quantum one-time token is independent of the program being protected,
and additional quantum resources serve only to increase the security of the
protocol. Due to this flexibility in adjusting the security, we believe that
our proposal is parsimonious enough to serve as a promising candidate for a
near-term useful demonstration of quantum computing in either the NISQ or early
fault tolerant regime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update: Resolved a bug where we used an insufficiently-strong
  definition of one-time authentication. See the remark on page 4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MADE: Graph Backdoor Defense with Masked Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Lin, Mingjie Li, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have garnered significant attention from
researchers due to their outstanding performance in handling graph-related
tasks, such as social network analysis, protein design, and so on. Despite
their widespread application, recent research has demonstrated that GNNs are
vulnerable to backdoor attacks, implemented by injecting triggers into the
training datasets. Trained on the poisoned data, GNNs will predict target
labels when attaching trigger patterns to inputs. This vulnerability poses
significant security risks for applications of GNNs in sensitive domains, such
as drug discovery. While there has been extensive research into backdoor
defenses for images, strategies to safeguard GNNs against such attacks remain
underdeveloped. Furthermore, we point out that conventional backdoor defense
methods designed for images cannot work well when directly implemented on graph
data. In this paper, we first analyze the key difference between image backdoor
and graph backdoor attacks. Then we tackle the graph defense problem by
presenting a novel approach called MADE, which devises an adversarial mask
generation mechanism that selectively preserves clean sub-graphs and further
leverages masks on edge weights to eliminate the influence of triggers
effectively. Extensive experiments across various graph classification tasks
demonstrate the effectiveness of MADE in significantly reducing the attack
success rate (ASR) while maintaining a high classification accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid <span class="highlight-title">Quantum</span>-Classical Autoencoder Framework for End-to-End
  Communication Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolun Zhang, Gan Zheng, Nguyen Van Huynh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of quantum machine learning to
End-to-End (E2E) communication systems in wireless fading scenarios. We
introduce a novel hybrid quantum-classical autoencoder architecture that
combines parameterized quantum circuits with classical deep neural networks
(DNNs). Specifically, we propose a hybrid quantum-classical autoencoder (QAE)
framework to optimize the E2E communication system. Our results demonstrate the
feasibility of the proposed hybrid system, and reveal that it is the first work
that can achieve comparable block error rate (BLER) performance to classical
DNN-based and conventional channel coding schemes, while significantly reducing
the number of trainable parameters. Additionally, the proposed QAE exhibits
steady and superior BLER convergence over the classical autoencoder baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted in IEEE Wireless Communications Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> The Rendezvous Between Extreme Value Theory and Next-generation Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19438v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19438v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinivas Sagar, Athira Subhash, Chen-Feng Liu, Ahmed Elzanaty, Yazan H. Al-Badarneh, Sheetal Kalyani, Mohamed-Slim Alouini, Mehdi Bennis, <span class="highlight-author">Lajos Hanzo</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Promising technologies such as massive multiple-input and multiple-output,
reconfigurable intelligent reflecting surfaces, non-terrestrial networks,
millimetre wave communication, ultra-reliable lowlatency communication are
envisioned as the enablers for next-generation (NG) networks. In contrast to
conventional communication systems meeting specific average performance
requirements, NG systems are expected to meet quality-of-service requirements
in extreme scenarios as well. In this regard, extreme value theory (EVT)
provides a powerful framework for the design of communication systems. In this
paper, we provide a comprehensive survey of advances in communication that
utilize EVT to characterize the extreme order statistics of interest. We first
give an overview of the history of EVT and then elaborate on the fundamental
theorems. Subsequently, we discuss different problems of interest in NG
communication systems and how EVT can be utilized for their analysis. We
finally point out the open challenges and future directions of EVT in NG
communication systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Radiance Field Delta Video Compression in Edge-Enabled Vehicular
  Metaverse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11857v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11857v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matúš Dopiriak, Eugen Šlapak, Juraj Gazda, Devendra S. Gurjar, Mohammad Abdullah Al Faruque, Marco Levorato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected and autonomous vehicles (CAVs) offload computationally intensive
tasks to multi-access edge computing (MEC) servers via
vehicle-to-infrastructure (V2I) communication, enabling applications within the
vehicular metaverse, which transforms physical environment into the digital
space enabling advanced analysis or predictive modeling. A core challenge is
physical-to-virtual (P2V) synchronization through digital twins (DTs), reliant
on MEC networks and ultra-reliable low-latency communication (URLLC). To
address this, we introduce radiance field (RF) delta video compression (RFDVC),
which uses RF-encoder and RF-decoder architecture using distributed RFs as DTs
storing photorealistic 3D urban scenes in compressed form. This method extracts
differences between CAV-frame capturing actual traffic and RF-frame capturing
empty scene from the same camera pose in batches encoded and transmitted over
the MEC network. Experiments show data savings up to 71% against H.264 codec
and 44% against H.265 codec under different conditions as lighting changes, and
rain. RFDVC also demonstrates resilience to transmission errors, achieving up
to +0.29 structural similarity index measure (SSIM) improvement at block error
rate (BLER) = 0.35 in non-rainy and +0.25 at BLER = 0.2 in rainy conditions,
ensuring superior visual quality compared to standard video coding (VC) methods
across various conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I. We changed the template. II. We removed biography section</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Over-the-Air Fusion of Sparse Spatial Features for Integrated Sensing
  and Edge AI over Broadband Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyan Liu, Qiao Lan, Kaibin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 6G mobile networks feature two new usage scenarios -- distributed sensing
and edge artificial intelligence (AI). Their natural integration, termed
integrated sensing and edge AI (ISEA), promises to create a platform that
enables intelligent environment perception for wide-ranging applications. A
basic operation in ISEA is for a fusion center to acquire and fuse features of
spatial sensing data distributed at many edge devices (known as agents), which
is confronted by a communication bottleneck due to multiple access over hostile
wireless channels. To address this issue, we propose a novel framework, called
Spatial Over-the-Air Fusion (Spatial AirFusion), which exploits radio waveform
superposition to aggregate spatially sparse features over the air and thereby
enables simultaneous access. The framework supports simultaneous aggregation
over multiple voxels, which partition the 3D sensing region, and across
multiple subcarriers. It exploits both spatial feature sparsity with channel
diversity to pair voxel-level aggregation tasks and subcarriers to maximize the
minimum receive signal-to-noise ratio among voxels. Optimally solving the
resultant mixed-integer problem of Voxel-Carrier Pairing and Power Allocation
(VoCa-PPA) is a focus of this work. The proposed approach hinges on derivations
of optimal power allocation as a closed-form function of voxel-carrier pairing
and a useful property of VoCa-PPA that allows dramatic solution space
reduction. Both a low-complexity greedy algorithm and an optimal tree-search
algorithm are then designed for VoCa-PPA. The latter is accelerated with a
customised compact search tree, node pruning and agent ordering. Extensive
simulations using real datasets demonstrate that Spatial AirFusion
significantly reduces computation errors and improves sensing accuracy compared
with conventional over-the-air computation without awareness of spatial
sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Unification of Logic and Information Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10414v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10414v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis A. Lastras, Barry Trager, Jonathan Lenchner, Wojtek Szpankowski, Chai Wah Wu, Mark Squillante, Alex Gray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today, the vast majority of the world's digital information is represented
using the fundamental assumption, introduced by Claude Shannon in 1948, that
``...the semantic aspects of communication are irrelevant to the engineering
problem (of the design of communication systems)...''. Consider, nonetheless,
the observation that we often combine a message with other information in order
to deduce new facts, thereby expanding the value of such a message. It is
noteworthy that to-date, no rigorous theory of communication has been put forth
which postulates the existence of deductive capabilities on the receiver's
side.
  The purpose of this paper is to present such a theory. We formally model such
deductive capabilities using logic reasoning, and present a rigorous theory
which covers the following generic scenario: Alice and Bob each have knowledge
of some logic sentence, and they wish to communicate as efficiently as possible
with the shared goal that, following their communication, Bob should be able to
deduce a particular logic sentence that Alice knows to be true, but that Bob
currently cannot prove. Many variants of this general setup are considered in
this article; in all cases we are able to provide sharp upper and lower bounds.
Our contribution includes the identification of the most fundamental
requirements that we place on a logic and associated logical language for all
of our results to apply. Practical algorithms that are in some cases
asymptotically optimal are provided, and we illustrate the potential practical
value of the design of communication systems that incorporate the assumption of
deductive capabilities at the receiver using experimental results that suggest
significant possible gains compared to classical systems.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid <span class="highlight-title">Quantum</span>-Classical Autoencoder Framework for End-to-End
  Communication Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolun Zhang, Gan Zheng, Nguyen Van Huynh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of quantum machine learning to
End-to-End (E2E) communication systems in wireless fading scenarios. We
introduce a novel hybrid quantum-classical autoencoder architecture that
combines parameterized quantum circuits with classical deep neural networks
(DNNs). Specifically, we propose a hybrid quantum-classical autoencoder (QAE)
framework to optimize the E2E communication system. Our results demonstrate the
feasibility of the proposed hybrid system, and reveal that it is the first work
that can achieve comparable block error rate (BLER) performance to classical
DNN-based and conventional channel coding schemes, while significantly reducing
the number of trainable parameters. Additionally, the proposed QAE exhibits
steady and superior BLER convergence over the classical autoencoder baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted in IEEE Wireless Communications Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FOGNA: An effective Sum-Difference Co-Array Design Based on Fourth-Order
  Cumulants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18122v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18122v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si Wang, Guoqiang Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Array structures based on the fourth-order difference co-array (FODCA)
provide more degrees of freedom (DOF). However, since the growth of DOF is
limited by a single case of fourth-order cumulant in FODCA, this paper aims to
design a sparse linear array (SLA) with higher DOF via exploring different
cases of fourth-order cumulants. This paper presents a mathematical framework
based on fourth-order cumulant to devise a fourth-order extend co-array
(FOECA), which is equivalent to FODCA. A novel SLA, namely fourth-order
generalized nested array (FOGNA), is proposed based on FOECA to provide
closed-form expressions for the sensor locations and enhance DOF to resolve
more signal sources in direction of arrival (DOA) estimation. FOGNA is
consisted of three subarrays, where the first is a concatenated nested array
and the other two subarrays are SLA with big inter-spacing between sensors.
When the total physical sensors of FOGNA are given, the number of sensors in
each subarray is determined by the designed method, which can obtain the
maximum DOF under the proposed array structure and derive closed-form
expressions for the sensor locations of FOGNA. The proposed array structure not
only achieves higher DOF than those of existing FODCAs but also reduces mutual
coupling effects. Numerical simulations are conducted to verify the superiority
of FOGNA on DOA estimation performance and enhanced DOF over other existing
FODCAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 29 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Flow at the Network Edge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12469v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12469v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Shao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) and their multimodal
variants have led to remarkable progress across various domains, demonstrating
impressive capabilities and unprecedented potential. In the era of ubiquitous
connectivity, leveraging communication networks to distribute intelligence is a
transformative concept, envisioning AI-powered services accessible at the
network edge. However, pushing large models from the cloud to
resource-constrained environments faces critical challenges. Model inference on
low-end devices leads to excessive latency and performance bottlenecks, while
raw data transmission over limited bandwidth networks causes high communication
overhead. This article presents AI Flow, a framework that streamlines the
inference process by jointly leveraging the heterogeneous resources available
across devices, edge nodes, and cloud servers, making intelligence flow across
networks. To facilitate cooperation among multiple computational nodes, the
proposed framework explores a paradigm shift in the design of communication
network systems from transmitting information flow to intelligence flow, where
the goal of communications is task-oriented and folded into the inference
process. Experimental results demonstrate the effectiveness of the proposed
framework through an image captioning use case, showcasing the ability to
reduce response latency while maintaining high-quality captions. This article
serves as a position paper for identifying the motivation, challenges, and
principles of AI Flow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calibrating Bayesian Learning via Regularization, Confidence
  Minimization, and Selective Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.11350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.11350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Huang, Sangwoo Park, Osvaldo Simeone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of artificial intelligence (AI) models in fields such as
engineering is limited by the known difficulty of quantifying the reliability
of an AI's decision. A well-calibrated AI model must correctly report its
accuracy on in-distribution (ID) inputs, while also enabling the detection of
out-of-distribution (OOD) inputs. A conventional approach to improve
calibration is the application of Bayesian ensembling. However, owing to
computational limitations and model misspecification, practical ensembling
strategies do not necessarily enhance calibration. This paper proposes an
extension of variational inference (VI)-based Bayesian learning that integrates
calibration regularization for improved ID performance, confidence minimization
for OOD detection, and selective calibration to ensure a synergistic use of
calibration regularization and confidence minimization. The scheme is
constructed successively by first introducing calibration-regularized Bayesian
learning (CBNN), then incorporating out-of-distribution confidence minimization
(OCM) to yield CBNN-OCM, and finally integrating also selective calibration to
produce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs
for which the calibration performance is expected to be insufficient. Numerical
results illustrate the trade-offs between ID accuracy, ID calibration, and OOD
calibration attained by both frequentist and Bayesian learning methods. Among
the main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance
as compared to existing state-of-the-art approaches at the cost of rejecting a
sufficiently large number of inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Radiance Field Delta Video Compression in Edge-Enabled Vehicular
  Metaverse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11857v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11857v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matúš Dopiriak, Eugen Šlapak, Juraj Gazda, Devendra S. Gurjar, Mohammad Abdullah Al Faruque, Marco Levorato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected and autonomous vehicles (CAVs) offload computationally intensive
tasks to multi-access edge computing (MEC) servers via
vehicle-to-infrastructure (V2I) communication, enabling applications within the
vehicular metaverse, which transforms physical environment into the digital
space enabling advanced analysis or predictive modeling. A core challenge is
physical-to-virtual (P2V) synchronization through digital twins (DTs), reliant
on MEC networks and ultra-reliable low-latency communication (URLLC). To
address this, we introduce radiance field (RF) delta video compression (RFDVC),
which uses RF-encoder and RF-decoder architecture using distributed RFs as DTs
storing photorealistic 3D urban scenes in compressed form. This method extracts
differences between CAV-frame capturing actual traffic and RF-frame capturing
empty scene from the same camera pose in batches encoded and transmitted over
the MEC network. Experiments show data savings up to 71% against H.264 codec
and 44% against H.265 codec under different conditions as lighting changes, and
rain. RFDVC also demonstrates resilience to transmission errors, achieving up
to +0.29 structural similarity index measure (SSIM) improvement at block error
rate (BLER) = 0.35 in non-rainy and +0.25 at BLER = 0.2 in rainy conditions,
ensuring superior visual quality compared to standard video coding (VC) methods
across various conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I. We changed the template. II. We removed biography section</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Over-the-Air Fusion of Sparse Spatial Features for Integrated Sensing
  and Edge AI over Broadband Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.17973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.17973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyan Liu, Qiao Lan, Kaibin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The 6G mobile networks feature two new usage scenarios -- distributed sensing
and edge artificial intelligence (AI). Their natural integration, termed
integrated sensing and edge AI (ISEA), promises to create a platform that
enables intelligent environment perception for wide-ranging applications. A
basic operation in ISEA is for a fusion center to acquire and fuse features of
spatial sensing data distributed at many edge devices (known as agents), which
is confronted by a communication bottleneck due to multiple access over hostile
wireless channels. To address this issue, we propose a novel framework, called
Spatial Over-the-Air Fusion (Spatial AirFusion), which exploits radio waveform
superposition to aggregate spatially sparse features over the air and thereby
enables simultaneous access. The framework supports simultaneous aggregation
over multiple voxels, which partition the 3D sensing region, and across
multiple subcarriers. It exploits both spatial feature sparsity with channel
diversity to pair voxel-level aggregation tasks and subcarriers to maximize the
minimum receive signal-to-noise ratio among voxels. Optimally solving the
resultant mixed-integer problem of Voxel-Carrier Pairing and Power Allocation
(VoCa-PPA) is a focus of this work. The proposed approach hinges on derivations
of optimal power allocation as a closed-form function of voxel-carrier pairing
and a useful property of VoCa-PPA that allows dramatic solution space
reduction. Both a low-complexity greedy algorithm and an optimal tree-search
algorithm are then designed for VoCa-PPA. The latter is accelerated with a
customised compact search tree, node pruning and agent ordering. Extensive
simulations using real datasets demonstrate that Spatial AirFusion
significantly reduces computation errors and improves sensing accuracy compared
with conventional over-the-air computation without awareness of spatial
sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Movable Antenna-Aided Secure Full-Duplex Multi-User Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10393v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10393v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingze Ding, Zijian Zhou, Bingli Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate physical layer security (PLS) for full-duplex
(FD) multi-user systems. We consider a base station (BS) that operates in FD
mode and transmits artificial noise (AN) to simultaneously protect uplink (UL)
and downlink (DL) transmissions. Conventional fixed-position antennas (FPAs) at
the FD BS struggle to fully exploit spatial degrees of freedom (DoFs) to
improve signal reception and suppress interference. To overcome this
limitation, we propose a novel FD BS architecture equipped with multiple
transmit and receive movable antennas (MAs). The MAs introduce the DoFs in
antenna position optimization, which can improve the performance of secure
communication systems. To serve users and counter the cooperative interception
of multiple eavesdroppers (Eves), we formulate a sum of secrecy rates (SSR)
maximization problem to jointly optimize the MA positions, the transmit,
receive, and AN beamformers at the BS, and the UL powers. We propose an
alternating optimization (AO) algorithm, which decomposes the original problem
into three sub-problems, to solve the challenging non-convex optimization
problem with highly coupled variables. Specifically, we propose the
multi-velocity particle swarm optimization (MVPSO), which is an improved
version of the standard particle swarm optimization (PSO), to simultaneously
optimize all MA positions. The transmit/AN beamformers and the UL powers are
solved by successive convex approximation (SCA). The optimal receive beamformer
is derived as a closed-form solution. Simulation results demonstrate the
effectiveness of the proposed algorithms and the advantages of MAs over
conventional FPAs in enhancing the security of FD multi-user systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE Transactions on Wireless
  Communications</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Phase transition of the 3-majority opinion dynamics with noisy
  interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco d'Amore, Isabella Ziccardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Communication noise is a common feature in several real-world scenarios where
systems of agents need to communicate in order to pursue some collective task.
In particular, many biologically inspired systems that try to achieve
agreements on some opinion must implement resilient dynamics that are not
strongly affected by noisy communications. In this work, we study the popular
3-Majority dynamics, an opinion dynamics which has been proved to be an
efficient protocol for the majority consensus problem, in which we introduce a
simple feature of uniform communication noise, following (d'Amore et al. 2020).
We prove that in the fully connected communication network of n agents and in
the binary opinion case, the process induced by the 3-Majority dynamics
exhibits a phase transition. For a noise probability $p < 1/3$, the dynamics
reaches in logarithmic time an almost-consensus metastable phase which lasts
for a polynomial number of rounds with high probability. Furthermore, departing
from previous analyses, we further characterize this phase by showing that
there exists an attractive equilibrium value $s_{\text{eq}} \in [n]$ for the
bias of the system, i.e. the difference between the majority community size and
the minority one. Moreover, the agreement opinion turns out to be the initial
majority one if the bias towards it is of magnitude $\Omega(\sqrt{n\log n})$ in
the initial configuration. If, instead, $p > 1/3$, no form of consensus is
possible, and any information regarding the initial majority opinion is lost in
logarithmic time with high probability. Despite more communications per-round
are allowed, the 3-Majority dynamics surprisingly turns out to be less
resilient to noise than the Undecided-State dynamics (d'Amore et al. 2020),
whose noise threshold value is $p = 1/2$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-30T00:00:00Z">2024-12-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Cryptography and Security <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Attack and Defense for LoRa Device Identification and
  Authentication via Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalin E. Sagduyu, Tugba Erpek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LoRa provides long-range, energy-efficient communications in Internet of
Things (IoT) applications that rely on Low-Power Wide-Area Network (LPWAN)
capabilities. Despite these merits, concerns persist regarding the security of
LoRa networks, especially in situations where device identification and
authentication are imperative to secure the reliable access to the LoRa
networks. This paper explores a deep learning (DL) approach to tackle these
concerns, focusing on two critical tasks, namely (i) identifying LoRa devices
and (ii) classifying them to legitimate and rogue devices. Deep neural networks
(DNNs), encompassing both convolutional and feedforward neural networks, are
trained for these tasks using actual LoRa signal data. In this setting, the
adversaries may spoof rogue LoRa signals through the kernel density estimation
(KDE) method based on legitimate device signals that are received by the
adversaries. Two cases are considered, (i) training two separate classifiers,
one for each of the two tasks, and (ii) training a multi-task classifier for
both tasks. The vulnerabilities of the resulting DNNs to manipulations in input
samples are studied in form of untargeted and targeted adversarial attacks
using the Fast Gradient Sign Method (FGSM). Individual and common perturbations
are considered against single-task and multi-task classifiers for the LoRa
signal analysis. To provide resilience against such attacks, a defense approach
is presented by increasing the robustness of classifiers with adversarial
training. Results quantify how vulnerable LoRa signal classification tasks are
to adversarial attacks and emphasize the need to fortify IoT applications
against these subtle yet effective threats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language
  Modeling Exploitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixuan Liu, Toan Tran, Tianhao Wang, Hongsheng Hu, Shuo Wang, Li Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) increasingly depend on web-scraped datasets,
concerns over unauthorized use of copyrighted or personal content for training
have intensified. Despite regulations such as the General Data Protection
Regulation (GDPR), data owners still have limited control over the use of their
content in model training. To address this, we propose ExpShield, a proactive
self-guard mechanism that empowers content owners to embed invisible
perturbations into their text, limiting data misuse in LLMs training without
affecting readability. This preemptive approach enables data owners to protect
sensitive content directly, without relying on a third-party to perform
defense. Starting from the random perturbation, we demonstrate the rationale
for using perturbation to conceal protected content. We further enhance the
efficiency by identifying memorization triggers and creating pitfalls to
diverge the model memorization in a more focused way. To validate our defense's
effectiveness, we propose a novel metric of instance exploitation which
captures the individual risk raised by model training. The experimental results
validate the effectiveness of our approach as the MIA AUC decreases from 0.95
to 0.55, and instance exploitation approaches zero. This suggests that the
individual risk does not increase after training, underscoring the significance
of proactive defenses in protecting copyrighted data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Generalizability of Machine Learning-based Ransomware Detection
  in Block Storage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Reategui, Roman Pletka, Dionysios Diamantopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ransomware represents a pervasive threat, traditionally countered at the
operating system, file-system, or network levels. However, these approaches
often introduce significant overhead and remain susceptible to circumvention by
attackers. Recent research activity started looking into the detection of
ransomware by observing block IO operations. However, this approach exhibits
significant detection challenges. Recognizing these limitations, our research
pivots towards enabling robust ransomware detection in storage systems keeping
in mind their limited computational resources available. To perform our
studies, we propose a kernel-based framework capable of efficiently extracting
and analyzing IO operations to identify ransomware activity. The framework can
be adopted to storage systems using computational storage devices to improve
security and fully hide detection overheads. Our method employs a refined set
of computationally light features optimized for ML models to accurately discern
malicious from benign activities.
  Using this lightweight approach, we study a wide range of generalizability
aspects and analyze the performance of these models across a large space of
setups and configurations covering a wide range of realistic real-world
scenarios. We reveal various trade-offs and provide strong arguments for the
generalizability of storage-based detection of ransomware and show that our
approach outperforms currently available ML-based ransomware detection in
storage. Empirical validation reveals that our decision tree-based models
achieve remarkable effectiveness, evidenced by higher median F1 scores of up to
12.8%, lower false negative rates of up to 10.9% and particularly decreased
false positive rates of up to 17.1% compared to existing storage-based
detection approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward Intelligent and Secure Cloud: Large Language Model Empowered
  Proactive Defense 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyang Zhou, Guang Cheng, Kang Du, Zihan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of cloud computing technologies and the increasing number
of cloud applications have provided a large number of benefits in daily lives.
However, the diversity and complexity of different components pose a
significant challenge to cloud security, especially when dealing with
sophisticated and advanced cyberattacks. Recent advancements in generative
foundation models (GFMs), particularly in the large language models (LLMs),
offer promising solutions for security intelligence. By exploiting the powerful
abilities in language understanding, data analysis, task inference, action
planning, and code generation, we present LLM-PD, a novel proactive defense
architecture that defeats various threats in a proactive manner. LLM-PD can
efficiently make a decision through comprehensive data analysis and sequential
reasoning, as well as dynamically creating and deploying actionable defense
mechanisms on the target cloud. Furthermore, it can flexibly self-evolve based
on experience learned from previous interactions and adapt to new attack
scenarios without additional training. The experimental results demonstrate its
remarkable ability in terms of defense effectiveness and efficiency,
particularly highlighting an outstanding success rate when compared with other
existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages; In submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Location-based Thermal Emission Side-Channel Analysis Using
  Iterative Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tun-Chieh Lou, Chung-Che Wang, Jyh-Shing Roger Jang, Henian Li, Lang Lin, Norman Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes the use of iterative transfer learning applied to deep
learning models for side-channel attacks. Currently, most of the side-channel
attack methods train a model for each individual byte, without considering the
correlation between bytes. However, since the models' parameters for attacking
different bytes may be similar, we can leverage transfer learning, meaning that
we first train the model for one of the key bytes, then use the trained model
as a pretrained model for the remaining bytes. This technique can be applied
iteratively, a process known as iterative transfer learning. Experimental
results show that when using thermal or power consumption map images as input,
and multilayer perceptron or convolutional neural network as the model, our
method improves average performance, especially when the amount of data is
insufficient.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GASLITEing the Retrieval: Exploring Vulnerabilities in Dense
  Embedding-based Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matan Ben-Tov, Mahmood Sharif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense embedding-based text retrieval$\unicode{x2013}$retrieval of relevant
passages from corpora via deep learning encodings$\unicode{x2013}$has emerged
as a powerful method attaining state-of-the-art search results and popularizing
the use of Retrieval Augmented Generation (RAG). Still, like other search
methods, embedding-based retrieval may be susceptible to search-engine
optimization (SEO) attacks, where adversaries promote malicious content by
introducing adversarial passages to corpora. To faithfully assess and gain
insights into the susceptibility of such systems to SEO, this work proposes the
GASLITE attack, a mathematically principled gradient-based search method for
generating adversarial passages without relying on the corpus content or
modifying the model. Notably, GASLITE's passages (1) carry adversary-chosen
information while (2) achieving high retrieval ranking for a selected query
distribution when inserted to corpora. We use GASLITE to extensively evaluate
retrievers' robustness, testing nine advanced models under varied threat
models, while focusing on realistic adversaries targeting queries on a specific
concept (e.g., a public figure). We found GASLITE consistently outperformed
baselines by $\geq$140% success rate, in all settings. Particularly,
adversaries using GASLITE require minimal effort to manipulate search
results$\unicode{x2013}$by injecting a negligible amount of adversarial
passages ($\leq$0.0001% of the corpus), they could make them visible in the
top-10 results for 61-100% of unseen concept-specific queries against most
evaluated models. Inspecting variance in retrievers' robustness, we identify
key factors that may contribute to models' susceptibility to SEO, including
specific properties in the embedding space's geometry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Tale of Two Imperatives: Privacy and Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Supriya Manna, Niladri Sett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning's preponderance across scientific domains has reshaped
high-stakes decision-making, making it essential to follow rigorous operational
frameworks that include both Right-to-Privacy (RTP) and Right-to-Explanation
(RTE). This paper examines the complexities of combining these two
requirements. For RTP, we focus on 'Differentially privacy' (DP), which is
considered the current gold standard for privacy-preserving machine learning
due to its strong quantitative guarantee of privacy. For RTE, we focus on
post-hoc explainers: they are the go-to option for model auditing as they
operate independently of model training. We formally investigate (DP) models
and various commonly-used post-hoc explainers: how to evaluate these explainers
subject to RTP, and analyze the intrinsic interactions between DP models and
these explainers. Furthermore, our work throws light on how RTP and RTE can be
effectively combined in high-stakes applications. Our study concludes by
outlining an industrial software pipeline, with the example of a wildly used
use-case, that respects both RTP and RTE requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for
  LLMs in Cybersecurity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Jing, Mengyun Tang, Xiaorong Shi, Xing Zheng, Sen Nie, Shi Wu, Yong Yang, Xiapu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Large Language Models (LLMs) is crucial for understanding their
capabilities and limitations across various applications, including natural
language processing and code generation. Existing benchmarks like MMLU, C-Eval,
and HumanEval assess general LLM performance but lack focus on specific expert
domains such as cybersecurity. Previous attempts to create cybersecurity
datasets have faced limitations, including insufficient data volume and a
reliance on multiple-choice questions (MCQs). To address these gaps, we propose
SecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in
the cybersecurity domain. SecBench includes questions in various formats (MCQs
and short-answer questions (SAQs)), at different capability levels (Knowledge
Retention and Logical Reasoning), in multiple languages (Chinese and English),
and across various sub-domains. The dataset was constructed by collecting
high-quality data from open sources and organizing a Cybersecurity Question
Design Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used
the powerful while cost-effective LLMs to (1). label the data and (2).
constructing a grading agent for automatic evaluation of SAQs.Benchmarking
results on 13 SOTA LLMs demonstrate the usability of SecBench, which is
arguably the largest and most comprehensive benchmark dataset for LLMs in
cybersecurity. More information about SecBench can be found at our website, and
the dataset can be accessed via the artifact link.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Privacy in Federated Learning through <span class="highlight-title">Quantum</span> Teleportation
  Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koffka Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning enables collaborative model training across multiple
clients without sharing raw data, thereby enhancing privacy. However, the
exchange of model updates can still expose sensitive information. Quantum
teleportation, a process that transfers quantum states between distant
locations without physical transmission of the particles themselves, has
recently been implemented in real-world networks. This position paper explores
the potential of integrating quantum teleportation into federated learning
frameworks to bolster privacy. By leveraging quantum entanglement and the
no-cloning theorem, quantum teleportation ensures that data remains secure
during transmission, as any eavesdropping attempt would be detectable. We
propose a novel architecture where quantum teleportation facilitates the secure
exchange of model parameters and gradients among clients and servers. This
integration aims to mitigate risks associated with data leakage and adversarial
attacks inherent in classical federated learning setups. We also discuss the
practical challenges of implementing such a system, including the current
limitations of quantum network infrastructure and the need for hybrid
quantum-classical protocols. Our analysis suggests that, despite these
challenges, the convergence of quantum communication technologies and federated
learning presents a promising avenue for achieving unprecedented levels of
privacy in distributed machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Similar but Patched Code Considered Harmful -- The Impact of Similar but
  Patched Code on Recurring Vulnerability Detection and How to Remove Them <span class="chip">ICSE 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Tan, Jiayuan Zhou, Xing Hu, Shengyi Pan, Kui Liu, Xin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying recurring vulnerabilities is crucial for ensuring software
security. Clone-based techniques, while widely used, often generate many false
alarms due to the existence of similar but patched (SBP) code, which is similar
to vulnerable code but is not vulnerable due to having been patched. Although
the SBP code poses a great challenge to the effectiveness of existing
approaches, it has not yet been well explored.
  In this paper, we propose a programming language agnostic framework, Fixed
Vulnerability Filter (FVF), to identify and filter such SBP instances in
vulnerability detection. Different from existing studies that leverage function
signatures, our approach analyzes code change histories to precisely pinpoint
SBPs and consequently reduce false alarms. Evaluation under practical scenarios
confirms the effectiveness and precision of our approach. Remarkably, FVF
identifies and filters 65.1% of false alarms from four vulnerability detection
tools (i.e., ReDeBug, VUDDY, MVP, and an elementary hash-based approach)
without yielding false positives.
  We further apply FVF to 1,081 real-world software projects and construct a
real-world SBP dataset containing 6,827 SBP functions. Due to the SBP nature,
the dataset can act as a strict benchmark to test the sensitivity of the
vulnerability detection approach in distinguishing real vulnerabilities and
SBPs. Using this dataset, we demonstrate the ineffectiveness of four
state-of-the-art deep learning-based vulnerability detection approaches. Our
dataset can help developers make a more realistic evaluation of vulnerability
detection approaches and also paves the way for further exploration of
real-world SBP scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 47th IEEE/ACM International Conference on Software
  Engineering (ICSE 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Blockchain</span>-Empowered Cyber-Secure Federated Learning for <span class="highlight-title">Trust</span>worthy
  Edge Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ervin Moore, Ahmed Imteaj, Md Zarif Hossain, Shabnam Rezapour, M. Hadi Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a privacy-preserving distributed machine learning
scheme, where each participant data remains on the participating devices and
only the local model generated utilizing the local computational power is
transmitted throughout the database. However, the distributed computational
nature of FL creates the necessity to develop a mechanism that can remotely
trigger any network agents, track their activities, and prevent threats to the
overall process posed by malicious participants. Particularly, the FL paradigm
may become vulnerable due to an active attack from the network participants,
called a poisonous attack. In such an attack, the malicious participant acts as
a benign agent capable of affecting the global model quality by uploading an
obfuscated poisoned local model update to the server. This paper presents a
cross-device FL model that ensures trustworthiness, fairness, and authenticity
in the underlying FL training process. We leverage trustworthiness by
constructing a reputation-based trust model based on contributions of agents
toward model convergence. We ensure fairness by identifying and removing
malicious agents from the training process through an outlier detection
technique. Further, we establish authenticity by generating a token for each
participating device through a distributed sensing mechanism and storing that
unique token in a blockchain smart contract. Further, we insert the trust
scores of all agents into a blockchain and validate their reputations using
various consensus mechanisms that consider the computational task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeSynthDP: Leveraging Large Language Models for Privacy-Preserving
  Synthetic Data Generation Using Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mahadi Hasan Nahid, Sadid Bin Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) models frequently rely on training data that may
include sensitive or personal information, raising substantial privacy
concerns. Legislative frameworks such as the General Data Protection Regulation
(GDPR) and the California Consumer Privacy Act (CCPA) have necessitated the
development of strategies that preserve privacy while maintaining the utility
of data. In this paper, we investigate the capability of Large Language Models
(LLMs) to generate synthetic datasets integrated with Differential Privacy (DP)
mechanisms, thereby enabling data-driven research and model training without
direct exposure of sensitive information. Our approach incorporates DP-based
noise injection methods, including Laplace and Gaussian distributions, into the
data generation process. We then evaluate the utility of these DP-enhanced
synthetic datasets by comparing the performance of ML models trained on them
against models trained on the original data. To substantiate privacy
guarantees, we assess the resilience of the generated synthetic data to
membership inference attacks and related threats. The experimental results
demonstrate that integrating DP within LLM-driven synthetic data generation
offers a viable balance between privacy protection and data utility. This study
provides a foundational methodology and insight into the privacy-preserving
capabilities of LLMs, paving the way for compliant and effective ML research
and applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FreStega: A Plug-and-Play Method for Boosting Imperceptibility and
  Capacity in Generative Linguistic Steganography for Real-World Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyi Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linguistic steganography embeds secret information in seemingly innocent
texts, safeguarding privacy in surveillance environments. Generative linguistic
steganography leverages the probability distribution of language models (LMs)
and applies steganographic algorithms to generate stego tokens, gaining
attention with recent Large Language Model (LLM) advancements. To enhance
security, researchers develop distribution-preserving stego algorithms to
minimize the gap between stego sampling and LM sampling. However, the reliance
on language model distributions, coupled with deviations from real-world cover
texts, results in insufficient imperceptibility when facing steganalysis
detectors in real-world scenarios. Moreover, LLM distributions tend to be more
deterministic, resulting in reduced entropy and, consequently, lower embedding
capacity. In this paper, we propose FreStega, a plug-and-play method to
reconstruct the distribution of language models used for generative linguistic
steganography. FreStega dynamically adjusts token probabilities from the
language model at each step of stegotext auto-regressive generation, leveraging
both sequential and spatial dimensions. In sequential adjustment, the
temperature is dynamically adjusted based on instantaneous entropy, enhancing
the diversity of stego texts and boosting embedding capacity. In the spatial
dimension, the distribution is aligned with guidance from the target domain
corpus, closely mimicking real cover text in the target domain. By reforming
the distribution, FreStega enhances the imperceptibility of stego text in
practical scenarios and improves steganographic capacity by 15.41\%, all
without compromising the quality of the generated text. FreStega serves as a
plug-and-play remedy to enhance the imperceptibility and embedding capacity of
existing distribution-preserving steganography methods in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DV-FSR: A Dual-View Target Attack Framework for Federated Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qitao Qin, Yucong Luo, Mingyue Cheng, Qingyang Mao, Chenyi Lei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated recommendation (FedRec) preserves user privacy by enabling
decentralized training of personalized models, but this architecture is
inherently vulnerable to adversarial attacks. Significant research has been
conducted on targeted attacks in FedRec systems, motivated by commercial and
social influence considerations. However, much of this work has largely
overlooked the differential robustness of recommendation models. Moreover, our
empirical findings indicate that existing targeted attack methods achieve only
limited effectiveness in Federated Sequential Recommendation (FSR) tasks.
Driven by these observations, we focus on investigating targeted attacks in FSR
and propose a novel dualview attack framework, named DV-FSR. This attack method
uniquely combines a sampling-based explicit strategy with a contrastive
learning-based implicit gradient strategy to orchestrate a coordinated attack.
Additionally, we introduce a specific defense mechanism tailored for targeted
attacks in FSR, aiming to evaluate the mitigation effects of the attack method
we proposed. Extensive experiments validate the effectiveness of our proposed
approach on representative sequential models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>I am requesting the withdrawal of my paper due to identified errors
  that require significant revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Customer Support: A Framework for Secure and Scalable
  Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07687v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07687v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anant Prakash Awasthi, Girdhar Gopal Agarwal, Chandraketu Singh, Rakshit Varma, Sanchit Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing reliance on artificial intelligence (AI) in customer support has
significantly improved operational efficiency and user experience. However,
traditional machine learning (ML) approaches, which require extensive local
training on sensitive datasets, pose substantial privacy risks and compliance
challenges with regulations like the General Data Protection Regulation (GDPR)
and California Consumer Privacy Act (CCPA). Existing privacy-preserving
techniques, such as anonymization, differential privacy, and federated
learning, address some concerns but face limitations in utility, scalability,
and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning
(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in
a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates
the need for local training on sensitive data by utilizing pre-trained LLMs to
generate responses directly. The framework incorporates real-time data
anonymization to redact or mask sensitive information, retrieval-augmented
generation (RAG) for domain-specific query resolution, and robust
post-processing to ensure compliance with regulatory standards. This
combination reduces privacy risks, simplifies compliance, and enhances
scalability and operational efficiency. Empirical analysis demonstrates that
the PP-ZSL framework provides accurate, privacy-compliant responses while
significantly lowering the costs and complexities of deploying AI-driven
customer support systems. The study highlights potential applications across
industries, including financial services, healthcare, e-commerce, legal
support, telecommunications, and government services. By addressing the dual
challenges of privacy and performance, this framework establishes a foundation
for secure, efficient, and regulatory-compliant AI applications in customer
interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantitative Measurement of Cyber Resilience: Modeling and
  Experimentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16307v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16307v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael J. Weisman, Alexander Kott, Jason E. Ellis, Brian J. Murphy, Travis W. Parker, Sidney Smith, Joachim Vandekerckhove
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cyber resilience is the ability of a system to resist and recover from a
cyber attack, thereby restoring the system's functionality. Effective design
and development of a cyber resilient system requires experimental methods and
tools for quantitative measuring of cyber resilience. This paper describes an
experimental method and test bed for obtaining resilience-relevant data as a
system (in our case -- a truck) traverses its route, in repeatable, systematic
experiments. We model a truck equipped with an autonomous cyber-defense system
and which also includes inherent physical resilience features. When attacked by
malware, this ensemble of cyber-physical features (i.e., "bonware") strives to
resist and recover from the performance degradation caused by the malware's
attack. We propose parsimonious mathematical models to aid in quantifying
systems' resilience to cyber attacks. Using the models, we identify
quantitative characteristics obtainable from experimental data, and show that
these characteristics can serve as useful quantitative measures of cyber
resilience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2302.04413,
  arXiv:2302.07941</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Security Weaknesses of Copilot-Generated Code in <span class="highlight-title">GitHub</span> Projects: An
  Empirical Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02059v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02059v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Fu, Peng Liang, Amjed Tahir, Zengyang Li, Mojtaba Shahin, Jiaxin Yu, Jinfu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern code generation tools utilizing AI models like Large Language Models
(LLMs) have gained increased popularity due to their ability to produce
functional code. However, their usage presents security challenges, often
resulting in insecure code merging into the code base. Thus, evaluating the
quality of generated code, especially its security, is crucial. While prior
research explored various aspects of code generation, the focus on security has
been limited, mostly examining code produced in controlled environments rather
than open source development scenarios. To address this gap, we conducted an
empirical study, analyzing code snippets generated by GitHub Copilot and two
other AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub
projects. Our analysis identified 733 snippets, revealing a high likelihood of
security weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets
affected. These issues span 43 Common Weakness Enumeration (CWE) categories,
including significant ones like CWE-330: Use of Insufficiently Random Values,
CWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site
Scripting. Notably, eight of those CWEs are among the 2023 CWE Top-25,
highlighting their severity. We further examined using Copilot Chat to fix
security issues in Copilot-generated code by providing Copilot Chat with
warning messages from the static analysis tools, and up to 55.5% of the
security issues can be fixed. We finally provide the suggestions for mitigating
security issues in generated code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Laminator: Verifiable ML Property Cards using Hardware-assisted
  Attestations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasisht Duddu, Oskari Järvinen, Lachlan J Gunn, N Asokan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulations increasingly call for various assurances from machine learning
(ML) model providers about their training data, training process, and model
behavior. For better transparency, industry (e.g., Huggingface and Google) has
adopted model cards and datasheets to describe various properties of training
datasets and models. In the same vein, we introduce the notion of inference
cards to describe the properties of a given inference (e.g., binding of the
output to the model and its corresponding input). We coin the term ML property
cards to collectively refer to these various types of cards.
  To prevent a malicious model provider from including false information in ML
property cards, they need to be verifiable. We show how to construct verifiable
ML property cards using property attestation, technical mechanisms by which a
prover (e.g., a model provider) can attest to various ML properties to a
verifier (e.g., an auditor). Since prior attestation mechanisms based purely on
cryptography are often narrowly focused (lacking versatility) and inefficient,
we need an efficient mechanism to attest different types of properties across
the entire ML model pipeline.
  Emerging widespread support for confidential computing has made it possible
to run and even train models inside hardware-assisted trusted execution
environments (TEEs), which provide highly efficient attestation mechanisms. We
propose Laminator, which uses TEEs to provide the first framework for
verifiable ML property cards via hardware-assisted ML property attestations.
Laminator is efficient in terms of overhead, scalable to large numbers of
verifiers, and versatile with respect to the properties it can prove during
training or inference.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operating Systems <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Timing Analysis and Priority-driven Enhancements of ROS 2 Multi-threaded
  Executors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoora Sobhani, Hyunjong Choi, Hyoseung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The second generation of Robotic Operating System, ROS 2, has gained much
attention for its potential to be used for safety-critical robotic
applications. The need to provide a solid foundation for timing correctness and
scheduling mechanisms is therefore growing rapidly. Although there are some
pioneering studies conducted on formally analyzing the response time of
processing chains in ROS 2, the focus has been limited to single-threaded
executors, and multi-threaded executors, despite their advantages, have not
been studied well. To fill this knowledge gap, in this paper, we propose a
comprehensive response-time analysis framework for chains running on ROS 2
multi-threaded executors. We first analyze the timing behavior of the default
scheduling scheme in ROS 2 multi-threaded executors, and then present
priority-driven scheduling enhancements to address the limitations of the
default scheme. Our framework can analyze chains with both arbitrary and
constrained deadlines and also the effect of mutually-exclusive callback
groups. Evaluation is conducted by a case study on NVIDIA Jetson AGX Xavier and
schedulability experiments using randomly-generated chains. The results
demonstrate that our analysis framework can safely upper-bound response times
under various conditions and the priority-driven scheduling enhancements not
only reduce the response time of critical chains but also improve analytical
bounds.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Mixture-of-Agents for Edge Inference with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Purbesh Mitra, Priyanka Kaswan, Sennur Ulukus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Agents (MoA) has recently been proposed as a method to enhance
performance of large language models (LLMs), enabling multiple individual LLMs
to work together for collaborative inference. This collaborative approach
results in improved responses to user prompts compared to relying on a single
LLM. In this paper, we consider such an MoA architecture in a distributed
setting, where LLMs operate on individual edge devices, each uniquely
associated with a user and equipped with its own distributed computing power.
These devices exchange information using decentralized gossip algorithms,
allowing different device nodes to talk without the supervision of a
centralized server. In the considered setup, different users have their own LLM
models to address user prompts. Additionally, the devices gossip either their
own user-specific prompts or augmented prompts to generate more refined answers
to certain queries. User prompts are temporarily stored in the device queues
when their corresponding LLMs are busy. Given the memory limitations of edge
devices, it is crucial to ensure that the average queue sizes in the system
remain bounded. In this paper, we address this by theoretically calculating the
queuing stability conditions for the device queues under reasonable
assumptions, which we validate experimentally as well. Further, we demonstrate
through experiments, leveraging open-source LLMs for the implementation of
distributed MoA, that certain MoA configurations produce higher-quality
responses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The
implementation is available at:
https://github.com/purbeshmitra/distributed_moa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Hangover Effects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Santucci, Eric Lax
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It's not unreasonable to think that in-game sporting performance can be
affected partly by what takes place off the court. We can't observe what
happens between games directly. Instead, we proxy for the possibility of
athletes partying by looking at play following games in party cities. We are
interested to see if teams exhibit a decline in performance the day following a
game in a city with active nightlife; we call this a "hangover effect". Part of
the question is determining a reasonable way to measure levels of nightlife,
and correspondingly which cities are notorious for it; we colloquially refer to
such cities as "party cities". To carry out this study, we exploit data on
bookmaker spreads: the expected score differential between two teams after
conditioning on observable performance in past games and expectations about the
upcoming game. We expect a team to meet the spread half the time, since this is
one of the easiest ways for bookmakers to guarantee a profit. We construct a
model which attempts to estimate the causal effect of visiting a "party city"
on subsequent day performance as measured by the odds of beating the spread. In
particular, we only consider the hangover effect on games played back-to-back
within 24 hours of each other. To the extent that odds of beating the spread
against next day opponent is uncorrelated with playing in a party city the day
before, which should be the case under an efficient betting market, we have
identification in our variable of interest. We find that visiting a city with
active nightlife the day prior to a game does have a statistically significant
negative effect on a team's likelihood of meeting bookmakers' expectations for
both NBA and MLB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Quantum</span> Error Correction near the Coding Theoretical Bound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daiki Komoto, Kenta Kasai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in quantum computing have led to the realization of
systems comprising tens of reliable logical qubits, constructed from thousands
of noisy physical qubits. However, many of the critical applications that
quantum computers aim to solve require quantum computations involving millions
or more logical qubits. This necessitates highly efficient quantum error
correction capable of handling large numbers of logical qubits. Classical error
correction theory is well-developed, with low-density parity-check (LDPC) codes
achieving performance limits by encoding large classical bits. Despite more
than two decades of effort, no efficiently decodable quantum error-correcting
code that approaches the hashing bound, which is a fundamental lower bound on
quantum capacity, had been discovered. Here, we present quantum
error-correcting codes constructed from classical LDPC codes that approach the
hashing bound while maintaining linear computational complexity in the number
of physical qubits. This result establishes a pathway toward realizing
large-scale, fault-tolerant quantum computers. By integrating our quantum error
correction scheme with devices capable of managing vast numbers of qubits, the
prospect of solving critical real-world problems through quantum computation is
brought significantly closer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to a journal for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Approximate Degenerate Ordered Statistics Decoding for <span class="highlight-title">Quantum</span>
  Codes via Reliable Subset Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ching-Feng Kung, Kao-Yueh Kuo, Ching-Yi Lai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient decoding of quantum codes is crucial for achieving high-performance
quantum error correction. In this paper, we introduce the concept of
approximate degenerate decoding and integrate it with ordered statistics
decoding (OSD). Previously, we proposed a reliability metric that leverages
both hard and soft decisions from the output of belief propagation (BP), which
is particularly useful for identifying highly reliable subsets of variables.
Using the approach of reliable subset reduction, we reduce the effective
problem size. Additionally, we identify a degeneracy condition that allows
high-order OSD to be simplified to order-0 OSD. By integrating these
techniques, we present an ADOSD algorithm that significantly improves OSD
efficiency in the code capacity noise model. We demonstrate the effectiveness
of our BP+ADOSD approach through extensive simulations on a varity of quantum
codes, including generalized hypergraph-product codes, topological codes,
lift-connected surface codes, and bivariate bicycle codes. The results indicate
that the BP+ADOSD decoder outperforms existing methods, achieving higher error
thresholds and enhanced performance at low error rates. Additionally, we
validate the efficiency of our approach in terms of computational time,
demonstrating that ADOSD requires, on average, the same amount of time as two
to three BP iterations on surface codes at a depolarizing error rate of around
$1\%$. All the proposed algorithms are compared using single-threaded CPU
implementations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cluster-Based Time-Variant Channel Characterization and Modeling for
  5G-Railways 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuejian Zhang, Ruisi He, Bo Ai, Mi Yang, Jianwen Ding, Shuaiqi Gao, Ziyi Qi, Zhengyu Zhang, Zhangdui Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of high-speed railways, 5G for Railways (5G-R) is
gradually replacing Global System for the Mobile Communications for Railway
(GSM-R) worldwide to meet increasing demands. The large bandwidth, array
antennas, and non-stationarity caused by high mobility has made 5G-R channel
characterization more complex. Therefore, it is essential to develop an
accurate channel model for 5G-R. However, researches on channel
characterization and time-variant models specific to 5G-R frequency bands and
scenarios is scarce. There are virtually no cluster-based time-variant channel
models that capture statistical properties of 5G-R channel. In this paper, we
propose a cluster-based time-variant channel model for 5G-R within an enhanced
3GPP framework, which incorporates time evolution features. Extensive channel
measurements are conducted on 5G-R private network test line in China. We then
extract and analyze typical channel fading characteristics and multipath
cluster characteristics. Furthermore, birth-death process of the clusters is
modeled by using a four-state Markov chain. Finally, a generalized clustered
delay line (CDL) model is established in accordance with 3GPP standard and
validated by comparing the results of measurements and simulations. This work
enhances the understanding of 5G-R channels and presents a flexible
cluster-based time-variant channel model. The results can be used in the
design, deployment, and optimization of 5G-R networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 13 figures, submitted to IEEE Transactions on Wireless
  Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Diffusion Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saber Jafarizadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Of stochastic differential equations, diffusion processes have been adopted
in numerous applications, as more relevant and flexible models. This paper
studies diffusion processes in a different setting, where for a given
stationary distribution and average variance, it seeks the diffusion process
with optimal convergence rate. It is shown that the optimal drift function is a
linear function and the convergence rate of the stochastic process is bounded
by the ratio of the average variance to the variance of the stationary
distribution. Furthermore, the concavity of the optimal relaxation time as a
function of the stationary distribution has been proven, and it is shown that
all Pearson diffusion processes of the Hypergeometric type with polynomial
functions of at most degree two as the variance functions are optimal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 0 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel Charting-assisted Non-orthogonal Pilot Allocation for Uplink
  XL-MIMO Transmission 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohong Che, Li You, Jue Wang, Zhenzhou Jin, Chenjie Xie, Xiqi Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extremely large-scale multiple-input multiple-output (XL-MIMO) is critical to
future wireless networks. The substantial increase in the number of base
station (BS) antennas introduces near-field propagation effects in the wireless
channels, complicating channel parameter estimation and increasing pilot
overhead. Channel charting (CC) has emerged as a potent unsupervised technique
to effectively harness varying high-dimensional channel statistics to enable
non-orthogonal pilot assignment and reduce pilot overhead. In this paper, we
investigate near-field channel estimation with reduced pilot overhead by
developing a CC-assisted pilot scheduling. To this end, we introduce a
polar-domain codebook to capture the power distribution of near-field XL-MIMO
channels. The CC-assisted approach uses such features as inputs to enable an
effective low-dimensional mapping of the inherent correlation patterns in
near-field user terminal (UT) channels. Building upon the mapped channel
correlations, we further propose a near-field CC-assisted pilot allocation
(NCC-PA) algorithm, which efficiently enhances channel orthogonality among
pilot-reusing UTs. Numerical results confirm that the NCC-PA algorithm
substantially elevates the wireless transmission performance, offering a marked
improvement over the conventional far-field CC-PA approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CF-CGN: Channel Fingerprints Extrapolation for Multi-band Massive MIMO
  Transmission based on Cycle-Consistent Generative Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenjie Xie, Li You, Zhenzhou Jin, Jinke Tang, Xiqi Gao, Xiang-Gen Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-band massive multiple-input multiple-output (MIMO) communication can
promote the cooperation of licensed and unlicensed spectra, effectively
enhancing spectrum efficiency for Wi-Fi and other wireless systems. As an
enabler for multi-band transmission, channel fingerprints (CF), also known as
the channel knowledge map or radio environment map, are used to assist channel
state information (CSI) acquisition and reduce computational complexity. In
this paper, we propose CF-CGN (Channel Fingerprints with Cycle-consistent
Generative Networks) to extrapolate CF for multi-band massive MIMO transmission
where licensed and unlicensed spectra cooperate to provide ubiquitous
connectivity. Specifically, we first model CF as a multichannel image and
transform the extrapolation problem into an image translation task, which
converts CF from one frequency to another by exploring the shared
characteristics of statistical CSI in the beam domain. Then, paired generative
networks are designed and coupled by variable-weight cycle consistency losses
to fit the reciprocal relationship at different bands. Matched with the coupled
networks, a joint training strategy is developed accordingly, supporting
synchronous optimization of all trainable parameters. During the inference
process, we also introduce a refining scheme to improve the extrapolation
accuracy based on the resolution of CF. Numerical results illustrate that our
proposed CF-CGN can achieve bidirectional extrapolation with an error of 5-17
dB lower than the benchmarks in different communication scenarios,
demonstrating its excellent generalization ability. We further show that the
sum rate performance assisted by CF-CGN-based CF is close to that with perfect
CSI for multi-band massive MIMO transmission.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Application of Normalized Min-Sum Decoding for BCH Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangwen Li, Xiao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-throughput decoding of BCH codes necessitates efficient and
parallelizable decoders. However, the algebraic rigidity of BCH codes poses
significant challenges to applying parallel belief propagation variants. To
address this, we propose a systematic design scheme for constructing
parity-check matrices using a heuristic approach. This involves a sequence of
binary sum operations and row cyclic shifts on the standard parity-check
matrix, aiming to generate a redundant, low-density, and quasi-regular matrix
with significantly fewer length-4 cycles. The relationships between frame error
rate, rank deficiency of minimum-weight dual-code codewords, and row redundancy
are empirically analyzed. For the revised normalized min-sum decoder, we
introduce three types of random automorphisms applied to decoder inputs. These
are unpacked and aggregated by summing messages after each iteration, achieving
a 1-2dB improvement in bit error rate compared to parallelizable counterparts
and two orders of magnitude faster convergence in iterations than iterative
rivals. Additionally, undetected errors are highlighted as a non-negligible
issue for very short BCH codes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEO <span class="highlight-title">Satellite</span>-Enabled Random <span class="highlight-title">Access</span> with Large Differential Delay and
  Doppler Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxiao Shen, Yongpeng Wu, Wenjun Zhang, Symeon Chatzinotas, Björn Ottersten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates joint device identification, channel estimation, and
symbol detection for LEO satellite-enabled grant-free random access systems,
specifically targeting scenarios where remote Internet-of-Things (IoT) devices
operate without global navigation satellite system (GNSS) assistance.
Considering the constrained power consumption of these devices, the large
differential delay and Doppler shift are handled at the satellite receiver. We
firstly propose a spreading-based multi-frame transmission scheme with
orthogonal time-frequency space (OTFS) modulation to mitigate the doubly
dispersive effect in time and frequency, and then analyze the input-output
relationship of the system. Next, we propose a receiver structure based on
three modules: a linear module for identifying active devices that leverages
the generalized approximate message passing algorithm to eliminate inter-user
and inter-carrier interference; a non-linear module that employs the message
passing algorithm to jointly estimate the channel and detect the transmitted
symbols; and a third module that aims to exploit the three dimensional block
channel sparsity in the delay-Doppler-angle domain. Soft information is
exchanged among the three modules by careful message scheduling. Furthermore,
the expectation-maximization algorithm is integrated to adjust phase rotation
caused by the fractional Doppler and to learn the hyperparameters in the
priors. Finally, the convolutional neural network is incorporated to enhance
the symbol detection. Simulation results demonstrate that the proposed
transmission scheme boosts the system performance, and the designed algorithms
outperform the conventional methods significantly in terms of the device
identification, channel estimation, and symbol detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by the IEEE Transactions on Wireless
  Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model Enabled Multi-Task Physical Layer Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyue Zheng, Linglong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advance of Artificial Intelligence (AI) is continuously reshaping
the future 6G wireless communications. Recently, the development of Large
Language Models (LLMs) offers a promising approach to effectively improve the
performance and generalization for different physical layer tasks. However,
most existing works finetune dedicated LLM networks for a single wireless
communication task separately. Thus performing diverse physical layer tasks
introduces extremely high training resources, memory usage, and deployment
costs. To solve the problem, we propose a LLM-enabled multi-task physical layer
network to unify multiple tasks with a single LLM. Specifically, we first
propose a multi-task LLM framework, which finetunes LLM to perform multi-user
precoding, signal detection and channel prediction simultaneously. Besides,
multi-task instruction module, input encoders, as well as output decoders, are
elaborately designed to distinguish multiple tasks and adapted the features of
different formats of wireless data for the features of LLM. Numerical
simulations are also displayed to verify the effectiveness of the proposed
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimally Decoding Two-Dimensional Reed-Solomon Codes up to the
  Half-Singleton Bound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhransh Singhvi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing Reed-Solomon (RS) codes capable of correcting insertion and
deletion errors (ins-del errors) has been the focus of numerous recent studies.
However, the development of efficient decoding algorithms for such RS codes has
not garnered significant attention and remains an important and intriguing open
problem. In this work, we take a first step toward addressing this problem by
designing an optimal-time decoding algorithm for the special case of
two-dimensional RS codes, capable of decoding up to the half-Singleton bound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beam Codebook Refinement for mmWave Devices with Random Orientations:
  Concept and Experimental Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bora Bozkurt, Ahmet Muaz Aktas, Hasan Atalay Gunel, Mohaned Chraiti, Ali Gorcin, Ibrahim Hokelek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in codebook-based beam-steering for
millimeter-wave (mmWave) systems due to its potential for low complexity and
rapid beam search. A key focus of recent research has been the design of
codebooks that strike a trade-off between achievable gain and codebook size,
which directly impacts beam search time. Statistical approaches have shown
promise by leveraging the likelihood that certain beam directions
(equivalently, sets of phase-shifter configurations) are more probable than
others. Such approaches are shown to be valid for static, non-rotating
transmission stations such as base stations. However, for the case of user
terminals that are constantly changing orientation, the possible phase-shifter
configurations become equally probable, rendering statistical methods less
relevant. On the other hand, user terminals come with a large number of
possible steering vector configurations, which can span up to six orders of
magnitude. Therefore, efficient solutions to reduce the codebook size (set of
possible steering vectors) without compromising array gain are needed. We
address this challenge by proposing a novel and practical codebook refinement
technique, aiming to reduce the codebook size while maintaining array gain
within $\gamma$ dB of the maximum achievable gain at any random orientation of
the user terminal. We project that a steering vector at a given angle could
effectively cover adjacent angles with a small gain loss compared to the
maximum achievable gain. We demonstrate experimentally that it is possible to
reduce the codebook size from $1024^{16}$ to just a few configurations (e.g.,
less than ten), covering all angles while maintaining the gain within
$\gamma=3$ dB of the maximum achievable gain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Graph Neural Networks for Next-Generation-IoT: Recent Advances and Open
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nguyen Xuan Tung, Le Tung Giang, Bui Duc Son, Seon Geun Jeong, Trinh Van Chien, Won Joo Hwang, <span class="highlight-author">Lajos Hanzo</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have emerged as a critical tool for optimizing
and managing the complexities of the Internet of Things (IoT) in
next-generation networks. This survey presents a comprehensive exploration of
how GNNs may be harnessed in 6G IoT environments, focusing on key challenges
and opportunities through a series of open questions. We commence with an
exploration of GNN paradigms and the roles of node, edge, and graph-level tasks
in solving wireless networking problems and highlight GNNs' ability to overcome
the limitations of traditional optimization methods. This guidance enhances
problem-solving efficiency across various next-generation (NG) IoT scenarios.
Next, we provide a detailed discussion of the application of GNN in advanced NG
enabling technologies, including massive MIMO, reconfigurable intelligent
surfaces, satellites, THz, mobile edge computing (MEC), and ultra-reliable low
latency communication (URLLC). We then delve into the challenges posed by
adversarial attacks, offering insights into defense mechanisms to secure
GNN-based NG-IoT networks. Next, we examine how GNNs can be integrated with
future technologies like integrated sensing and communication (ISAC),
satellite-air-ground-sea integrated networks (SAGSIN), and quantum computing.
Our findings highlight the transformative potential of GNNs in improving
efficiency, scalability, and security within NG-IoT systems, paving the way for
future advances. Finally, we propose a set of design guidelines to facilitate
the development of efficient, scalable, and secure GNN models tailored for NG
IoT applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 15 figures, and 6 tables. Submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfAlign: Inference-aware language model alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, Ananda Theertha Suresh, Ahmad Beirami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model alignment has become a critical step in training modern
generative language models. The goal of alignment is to finetune a reference
model such that the win rate of a sample from the aligned model over a sample
from the reference model is high, subject to a KL divergence constraint. Today,
we are increasingly using inference-time algorithms (e.g., Best-of-N,
controlled decoding, tree search) to decode from language models rather than
standard sampling. However, the alignment objective does not capture such
inference-time decoding procedures. We show that the existing alignment
framework is sub-optimal in view of such inference-time methods. We then modify
the alignment objective and propose a framework for inference-aware alignment
(IAPO). We prove that for any inference-time decoding algorithm, the optimal
solution that optimizes the inference-time win rate of the aligned policy
against the reference policy is the solution to the typical RLHF problem with a
transformation of the reward. This motivates us to provide the KL-regularized
calibrate-and-transform RL (CTRL) algorithm to solve this problem, which
involves a reward calibration step and a KL-regularized reward maximization
step with a transformation of the calibrated reward. We particularize our study
to two important inference-time strategies: best-of-N sampling and best-of-N
jailbreaking, where N responses are sampled from the model and the one with the
highest or lowest reward is selected. We propose specific transformations for
these strategies and demonstrate that our framework offers significant
improvements over existing state-of-the-art methods for language model
alignment. Empirically, we outperform baselines that are designed without
taking inference-time decoding into consideration by 8-12% and 4-9% on
inference-time win rates over the Anthropic helpfulness and harmlessness dialog
benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fluid Antenna Index Modulation for MIMO Systems: Robust Transmission and
  Low-Complexity Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Guo, Yin Xu, Dazhi He, Cixiao Zhang, Hanjiang Hong, Kai-Kit Wong, Wenjun Zhang, Yiyan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fluid antenna (FA) index modulation (IM)-enabled multiple-input
multiple-output (MIMO) system, referred to as FA-IM, significantly enhances
spectral efficiency (SE) compared to the conventional FA-assisted MIMO system.
To improve robustness against the high spatial correlation among multiple
activated ports of the fluid antenna, this paper proposes an innovative FA
grouping-based IM (FAG-IM) system. A block grouping scheme is employed based on
the spatial correlation model and the distribution structure of the ports.
Then, a closed-form expression for the average bit error probability (ABEP)
upper bound of the FAG-IM system is derived. To reduce the complexity of the
receiver, the message passing architecture is incorporated into the FAG-IM
system. Building on this, an efficient approximate message passing (AMP)
detector, named structured AMP (S-AMP) detector, is proposed by exploiting the
structural characteristics of the transmitted signals. Simulation results
confirm that the proposed FAG-IM system significantly outperforms the existing
FA-IM system in the presence of spatial correlation, achieving more robust
transmission. Furthermore, it is demonstrated that the proposed low-complexity
S-AMP detector not only reduces time complexity to a linear scale but also
substantially improves bit error rate (BER) performance compared to the minimum
mean square error (MMSE) detector, thereby enhancing the practical feasibility
of the FAG-IM system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to an IEEE journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impossibility of Local State Transformation via Hypercontractivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1307.2747v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1307.2747v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Delgosha, Salman Beigi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Local state transformation is the problem of transforming an arbitrary number
of copies of a bipartite resource state to a bipartite target state under local
operations. That is, given two bipartite states, is it possible to transform an
arbitrary number of copies of one of them to one copy of the other state under
local operations only? This problem is a hard one in general since we assume
that the number of copies of the resource state is arbitrarily large. In this
paper we prove some bounds on this problem using the hypercontractivity
properties of some super-operators corresponding to bipartite states. We
measure hypercontractivity in terms of both the usual super-operator norms as
well as completely bounded norms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, fixed the statement of theorem 16</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoding <span class="highlight-title">quantum</span> information via the Petz recovery map 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1504.04449v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1504.04449v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salman Beigi, Nilanjana Datta, Felix Leditzky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We obtain a lower bound on the maximum number of qubits, $Q^{n,
\epsilon}(\mathcal{N})$, which can be transmitted over $n$ uses of a quantum
channel $\mathcal{N}$, for a given non-zero error threshold $\epsilon$. To
obtain our result, we first derive a bound on the one-shot entanglement
transmission capacity of the channel, and then compute its asymptotic expansion
up to the second order. In our method to prove this achievability bound, the
decoding map, used by the receiver on the output of the channel, is chosen to
be the \emph{Petz recovery map} (also known as the \emph{transpose channel}).
Our result, in particular, shows that this choice of the decoder can be used to
establish the coherent information as an achievable rate for quantum
information transmission. Applying our achievability bound to the 50-50 erasure
channel (which has zero quantum capacity), we find that there is a sharp error
threshold above which $Q^{n, \epsilon}(\mathcal{N})$ scales as $\sqrt{n}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, removed section 4 of the previous version which included an
  incorrect lemma</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-component spatiotemporal template for activation-inhibition of
  speech in ECoG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Easthope
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  I compute the average trial-by-trial power of band-limited speech activity
across epochs of multi-channel high-density electrocorticography (ECoG)
recorded from multiple subjects during a consonant-vowel speaking task. I show
that previously seen anti-correlations of average beta frequency activity
(12-35 Hz) to high-frequency gamma activity (70-140 Hz) during speech movement
are observable between individual ECoG channels in the sensorimotor cortex
(SMC). With this I fit a variance-based model using principal component
analysis to the band-powers of individual channels of session-averaged ECoG
data in the SMC and project SMC channels onto their lower-dimensional principal
components.
  Spatiotemporal relationships between speech-related activity and principal
components are identified by correlating the principal components of both
frequency bands to individual ECoG channels over time using windowed
correlation. Correlations of principal component areas to sensorimotor areas
reveal a distinct two-component activation-inhibition-like representation for
speech that resembles distinct local sensorimotor areas recently shown to have
complex interplay in whole-body motor control, inhibition, and posture. Notably
the third principal component shows insignificant correlations across all
subjects, suggesting two components of ECoG are sufficient to represent SMC
activity during speech movement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Attack and Defense for LoRa Device Identification and
  Authentication via Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalin E. Sagduyu, Tugba Erpek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LoRa provides long-range, energy-efficient communications in Internet of
Things (IoT) applications that rely on Low-Power Wide-Area Network (LPWAN)
capabilities. Despite these merits, concerns persist regarding the security of
LoRa networks, especially in situations where device identification and
authentication are imperative to secure the reliable access to the LoRa
networks. This paper explores a deep learning (DL) approach to tackle these
concerns, focusing on two critical tasks, namely (i) identifying LoRa devices
and (ii) classifying them to legitimate and rogue devices. Deep neural networks
(DNNs), encompassing both convolutional and feedforward neural networks, are
trained for these tasks using actual LoRa signal data. In this setting, the
adversaries may spoof rogue LoRa signals through the kernel density estimation
(KDE) method based on legitimate device signals that are received by the
adversaries. Two cases are considered, (i) training two separate classifiers,
one for each of the two tasks, and (ii) training a multi-task classifier for
both tasks. The vulnerabilities of the resulting DNNs to manipulations in input
samples are studied in form of untargeted and targeted adversarial attacks
using the Fast Gradient Sign Method (FGSM). Individual and common perturbations
are considered against single-task and multi-task classifiers for the LoRa
signal analysis. To provide resilience against such attacks, a defense approach
is presented by increasing the robustness of classifiers with adversarial
training. Results quantify how vulnerable LoRa signal classification tasks are
to adversarial attacks and emphasize the need to fortify IoT applications
against these subtle yet effective threats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepF-fNet: a physics-informed neural network for vibration isolation
  optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Tollardo, F. Cadini, M. Giglio, L. Lomazzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural optimization is essential for designing safe, efficient, and
durable components with minimal material usage. Traditional methods for
vibration control often rely on active systems to mitigate unpredictable
vibrations, which may lead to resonance and potential structural failure.
However, these methods face significant challenges when addressing the
nonlinear inverse eigenvalue problems required for optimizing structures
subjected to a wide range of frequencies. As a result, no existing approach has
effectively addressed the need for real-time vibration suppression within this
context, particularly in high-performance environments such as automotive
noise, vibration and harshness, where computational efficiency is crucial.
  This study introduces DeepF-fNet, a novel neural network framework designed
to replace traditional active systems in vibration-based structural
optimization. Leveraging DeepONets within the context of physics-informed
neural networks, DeepF-fNet integrates both data and the governing physical
laws. This enables rapid identification of optimal parameters to suppress
critical vibrations at specific frequencies, offering a more efficient and
real-time alternative to conventional methods.
  The proposed framework is validated through a case study involving a locally
resonant metamaterial used to isolate structures from user-defined frequency
ranges. The results demonstrate that DeepF-fNet outperforms traditional genetic
algorithms in terms of computational speed while achieving comparable results,
making it a promising tool for vibration-sensitive applications. By replacing
active systems with machine learning techniques, DeepF-fNet paves the way for
more efficient and cost-effective structural optimization in real-world
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Array Sensor Selection in ISAC with Identifiability Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Rajamäki, Piya Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates array geometry and waveform design for integrated
sensing and communications (ISAC) employing sensor selection. We consider ISAC
via index modulation, where various subsets of transmit (Tx) sensors are used
for both communications and monostatic active sensing. The set of Tx subarrays
make up a codebook, whose cardinality we maximize (for communications) subject
to guaranteeing a desired target identifiability (for sensing). To characterize
the size of this novel optimal codebook, we derive first upper and lower
bounds, which are tight in case of the canonical uniform linear array (ULA) and
any nonredundant array. We show that the ULA achieves a large codebook -
comparable to the size of the conventional unconstrained case - as satisfying
the identifiability constraint only requires including two specific sensors in
each Tx subarray (codeword). In contrast, nonredundant arrays, which have the
largest identifiability for a given number of physical sensors, only have a
single admissible codeword, rendering them ineffectual for communications via
sensor selection alone. The results serve as a step towards an analytical
understanding of the limits of sensor selection in ISAC and the fundamental
trade-offs therein.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\copyright 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Maximum-Likelihood Estimation of Wideband Polynomial-Phase
  Signals on Sensor Array 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaleb Debre, Tai Fei, Marius Pesavento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel sequential estimator for the direction-of-arrival
and polynomial coefficients of wideband polynomial-phase signals impinging on a
sensor array. Addressing the computational challenges of Maximum-likelihood
estimation for this problem, we propose a method leveraging random sampling
consensus (RANSAC) applied to the time-frequency spatial signatures of sources.
Our approach supports multiple sources and higher-order polynomials by
employing coherent array processing and sequential approximations of the
Maximum-likelihood cost function. We also propose a low-complexity variant that
estimates source directions via angular domain random sampling. Numerical
evaluations demonstrate that the proposed methods achieve Cram\'er-Rao bounds
in challenging multi-source scenarios, including closely spaced time-frequency
spatial signatures, highlighting their suitability for advanced radar signal
processing applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI Empowered Semantic Feature Multiple <span class="highlight-title">Access</span> (SFMA) Over
  Wireless Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxiang Wang, Yinchao Yang, Zhaohui Yang, Chongwen Huang, Mingzhe Chen, Zhaoyang Zhang, Mohammad Shikh-Bahaei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates a novel generative artificial intelligence (GAI)
empowered multi-user semantic communication system called semantic feature
multiple access (SFMA) for video transmission, which comprises a base station
(BS) and paired users. The BS generates and combines semantic information of
several frames simultaneously requested by paired users into a single signal.
Users recover their frames from this combined signal and input the recovered
frames into a GAI-based video frame interpolation model to generate the
intermediate frame. To optimize transmission rates and temporal gaps between
simultaneously transmitted frames, we formulate an optimization problem to
maximize the system sum rate while minimizing temporal gaps. Since the standard
signal-to-interference-plus-noise ratio (SINR) equation does not accurately
capture the performance of our semantic communication system, we introduce a
weight parameter into the SINR equation to better represent the system's
performance. Due to its dependence on transmit power, we propose a three-step
solution. First, we develop a user pairing algorithm that pairs two users with
the highest preference value, a weighted combination of semantic transmission
rate and temporal gap. Second, we optimize inter-group power allocation by
formulating an optimization problem that allocates proper transmit power across
all user groups to maximize system sum rates while satisfying each user's
minimum rate requirement. Third, we address intra-group power allocation to
enhance each user's performance. Simulation results demonstrate that our method
improves transmission rates by up to 24.8%, 45.8%, and 66.1% compared to
fixed-power non-orthogonal multiple access (F-NOMA), orthogonal joint
source-channel coding (O-JSCC), and orthogonal frequency division multiple
access (OFDMA), respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Channel Charting-assisted Non-orthogonal Pilot Allocation for Uplink
  XL-MIMO Transmission 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohong Che, Li You, Jue Wang, Zhenzhou Jin, Chenjie Xie, Xiqi Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extremely large-scale multiple-input multiple-output (XL-MIMO) is critical to
future wireless networks. The substantial increase in the number of base
station (BS) antennas introduces near-field propagation effects in the wireless
channels, complicating channel parameter estimation and increasing pilot
overhead. Channel charting (CC) has emerged as a potent unsupervised technique
to effectively harness varying high-dimensional channel statistics to enable
non-orthogonal pilot assignment and reduce pilot overhead. In this paper, we
investigate near-field channel estimation with reduced pilot overhead by
developing a CC-assisted pilot scheduling. To this end, we introduce a
polar-domain codebook to capture the power distribution of near-field XL-MIMO
channels. The CC-assisted approach uses such features as inputs to enable an
effective low-dimensional mapping of the inherent correlation patterns in
near-field user terminal (UT) channels. Building upon the mapped channel
correlations, we further propose a near-field CC-assisted pilot allocation
(NCC-PA) algorithm, which efficiently enhances channel orthogonality among
pilot-reusing UTs. Numerical results confirm that the NCC-PA algorithm
substantially elevates the wireless transmission performance, offering a marked
improvement over the conventional far-field CC-PA approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CF-CGN: Channel Fingerprints Extrapolation for Multi-band Massive MIMO
  Transmission based on Cycle-Consistent Generative Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenjie Xie, Li You, Zhenzhou Jin, Jinke Tang, Xiqi Gao, Xiang-Gen Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-band massive multiple-input multiple-output (MIMO) communication can
promote the cooperation of licensed and unlicensed spectra, effectively
enhancing spectrum efficiency for Wi-Fi and other wireless systems. As an
enabler for multi-band transmission, channel fingerprints (CF), also known as
the channel knowledge map or radio environment map, are used to assist channel
state information (CSI) acquisition and reduce computational complexity. In
this paper, we propose CF-CGN (Channel Fingerprints with Cycle-consistent
Generative Networks) to extrapolate CF for multi-band massive MIMO transmission
where licensed and unlicensed spectra cooperate to provide ubiquitous
connectivity. Specifically, we first model CF as a multichannel image and
transform the extrapolation problem into an image translation task, which
converts CF from one frequency to another by exploring the shared
characteristics of statistical CSI in the beam domain. Then, paired generative
networks are designed and coupled by variable-weight cycle consistency losses
to fit the reciprocal relationship at different bands. Matched with the coupled
networks, a joint training strategy is developed accordingly, supporting
synchronous optimization of all trainable parameters. During the inference
process, we also introduce a refining scheme to improve the extrapolation
accuracy based on the resolution of CF. Numerical results illustrate that our
proposed CF-CGN can achieve bidirectional extrapolation with an error of 5-17
dB lower than the benchmarks in different communication scenarios,
demonstrating its excellent generalization ability. We further show that the
sum rate performance assisted by CF-CGN-based CF is close to that with perfect
CSI for multi-band massive MIMO transmission.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Deep Synthesis of MIMO Sensing Waveforms with Desired
  Transmit Beampattern 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vesa Saarinen, Robin Rajamäki, Visa Koivunen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a generative deep learning model for the synthesis of
multiple-input multiple-output (MIMO) active sensing waveforms with desired
properties, including constant modulus and a user-defined beampattern. The
proposed approach is capable synthesizing unique phase codes of on-the-fly,
which has the potential to reduce interference between co-existing active
sensing systems and facilitate Low Probability of Intercept/Low Probability of
Detection (LPI/LPD) radar operation. The paper extends our earlier work on
synthesis of approximately orthogonal MIMO phase codes by introducing flexible
control over the transmit beampatterns. The developed machine learning method
employs a conditional Wasserstein Generative Adversarial Network (GAN)
structure. The main benefits of the method are its ability to discover new
waveforms on-demand (post training) and generate demanding beampatterns at
lower computational complexity compared to structured optimization approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-Augmented Generation for Mobile Edge Computing via Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runtao Ren, Yinyu Wu, Xuhui Zhang, Jinke Ren, Yanyan Shen, Shuqiang Wang, Kim-Fung Tsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of mobile edge computing (MEC) has introduced significant
challenges in optimizing resource allocation in highly dynamic wireless
communication systems, in which task offloading decisions should be made in
real-time. However, existing resource allocation strategies cannot well adapt
to the dynamic and heterogeneous characteristics of MEC systems, since they are
short of scalability, context-awareness, and interpretability. To address these
issues, this paper proposes a novel retrieval-augmented generation (RAG) method
to improve the performance of MEC systems. Specifically, a latency minimization
problem is first proposed to jointly optimize the data offloading ratio,
transmit power allocation, and computing resource allocation. Then, an
LLM-enabled information-retrieval mechanism is proposed to solve the problem
efficiently. Extensive experiments across multi-user, multi-task, and highly
dynamic offloading scenarios show that the proposed method consistently reduces
latency compared to several DL-based approaches, achieving 57% improvement
under varying user computing ability, 86% with different servers, 30% under
distinct transmit powers, and 42% for varying data volumes. These results show
the effectiveness of LLM-driven solutions to solve the resource allocation
problems in MEC systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been submitted to IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movable Antennas Enabled ISAC Systems: Fundamentals, Opportunities, and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhendong Li, Jianle Ba, Zhou Su, Jinyuan Huang, Haixia Peng, Wen Chen, Linkang Du, Tom H. Luan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The movable antenna (MA)-enabled integrated sensing and communication (ISAC)
system attracts widespread attention as an innovative framework. The ISAC
system integrates sensing and communication functions, achieving resource
sharing across various domains, significantly enhancing communication and
sensing performance, and promoting the intelligent interconnection of
everything. Meanwhile, MA utilizes the spatial variations of wireless channels
by dynamically adjusting the positions of MA elements at the transmitter and
receiver to improve the channel and further enhance the performance of the ISAC
systems. In this paper, we first outline the fundamental principles of MA and
introduce the application scenarios of MA-enabled ISAC systems. Then, we
summarize the advantages of MA-enabled ISAC systems in enhancing spectral
efficiency, achieving flexible and precise beamforming, and making the signal
coverage range adjustable. Besides, a specific case is studied to show the
performance gains in terms of transmit power that MA brings to ISAC systems.
Finally, we discuss the challenges of MA-enabled ISAC and future research
directions, aiming to provide insights for future research on MA-enabled ISAC
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEO <span class="highlight-title">Satellite</span>-Enabled Random <span class="highlight-title">Access</span> with Large Differential Delay and
  Doppler Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxiao Shen, Yongpeng Wu, Wenjun Zhang, Symeon Chatzinotas, Björn Ottersten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates joint device identification, channel estimation, and
symbol detection for LEO satellite-enabled grant-free random access systems,
specifically targeting scenarios where remote Internet-of-Things (IoT) devices
operate without global navigation satellite system (GNSS) assistance.
Considering the constrained power consumption of these devices, the large
differential delay and Doppler shift are handled at the satellite receiver. We
firstly propose a spreading-based multi-frame transmission scheme with
orthogonal time-frequency space (OTFS) modulation to mitigate the doubly
dispersive effect in time and frequency, and then analyze the input-output
relationship of the system. Next, we propose a receiver structure based on
three modules: a linear module for identifying active devices that leverages
the generalized approximate message passing algorithm to eliminate inter-user
and inter-carrier interference; a non-linear module that employs the message
passing algorithm to jointly estimate the channel and detect the transmitted
symbols; and a third module that aims to exploit the three dimensional block
channel sparsity in the delay-Doppler-angle domain. Soft information is
exchanged among the three modules by careful message scheduling. Furthermore,
the expectation-maximization algorithm is integrated to adjust phase rotation
caused by the fractional Doppler and to learn the hyperparameters in the
priors. Finally, the convolutional neural network is incorporated to enhance
the symbol detection. Simulation results demonstrate that the proposed
transmission scheme boosts the system performance, and the designed algorithms
outperform the conventional methods significantly in terms of the device
identification, channel estimation, and symbol detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by the IEEE Transactions on Wireless
  Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Experimental Study of Passive UAV Tracking with Digital Arrays and
  Cellular Downlink Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Sun, Chao Yu, Yan Luo, Tony Xiao Han, Haisheng Tan, Rui Wang, Francis C. M. Lau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the prospects of the low-altitude economy (LAE) and the popularity of
unmanned aerial vehicles (UAVs), there are increasing demands on monitoring
flying objects at low altitude in wide urban areas. In this work, the widely
deployed long-term evolution (LTE) base station (BS) is exploited to illuminate
UAVs in bistatic trajectory tracking. Specifically, a passive sensing receiver
with two digital antenna arrays is proposed and developed to capture both the
line-of-sight (LoS) signal and the scattered signal off a target UAV. From
their cross ambiguity function, the bistatic range, Doppler shift and
angle-of-arrival (AoA) of the target UAV can be detected in a sequence of time
slots. In order to address missed detections and false alarms of passive
sensing, a multi-target tracking framework is adopted to track the trajectory
of the target UAV. It is demonstrated by experiments that the proposed UAV
tracking system can achieve a meter-level accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures, submitted to IEEE Journal for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model Enabled Multi-Task Physical Layer Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyue Zheng, Linglong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advance of Artificial Intelligence (AI) is continuously reshaping
the future 6G wireless communications. Recently, the development of Large
Language Models (LLMs) offers a promising approach to effectively improve the
performance and generalization for different physical layer tasks. However,
most existing works finetune dedicated LLM networks for a single wireless
communication task separately. Thus performing diverse physical layer tasks
introduces extremely high training resources, memory usage, and deployment
costs. To solve the problem, we propose a LLM-enabled multi-task physical layer
network to unify multiple tasks with a single LLM. Specifically, we first
propose a multi-task LLM framework, which finetunes LLM to perform multi-user
precoding, signal detection and channel prediction simultaneously. Besides,
multi-task instruction module, input encoders, as well as output decoders, are
elaborately designed to distinguish multiple tasks and adapted the features of
different formats of wireless data for the features of LLM. Numerical
simulations are also displayed to verify the effectiveness of the proposed
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beam Codebook Refinement for mmWave Devices with Random Orientations:
  Concept and Experimental Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bora Bozkurt, Ahmet Muaz Aktas, Hasan Atalay Gunel, Mohaned Chraiti, Ali Gorcin, Ibrahim Hokelek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing interest in codebook-based beam-steering for
millimeter-wave (mmWave) systems due to its potential for low complexity and
rapid beam search. A key focus of recent research has been the design of
codebooks that strike a trade-off between achievable gain and codebook size,
which directly impacts beam search time. Statistical approaches have shown
promise by leveraging the likelihood that certain beam directions
(equivalently, sets of phase-shifter configurations) are more probable than
others. Such approaches are shown to be valid for static, non-rotating
transmission stations such as base stations. However, for the case of user
terminals that are constantly changing orientation, the possible phase-shifter
configurations become equally probable, rendering statistical methods less
relevant. On the other hand, user terminals come with a large number of
possible steering vector configurations, which can span up to six orders of
magnitude. Therefore, efficient solutions to reduce the codebook size (set of
possible steering vectors) without compromising array gain are needed. We
address this challenge by proposing a novel and practical codebook refinement
technique, aiming to reduce the codebook size while maintaining array gain
within $\gamma$ dB of the maximum achievable gain at any random orientation of
the user terminal. We project that a steering vector at a given angle could
effectively cover adjacent angles with a small gain loss compared to the
maximum achievable gain. We demonstrate experimentally that it is possible to
reduce the codebook size from $1024^{16}$ to just a few configurations (e.g.,
less than ten), covering all angles while maintaining the gain within
$\gamma=3$ dB of the maximum achievable gain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved ICNN-LSTM Model Classification Based on Attitude Sensor Data
  for Hazardous State Assessment of Magnetic Adhesion Climbing Wall Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Ma, He Xu, Jielong Dou, Yi Qin, Xueyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic adhesion tracked climbing robots are widely utilized in
high-altitude inspection, welding, and cleaning tasks due to their ability to
perform various operations against gravity on vertical or inclined walls.
However, during operation, the robot may experience overturning torque caused
by its own weight and load, which can lead to the detachment of magnetic plates
and subsequently pose safety risks. This paper proposes an improved ICNN-LSTM
network classification method based on Micro-Electro-Mechanical Systems (MEMS)
attitude sensor data for real-time monitoring and assessment of hazardous
states in magnetic adhesion tracked climbing robots. Firstly, a data
acquisition strategy for attitude sensors capable of capturing minute
vibrations is designed. Secondly, a feature extraction and classification model
combining an Improved Convolutional Neural Network (ICNN) with a Long
Short-Term Memory (LSTM) network is proposed. Experimental validation
demonstrates that the proposed minute vibration sensing method achieves
significant results, and the proposed classification model consistently
exhibits high accuracy compared to other models. The research findings provide
effective technical support for the safe operation of climbing robots
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures, manuscript for Journal of Autonomous Robots</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Efficient LoRaWAN in LEO <span class="highlight-title">Satellite</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muskan Shergill, Zach Thompson, Guanqun Song, Ting Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LPWAN service's inexpensive cost and long range capabilities make it a
promising addition and countless satellite companies have started taking
advantage of this technology to connect IoT users across the globe. However,
LEO satellites have the unique challenge of using rechargeable batteries and
green solar energy to power their components. LPWAN technology is not optimized
to maximize battery lifespan of network nodes. By incorporating a MAC protocol
that maximizes node the battery lifespan across the network, we can reduce
battery waste and usage of scarce Earth resources to develop satellite
batteries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ultralight Signal Classification Model for Automatic Modulation
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Daniele Genuardi Oquendo, Agustín Matías Galante Cerviño, Nilotpal Kanti Sinha, Luc Andrea, Sam Mugel, Román Orús
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing complexity of radar signals demands responsive and accurate
detection systems that can operate efficiently on resource-constrained edge
devices. Existing models, while effective, often rely on substantial
computational resources and large datasets, making them impractical for edge
deployment. In this work, we propose an ultralight hybrid neural network
optimized for edge applications, delivering robust performance across
unfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less
than 100 samples per class, and significantly reducing computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UAV Communications: Impact of Obstacles on Channel Characteristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamal Shayegan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Unmanned Aerial Vehicles (UAVs) have been utilized as
effective platforms for carrying Wi-Fi Access Points (APs) and cellular Base
Stations (BSs), enabling low-cost, agile, and flexible wireless networks with
high Quality of Service (QoS). The next generation of wireless communications
will rely on increasingly higher frequencies, which are easily obstructed by
obstacles. One of the most critical concepts yet to be fully addressed is
positioning the UAV at optimal coordinates while accounting for obstacles. To
ensure a line of sight (LoS) between UAVs and user equipment (UE), improve QoS,
and establish reliable wireless links with maximum coverage, obstacles must be
integrated into the proposed placement algorithms. This paper introduces a
simulation-based measurement approach for characterizing an air-to-ground (AG)
channel in a simple scenario. By considering obstacles, we present a novel
perspective on channel characterization. The results, in terms of throughput,
packet delivery, packet loss, and delay, are compared using the proposed
positioning approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fluid Antenna Index Modulation for MIMO Systems: Robust Transmission and
  Low-Complexity Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinghao Guo, Yin Xu, Dazhi He, Cixiao Zhang, Hanjiang Hong, Kai-Kit Wong, Wenjun Zhang, Yiyan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fluid antenna (FA) index modulation (IM)-enabled multiple-input
multiple-output (MIMO) system, referred to as FA-IM, significantly enhances
spectral efficiency (SE) compared to the conventional FA-assisted MIMO system.
To improve robustness against the high spatial correlation among multiple
activated ports of the fluid antenna, this paper proposes an innovative FA
grouping-based IM (FAG-IM) system. A block grouping scheme is employed based on
the spatial correlation model and the distribution structure of the ports.
Then, a closed-form expression for the average bit error probability (ABEP)
upper bound of the FAG-IM system is derived. To reduce the complexity of the
receiver, the message passing architecture is incorporated into the FAG-IM
system. Building on this, an efficient approximate message passing (AMP)
detector, named structured AMP (S-AMP) detector, is proposed by exploiting the
structural characteristics of the transmitted signals. Simulation results
confirm that the proposed FAG-IM system significantly outperforms the existing
FA-IM system in the presence of spatial correlation, achieving more robust
transmission. Furthermore, it is demonstrated that the proposed low-complexity
S-AMP detector not only reduces time complexity to a linear scale but also
substantially improves bit error rate (BER) performance compared to the minimum
mean square error (MMSE) detector, thereby enhancing the practical feasibility
of the FAG-IM system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to an IEEE journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multi-Agent Multi-Environment Mixed Q-Learning for Partially
  Decentralized Wireless Network Optimization <span class="chip">SP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Talha Bozkus, Urbashi Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Q-learning is a powerful tool for network control and policy optimization in
wireless networks, but it struggles with large state spaces. Recent
advancements, like multi-environment mixed Q-learning (MEMQ), improves
performance and reduces complexity by integrating multiple Q-learning
algorithms across multiple related environments so-called digital cousins.
However, MEMQ is designed for centralized single-agent networks and is not
suitable for decentralized or multi-agent networks. To address this challenge,
we propose a novel multi-agent MEMQ algorithm for partially decentralized
wireless networks with multiple mobile transmitters (TXs) and base stations
(BSs), where TXs do not have access to each other's states and actions. In
uncoordinated states, TXs act independently to minimize their individual costs.
In coordinated states, TXs use a Bayesian approach to estimate the joint state
based on local observations and share limited information with leader TX to
minimize joint cost. The cost of information sharing scales linearly with the
number of TXs and is independent of the joint state-action space size. The
proposed scheme is 50% faster than centralized MEMQ with only a 20% increase in
average policy error (APE) and is 25% faster than several advanced
decentralized Q-learning algorithms with 40% less APE. The convergence of the
algorithm is also demonstrated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2025 IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Movable Antenna Enabled Integrated Sensing and Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanting Lyu, Songjie Yang, Yue Xiu, Zhongpei Zhang, Chadi Assi, Chau Yuen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate a novel integrated sensing and communication
(ISAC) system aided by movable antennas (MAs). A bistatic radar system, in
which the base station (BS) is configured with MAs, is integrated into a
multi-user multiple-input-single-output (MU-MISO) system. Flexible beamforming
is studied by jointly optimizing the antenna coefficients and the antenna
positions. Compared to conventional fixed-position antennas (FPAs), MAs provide
a new degree of freedom (DoF) in beamforming to reconfigure the field response,
and further improve the received signal quality for both wireless communication
and sensing. We propose a communication rate and sensing mutual information
(MI) maximization problem by flexible beamforming optimization. The complex
fractional objective function with logarithms are first transformed with the
fractional programming (FP) framework. Then, we propose an efficient algorithm
to address the non-convex problem with coupled variables by alternatively
solving four sub-problems. We derive the closed-form expression to update the
antenna coefficients by Karush-Kuhn-Tucker (KKT) conditions. To improve the
direct gradient ascent (DGA) scheme in updating the positions of the antennas,
a 3-stage search-based projected GA (SPGA) method is proposed. Simulation
results show that MAs significantly enhance the overall performance of the ISAC
system, achieving 59.8\% performance gain compared to conventional ISAC system
enabled by FPAs. Meanwhile, the proposed SPGA-based method has remarkable
performance improvement compared the DGA method in antenna position
optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESI-GAL: EEG Source Imaging-based Trajectory Estimation for Grasp and
  Lift Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11500v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11500v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anant Jain, Lalan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electroencephalogram (EEG) signals-based motor kinematics prediction (MKP)
has been an active area of research to develop brain-computer interface (BCI)
systems such as exosuits, prostheses, and rehabilitation devices. However, EEG
source imaging (ESI) based kinematics prediction is sparsely explored in the
literature. In this study, pre-movement EEG features are utilized to predict
three-dimensional (3D) hand kinematics for the grasp-and-lift motor task. A
public dataset, WAY-EEG-GAL, is utilized for MKP analysis. In particular,
sensor-domain (EEG data) and source-domain (ESI data) based features from the
frontoparietal region are explored for MKP. Deep learning-based models are
explored to achieve efficient kinematics decoding. Various time-lagged and
window sizes are analyzed for hand kinematics prediction. Subsequently,
intra-subject and inter-subject MKP analysis is performed to investigate the
subject-specific and subject-independent motor-learning capabilities of the
neural decoders. The Pearson correlation coefficient (PCC) is used as the
performance metric for kinematics trajectory decoding. The rEEGNet neural
decoder achieved the best performance with sensor-domain and source-domain
features with a time lag and window size of 100 ms and 450 ms, respectively.
The highest mean PCC values of 0.790, 0.795, and 0.637 are achieved using
sensor-domain features, while 0.769, 0.777, and 0.647 are achieved using
source-domain features in x, y, and z-directions, respectively. This study
explores the feasibility of trajectory prediction using EEG sensor-domain and
source-domain EEG features for the grasp-and-lift task. Furthermore,
inter-subject trajectory estimation is performed using the proposed deep
learning decoder with EEG source domain features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An LSTM Feature Imitation Network for Hand Movement Recognition from
  sEMG Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19356v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19356v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuheng Wu, S. Farokh Atashzar, Mohammad M. Ghassemi, Tuka Alhanai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface Electromyography (sEMG) is a non-invasive signal that is used in the
recognition of hand movement patterns, the diagnosis of diseases, and the
robust control of prostheses. Despite the remarkable success of recent
end-to-end Deep Learning approaches, they are still limited by the need for
large amounts of labeled data. To alleviate the requirement for big data, we
propose utilizing a feature-imitating network (FIN) for closed-form temporal
feature learning over a 300ms signal window on Ninapro DB2, and applying it to
the task of 17 hand movement recognition. We implement a lightweight LSTM-FIN
network to imitate four standard temporal features (entropy, root mean square,
variance, simple square integral). We observed that the LSTM-FIN network can
achieve up to 99\% R2 accuracy in feature reconstruction and 80\% accuracy in
hand movement recognition. Our results also showed that the model can be
robustly applied for both within- and cross-subject movement recognition, as
well as simulated low-latency environments. Overall, our work demonstrates the
potential of the FIN modeling paradigm in data-scarce scenarios for sEMG signal
processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2025 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Mixture-of-Agents for Edge Inference with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Purbesh Mitra, Priyanka Kaswan, Sennur Ulukus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Agents (MoA) has recently been proposed as a method to enhance
performance of large language models (LLMs), enabling multiple individual LLMs
to work together for collaborative inference. This collaborative approach
results in improved responses to user prompts compared to relying on a single
LLM. In this paper, we consider such an MoA architecture in a distributed
setting, where LLMs operate on individual edge devices, each uniquely
associated with a user and equipped with its own distributed computing power.
These devices exchange information using decentralized gossip algorithms,
allowing different device nodes to talk without the supervision of a
centralized server. In the considered setup, different users have their own LLM
models to address user prompts. Additionally, the devices gossip either their
own user-specific prompts or augmented prompts to generate more refined answers
to certain queries. User prompts are temporarily stored in the device queues
when their corresponding LLMs are busy. Given the memory limitations of edge
devices, it is crucial to ensure that the average queue sizes in the system
remain bounded. In this paper, we address this by theoretically calculating the
queuing stability conditions for the device queues under reasonable
assumptions, which we validate experimentally as well. Further, we demonstrate
through experiments, leveraging open-source LLMs for the implementation of
distributed MoA, that certain MoA configurations produce higher-quality
responses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The
implementation is available at:
https://github.com/purbeshmitra/distributed_moa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel DNA Sequence Alignment on High-Performance Systems with CUDA
  and MPI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.21103v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.21103v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linus Zwaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence alignment is a cornerstone of bioinformatics, widely used to
identify similarities between DNA, RNA, and protein sequences and studying
evolutionary relationships and functional properties. The Needleman-Wunsch
algorithm remains a robust and accurate method for global sequence alignment.
However, its computational complexity, O(mn), poses significant challenges when
processing large-scale datasets or performing multiple sequence alignments. To
address these limitations, a hybrid implementation of the Needleman-Wunsch
algorithm that leverages CUDA for parallel execution on GPUs and MPI for
distributed computation across multiple nodes on a supercomputer is proposed.
CUDA efficiently offloads computationally intensive tasks to GPU cores, while
MPI enables communication and workload distribution across nodes to handle
large-scale alignments.
  This work details the implementation and performance evaluation of the
Needleman-Wunsch algorithm in a massively parallel computing environment.
Experimental results demonstrate significant acceleration of the alignment
process compared to traditional CPU-based implementations, particularly for
large input sizes and multiple sequence alignments. In summary, the combination
of CUDA and MPI effectively overcomes the computational bottlenecks inherent to
the Needleman-Wunsch algorithm without requiring substantial modifications to
the underlying algorithm, highlighting the potential of high-performance
computing in advancing sequence alignment workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Heuristics for Scheduling DNN Inferencing on Edge and Cloud for
  Personalized UAV Fleets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suman Raj, Radhika Mittal, Harshil Gupta, Yogesh Simmhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drone fleets with onboard cameras coupled with computer vision and DNN
inferencing models can support diverse applications. One such novel domain is
for one or more buddy drones to assist Visually Impaired People (VIPs) lead an
active lifestyle. Video inferencing tasks from such drones can help both
navigate the drone and provide situation awareness to the VIP, and hence have
strict execution deadlines. We propose a deadline-driven heuristic, DEMS-A, to
schedule diverse DNN tasks generated continuously to perform inferencing over
video segments generated by multiple drones linked to an edge, with the option
to execute on the cloud. We use strategies like task dropping, work stealing
and migration, and dynamic adaptation to cloud variability, to guarantee a
Quality of Service (QoS), i.e. maximize the utility and the number of tasks
completed. We also introduce an additional Quality of Experience (QoE) metric
useful to the assistive drone domain, which values the frequency of success for
task types to ensure the responsiveness and reliability of the VIP application.
We extend our DEMS solution to GEMS to solve this. We evaluate these
strategies, using (i) an emulated setup of a fleet of over 80 drones supporting
over 25 VIPs, with real DNN models executing on pre-recorded drone video
streams, using Jetson Nano edges and AWS Lambda cloud functions, and (ii) a
real-world setup of a Tello drone and a Jetson Orin Nano edge generating drone
commands to follow a VIP in real-time. Our strategies present a task completion
rate of up to 88%, up to 2.7x higher QoS utility compared to the baselines, a
further 16% higher QoS utility while adapting to network variability, and up to
75% higher QoE utility. Our practical validation exhibits task completion of up
to 87% for GEMS and 33% higher total utility of GEMS compared to edge-only.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FastCHGNet: Training one Universal Interatomic Potential to 1.5 Hours
  with 32 GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanchang Zhou, Siyu Hu, Chen Wang, Lin-Wang Wang, Guangming Tan, Weile Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural network universal interatomic potentials (GNN-UIPs) have
demonstrated remarkable generalization and transfer capabilities in material
discovery and property prediction. These models can accelerate molecular
dynamics (MD) simulation by several orders of magnitude while maintaining
\textit{ab initio} accuracy, making them a promising new paradigm in material
simulations. One notable example is Crystal Hamiltonian Graph Neural Network
(CHGNet), pretrained on the energies, forces, stresses, and magnetic moments
from the MPtrj dataset, representing a state-of-the-art GNN-UIP model for
charge-informed MD simulations. However, training the CHGNet model is
time-consuming(8.3 days on one A100 GPU) for three reasons: (i) requiring
multi-layer propagation to reach more distant atom information, (ii) requiring
second-order derivatives calculation to finish weights updating and (iii) the
implementation of reference CHGNet does not fully leverage the computational
capabilities. This paper introduces FastCHGNet, an optimized CHGNet, with three
contributions: Firstly, we design innovative Force/Stress Readout modules to
decompose Force/Stress prediction. Secondly, we adopt massive optimizations
such as kernel fusion, redundancy bypass, etc, to exploit GPU computation power
sufficiently. Finally, we extend CHGNet to support multiple GPUs and propose a
load-balancing technique to enhance GPU utilization. Numerical results show
that FastCHGNet reduces memory footprint by a factor of 3.59. The final
training time of FastCHGNet can be decreased to \textbf{1.53 hours} on 32 GPUs
without sacrificing model accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Blockchain</span>-Empowered Cyber-Secure Federated Learning for <span class="highlight-title">Trust</span>worthy
  Edge Computing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ervin Moore, Ahmed Imteaj, Md Zarif Hossain, Shabnam Rezapour, M. Hadi Amini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a privacy-preserving distributed machine learning
scheme, where each participant data remains on the participating devices and
only the local model generated utilizing the local computational power is
transmitted throughout the database. However, the distributed computational
nature of FL creates the necessity to develop a mechanism that can remotely
trigger any network agents, track their activities, and prevent threats to the
overall process posed by malicious participants. Particularly, the FL paradigm
may become vulnerable due to an active attack from the network participants,
called a poisonous attack. In such an attack, the malicious participant acts as
a benign agent capable of affecting the global model quality by uploading an
obfuscated poisoned local model update to the server. This paper presents a
cross-device FL model that ensures trustworthiness, fairness, and authenticity
in the underlying FL training process. We leverage trustworthiness by
constructing a reputation-based trust model based on contributions of agents
toward model convergence. We ensure fairness by identifying and removing
malicious agents from the training process through an outlier detection
technique. Further, we establish authenticity by generating a token for each
participating device through a distributed sensing mechanism and storing that
unique token in a blockchain smart contract. Further, we insert the trust
scores of all agents into a blockchain and validate their reputations using
various consensus mechanisms that consider the computational task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning with MMD-based Early Stopping for Adaptive GNSS
  Interference Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nishant S. Gaikwad, Lucas Heublein, Nisha L. Raichur, Tobias Feigl, Christopher Mutschler, Felix Ott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enables multiple devices to collaboratively train a
global model while maintaining data on local servers. Each device trains the
model on its local server and shares only the model updates (i.e., gradient
weights) during the aggregation step. A significant challenge in FL is managing
the feature distribution of novel and unbalanced data across devices. In this
paper, we propose an FL approach using few-shot learning and aggregation of the
model weights on a global server. We introduce a dynamic early stopping method
to balance out-of-distribution classes based on representation learning,
specifically utilizing the maximum mean discrepancy of feature embeddings
between local and global models. An exemplary application of FL is to
orchestrate machine learning models along highways for interference
classification based on snapshots from global navigation satellite system
(GNSS) receivers. Extensive experiments on four GNSS datasets from two
real-world highways and controlled environments demonstrate that our FL method
surpasses state-of-the-art techniques in adapting to both novel interference
classes and multipath scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Git repository:
  https://gitlab.cc-asp.fraunhofer.de/darcy_gnss/federated_learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling data distribution for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyuan Zhao, Hanlin Gu, Lixin Fan, Yuxing Han, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) facilitates collaborative training of a global model
whose performance is boosted by private data owned by distributed clients,
without compromising data privacy. Yet the wide applicability of FL is hindered
by entanglement of data distributions across different clients. This paper
demonstrates for the first time that by disentangling data distributions FL can
in principle achieve efficiencies comparable to those of distributed systems,
requiring only one round of communication. To this end, we propose a novel
FedDistr algorithm, which employs stable diffusion models to decouple and
recover data distributions. Empirical results on the CIFAR100 and DomainNet
datasets show that FedDistr significantly enhances model utility and efficiency
in both disentangled and near-disentangled scenarios while ensuring privacy,
outperforming traditional federated learning methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A High Energy-Efficiency Multi-core Neuromorphic Architecture for Deep
  SNN Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjing Li, Huihui Zhou, Xiaofeng Xu, Zhiwei Zhong, Puli Quan, Xueke Zhu, Yanyu Lin, Wenjie Lin, Hongyu Guo, Junchao Zhang, Yunhao Ma, Wei Wang, Qingyan Meng, Zhengyu Ma, Guoqi Li, Xiaoxin Cui, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a growing necessity for edge training to adapt to dynamically
changing environment. Neuromorphic computing represents a significant pathway
for high-efficiency intelligent computation in energy-constrained edges, but
existing neuromorphic architectures lack the ability of directly training
spiking neural networks (SNNs) based on backpropagation. We develop a
multi-core neuromorphic architecture with Feedforward-Propagation,
Back-Propagation, and Weight-Gradient engines in each core, supporting high
efficient parallel computing at both the engine and core levels. It combines
various data flows and sparse computation optimization by fully leveraging the
sparsity in SNN training, obtaining a high energy efficiency of 1.05TFLOPS/W@
FP16 @ 28nm, 55 ~ 85% reduction of DRAM access compared to A100 GPU in SNN
trainings, and a 20-core deep SNN training and a 5-worker federated learning on
FPGAs. Our study develops the first multi-core neuromorphic architecture
supporting the direct SNN training, facilitating the neuromorphic computing in
edge-learnable applications.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-29T00:00:00Z">2024-12-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Cryptography and Security <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Identity and <span class="highlight-title">Access</span> Management in Multiple Cloud
  Environments: Models, Issues, and Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfredo Cuzzocrea, Islam Belmerabet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses the attention on privacy-preserving identity and access
management in multiple Cloud environments, which is an annoying problem in the
modern big data era. Within this conceptual context, the paper describes
contemporaneous models and issues, and put the basis for future solid
solutions. Finally, we provide a summary table where we embed an innovative
taxonomy of state-of-the-art research proposals in the reference scientific
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attacks on the neural network and defense methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Korenev, G. Belokrylov, B. Lodonova, A. Novokhrestov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article will discuss the use of attacks on a neural network trained on
audio data, as well as possible methods of protection against these attacks.
FGSM, PGD and CW attacks, as well as data poisoning, will be considered. Within
the framework of protection, Art-IBM and advertorch libraries will be
considered. The obtained accuracy metrics within the framework of attack
applications are presented
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multiparty Homomorphic Encryption Approach to Confidential Federated
  Kaplan Meier Survival Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narasimha Raghavan Veeraragavan, Svetlana Boudko, Jan Franz Nygård
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of healthcare data has expanded opportunities for
collaborative research, yet stringent privacy regulations hinder pooling
sensitive patient records. We propose a \emph{multiparty homomorphic
encryption-based} framework for \emph{privacy-preserving federated
Kaplan--Meier survival analysis}, offering native floating-point support, a
theoretical model, and explicit reconstruction-attack mitigation. Compared to
prior work, our framework ensures encrypted federated survival estimates
closely match centralized outcomes, supported by formal utility-loss bounds
that demonstrate convergence as aggregation and decryption noise diminish.
Extensive experiments on the NCCTG Lung Cancer and synthetic Breast Cancer
datasets confirm low \emph{mean absolute error (MAE)} and \emph{root mean
squared error (RMSE)}, indicating negligible deviations between encrypted and
non-encrypted survival curves. Log-rank and numerical accuracy tests reveal
\emph{no significant difference} between federated encrypted and non-encrypted
analyses, preserving statistical validity. A reconstruction-attack evaluation
shows smaller federations (2--3 providers) with overlapping data between the
institutions are vulnerable, a challenge mitigated by multiparty encryption.
Larger federations (5--50 sites) degrade reconstruction accuracy further, with
encryption improving confidentiality. Despite an 8--19$\times$ computational
overhead, threshold-based homomorphic encryption is \emph{feasible for
moderate-scale deployments}, balancing security and runtime. By providing
robust privacy guarantees alongside high-fidelity survival estimates, our
framework advances the state-of-the art in secure multi-institutional survival
analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cut the Deadwood Out: Post-Training Model Purification with Selective
  Module Substitution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Tong, Weijun Li, Xuanli He, Haolan Zhan, Qiongkai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of DNNs often depends on training with large-scale datasets, but
building such datasets is both expensive and challenging. Consequently, public
datasets from open-source platforms like HuggingFace have become popular,
posing significant risks of data poisoning attacks. Existing backdoor defenses
in NLP primarily focus on identifying and removing poisoned samples; however,
purifying a backdoored model with these sample-cleaning approaches typically
requires expensive retraining. Therefore, we propose Greedy Module Substitution
(GMS), which identifies and substitutes ''deadwood'' modules (i.e., components
critical to backdoor pathways) in a backdoored model to purify it. Our method
relaxes the common dependency of prior model purification methods on clean
datasets or clean auxiliary models. When applied to RoBERTa-large under
backdoor attacks, GMS demonstrates strong effectiveness across various
settings, particularly against widely recognized challenging attacks like LWS,
achieving a post-purification attack success rate (ASR) of 9.7% on SST-2
compared to 58.8% for the best baseline approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sub-optimal Learning in Meta-Classifier Attacks: A Study of Membership
  Inference on Differentially Private Location Aggregates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhan Liu, Florent Guepin, Igor Shilov, Yves-Alexandre De Montjoye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread collection and sharing of location data, even in aggregated
form, raises major privacy concerns. Previous studies used
meta-classifier-based membership inference attacks~(MIAs) with multi-layer
perceptrons~(MLPs) to estimate privacy risks in location data, including when
protected by differential privacy (DP). In this work, however, we show that a
significant gap exists between the expected attack accuracy given by DP and the
empirical attack accuracy even with informed attackers (also known as DP
attackers), indicating a potential underestimation of the privacy risk. To
explore the potential causes for the observed gap, we first propose two new
metric-based MIAs: the one-threshold attack and the two-threshold attack. We
evaluate their performances on real-world location data and find that different
data distributions require different attack strategies for optimal performance:
the one-threshold attack is more effective with Gaussian DP noise, while the
two-threshold attack performs better with Laplace DP noise. Comparing their
performance with one of the MLP-based attack models in previous works shows
that the MLP only learns the one-threshold rule, leading to a suboptimal
performance under the Laplace DP noise and an underestimation of the privacy
risk. Second, we theoretically prove that MLPs can encode complex rules~(\eg,
the two-threshold attack rule), which can be learned when given a substantial
amount of training data. We conclude by discussing the implications of our
findings in practice, including broader applications extending beyond location
aggregates to any differentially private datasets containing multiple
observations per individual and how techniques such as synthetic data
generation and pre-training might enable MLP to learn more complex optimal
rules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cool, But What About Oracles? An Oracle-Based Perspective on <span class="highlight-title">Blockchain</span>
  Integration in the Accounting Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio Caldarelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bitcoin Network is a sophisticated accounting system that allows its
underlying cryptocurrency to be trusted even in the absence of a reliable
financial authority. Given its undeniable success, the technology, generally
referred to as blockchain, has also been proposed as a means to improve legacy
accounting systems. Accounting for real-world data, however, requires the
intervention of a third party known as an Oracle, which, having not the same
characteristics as a blockchain, could potentially reduce the expected
integration benefit. Through a systematic review of the literature, this study
aims to investigate whether the papers concerning blockchain integration in
accounting consider and address the limitations posed by oracles. A broad
overview of the limitations that emerged in the literature is provided and
distinguished according to the specific accounting integration. Results support
the view that although research on the subject counts numerous articles, actual
studies considering oracle limitations are lacking. Interestingly, despite the
scarce production of papers addressing oracles in various accounting sectors,
reporting for ESG already shows interesting workarounds for oracle limitations,
with permissioned chains envisioned as a valid support for the safe storage of
sustainability data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript is not Proofread. Some tables and figures, as well as
  paragraph content may be subject to change in the journal version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multidisciplinary Approach to Telegram Data Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Velizar Varbanov, Kalin Kopanov, Tatiana Atanasova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a multidisciplinary approach to analyzing data from
Telegram for early warning information regarding cyber threats. With the
proliferation of hacktivist groups utilizing Telegram to disseminate
information regarding future cyberattacks or to boast about successful ones,
the need for effective data analysis methods is paramount. The primary
challenge lies in the vast number of channels and the overwhelming volume of
data, necessitating advanced techniques for discerning pertinent risks amidst
the noise. To address this challenge, we employ a combination of neural network
architectures and traditional machine learning algorithms. These methods are
utilized to classify and identify potential cyber threats within the Telegram
data. Additionally, sentiment analysis and entity recognition techniques are
incorporated to provide deeper insights into the nature and context of the
communicated information. The study evaluates the effectiveness of each method
in detecting and categorizing cyber threats, comparing their performance and
identifying areas for improvement. By leveraging these diverse analytical
tools, we aim to enhance early warning systems for cyber threats, enabling more
proactive responses to potential security breaches. This research contributes
to the ongoing efforts to bolster cybersecurity measures in an increasingly
interconnected digital landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 table, 2 figures, 24th International Multidisciplinary
  Scientific GeoConference SGEM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segmented Private Data Aggregation in the Multi-message Shuffle Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19639v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19639v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaowei Wang, Hongqiao Chen, Sufen Zeng, Ruilin Yang, Hui Jiang, Peigen Ye, Kaiqi Yu, Rundong Mei, Shaozheng Huang, Wei Yang, Bangzhou Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The shuffle model of differential privacy (DP) offers compelling
privacy-utility trade-offs in decentralized settings (e.g., internet of things,
mobile edge networks). Particularly, the multi-message shuffle model, where
each user may contribute multiple messages, has shown that accuracy can
approach that of the central model of DP. However, existing studies typically
assume a uniform privacy protection level for all users, which may deter
conservative users from participating and prevent liberal users from
contributing more information, thereby reducing the overall data utility, such
as the accuracy of aggregated statistics. In this work, we pioneer the study of
segmented private data aggregation within the multi-message shuffle model of
DP, introducing flexible privacy protection for users and enhanced utility for
the aggregation server. Our framework not only protects users' data but also
anonymizes their privacy level choices to prevent potential data leakage from
these choices. To optimize the privacy-utility-communication trade-offs, we
explore approximately optimal configurations for the number of blanket messages
and conduct almost tight privacy amplification analyses within the shuffle
model. Through extensive experiments, we demonstrate that our segmented
multi-message shuffle framework achieves a reduction of about 50\% in
estimation error compared to existing approaches, significantly enhancing both
privacy and utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix typo in an author's name</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Post-<span class="highlight-title">Quantum</span> Cryptography(PQC): Generalized ElGamal Cipher over
  GL(8,F251) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1702.03587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1702.03587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Hecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-quantum cryptography (PQC) attempts to find cryptographic protocols
resistant to attacks using for instance Shor's polynomial time algorithm for
numerical field problems like integer factorization (IFP) or the discrete
logarithm (DLP). Other aspects are the backdoors discovered in deterministic
random generators or recent advances in solving some instances of DLP. Using
alternative algebraic structures like non-commutative or non-associative
partial groupoids, magmas, monoids, semigroups, quasigroups or groups, are
valid choices for these new protocols. This paper focuses on an asymmetric
cipher based on a generalized ElGamal non-arbitrated protocol using a
non-commutative general linear group. The developed protocol forces a hard
subgroup membership search problem into a non-commutative structure. The
protocol involves at first a generalized Diffie-Hellman key interchange and
further on the private and public parameters are recursively updated each time
a new cipher session is launched. Security is based on a hard variation of the
Generalized Symmetric Decomposition Problem (GSDP). Working with GL(8, F251)
64-bit security is achieved, and if GL(16, F251) is chosen, the security rises
to 127-bit. An appealing feature is that there is no need for big number
libraries as all arithmetic is performed in Z_251. Therefore the new protocol
is particularly useful for computational platforms with very limited
capabilities like smartphones or smartcards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 Tables, 14 Figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs
  via Bidirectional Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongwu Wang, Fangxin Liu, Mingshuai Li, Li Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient parallelization of Large Language Models (LLMs) with long sequences
is essential but challenging due to their significant computational and memory
demands, particularly stemming from communication bottlenecks in attention
mechanisms. While sequence parallelism (SP) has been introduced as a potential
solution, existing methods often suffer from limited scalability or
inefficiency, rendering their effectiveness.
  Ring-Attention demonstrates the potential for scaling sequence processing but
faces significant limitations due to its reliance on peer-to-peer (P2P)
communication and inefficient utilization of network resources. As the degree
of SP increases, the quadratic decrease in computation time per step contrasts
sharply with the linear reduction in communication volume, exacerbating
communication bottlenecks. To address these challenges, we propose TokenRing, a
fine-grained parallel framework that leverages bidirectional P2P communication
to effectively overlap computation and data transmission. By partitioning the
attention block and concurrently transmitting Query and block outputs (i.e.,
$block\_out$ and $block\_lse$) within a fully connected mesh topology,
TokenRing achieves significant reductions in communication overhead and better
load balancing. These innovations improve the scalability and efficiency of
distributed Transformer models, particularly for long-context sequences.
Experimental results demonstrate that TokenRing enhances throughput and reduces
communication latency. Moreover, its design adapts seamlessly to various
multi-GPU interconnect solutions, such as Huawei Ascend, ensuring broad
compatibility and cost-effectiveness for distributed LLM inference and
training. The code is available at:
\url{https://github.com/ACA-Lab-SJTU/token-ring}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeutronTP: Load-Balanced Distributed Full-Graph GNN Training with Tensor
  Parallelism <span class="chip">VLDB2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ai, Hao Yuan, Zeyu Ling, Qiange Wang, Yanfeng Zhang, Zhenbo Fu, Chaoyi Chen, Yu Gu, Ge Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have emerged as a promising direction. Training
large-scale graphs that relies on distributed computing power poses new
challenges. Existing distributed GNN systems leverage data parallelism by
partitioning the input graph and distributing it to multiple workers. However,
due to the irregular nature of the graph structure, existing distributed
approaches suffer from unbalanced workloads and high overhead in managing
cross-worker vertex dependencies. In this paper, we leverage tensor parallelism
for distributed GNN training. GNN tensor parallelism eliminates cross-worker
vertex dependencies by partitioning features instead of graph structures.
Different workers are assigned training tasks on different feature slices with
the same dimensional size, leading to a complete load balance. We achieve
efficient GNN tensor parallelism through two critical functions. Firstly, we
employ a generalized decoupled training framework to decouple NN operations
from graph aggregation operations, significantly reducing the communication
overhead caused by NN operations which must be computed using complete
features. Secondly, we employ a memory-efficient task scheduling strategy to
support the training of large graphs exceeding single GPU memory, while further
improving performance by overlapping communication and computation. By
integrating the above techniques, we propose a distributed GNN training system
NeutronTP. Our experimental results on a 16-node Aliyun cluster demonstrate
that NeutronTP achieves 1.29X-8.72X speedup over state-of-the-art GNN systems
including DistDGL, NeutronStar, and Sancus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages 16 figures, VLDB2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Hybrid Sketching for $\ell_2$-Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neophytos Charalambides, Arya Mazumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear algebraic operations are ubiquitous in engineering applications, and
arise often in a variety of fields including statistical signal processing and
machine learning. With contemporary large datasets, to perform linear algebraic
methods and regression tasks, it is necessary to resort to both distributed
computations as well as data compression. In this paper, we study
\textit{distributed} $\ell_2$-subspace embeddings, a common technique used to
efficiently perform linear regression. In our setting, data is distributed
across multiple computing nodes and a goal is to minimize communication between
the nodes and the coordinator in the distributed centralized network, while
maintaining the geometry of the dataset. Furthermore, there is also the concern
of keeping the data private and secure from potential adversaries. In this
work, we address these issues through randomized sketching, where the key idea
is to apply distinct sketching matrices on the local datasets. A novelty of
this work is that we also consider \textit{hybrid sketching}, \textit{i.e.} a
second sketch is applied on the aggregated locally sketched datasets, for
enhanced embedding results. One of the main takeaways of this work is that by
hybrid sketching, we can interpolate between the trade-offs that arise in
off-the-shelf sketching matrices. That is, we can obtain gains in terms of
embedding dimension or multiplication time. Our embedding arguments are also
justified numerically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asynchronous Federated Clustering with Unknown Number of Clusters <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Zhang, Yiqun Zhang, Yang Lu, Mengke Li, Xi Chen, Yiu-ming Cheung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Clustering (FC) is crucial to mining knowledge from unlabeled
non-Independent Identically Distributed (non-IID) data provided by multiple
clients while preserving their privacy. Most existing attempts learn cluster
distributions at local clients, and then securely pass the desensitized
information to the server for aggregation. However, some tricky but common FC
problems are still relatively unexplored, including the heterogeneity in terms
of clients' communication capacity and the unknown number of proper clusters
$k^*$. To further bridge the gap between FC and real application scenarios,
this paper first shows that the clients' communication asynchrony and unknown
$k^*$ are complex coupling problems, and then proposes an Asynchronous
Federated Cluster Learning (AFCL) method accordingly. It spreads the excessive
number of seed points to the clients as a learning medium and coordinates them
across the clients to form a consensus. To alleviate the distribution imbalance
cumulated due to the unforeseen asynchronous uploading from the heterogeneous
clients, we also design a balancing mechanism for seeds updating. As a result,
the seeds gradually adapt to each other to reveal a proper number of clusters.
Extensive experiments demonstrate the efficacy of AFCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GreenLLM: Disaggregating Large Language Model Serving on Heterogeneous
  GPUs for Lower Carbon Emissions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyao Shi, Yanran Wu, Sihang Liu, Yi Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs have been widely adopted across many real-world applications. However,
their widespread use comes with significant environmental costs due to their
high computational intensity and resource demands. Specifically, this has
driven the development of new generations of high-performing GPUs, exacerbating
the problem of electronic waste and accelerating the premature disposal of
devices. To address this problem, this paper focuses on reducing the carbon
emissions of LLM serving by reusing older, low-performing GPUs. We present
GreenLLM, an SLO-aware LLM serving framework designed to minimize carbon
emissions by reusing older GPUs. GreenLLM builds on two identified use cases
that disaggregate specific computations onto older GPUs, reducing carbon
emissions while meeting performance goals. To deepen our understanding of the
potential carbon savings from disaggregation, we also provide a theoretical
analysis of its relationship with carbon intensity and GPU lifetime. Our
evaluations show that GreenLLM reduces carbon emissions by up to 40.6% compared
to running standard LLM serving on new GPU only, meeting latency SLOs for over
90% of requests across various applications, latency requirements, carbon
intensities, and GPU lifetimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Analysis of Federated Learning with Arbitrary Client
  Participation <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13648v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13648v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqiang Wang, Mingyue Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) faces challenges of intermittent client availability
and computation/communication efficiency. As a result, only a small subset of
clients can participate in FL at a given time. It is important to understand
how partial client participation affects convergence, but most existing works
have either considered idealized participation patterns or obtained results
with non-zero optimality error for generic patterns. In this paper, we provide
a unified convergence analysis for FL with arbitrary client participation. We
first introduce a generalized version of federated averaging (FedAvg) that
amplifies parameter updates at an interval of multiple FL rounds. Then, we
present a novel analysis that captures the effect of client participation in a
single term. By analyzing this term, we obtain convergence upper bounds for a
wide range of participation patterns, including both non-stochastic and
stochastic cases, which match either the lower bound of stochastic gradient
descent (SGD) or the state-of-the-art results in specific settings. We also
discuss various insights, recommendations, and experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at NeurIPS 2022. This latest version includes a minor fix
  of Step (a) in Equation (C.15) in the proof, which only affects a numerical
  constant in the learning rate choice for the theory. The convergence bounds
  expressed in $\mathcal{O}(\cdot)$ as well as all the main findings and
  conclusions remain the same</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Variational Autoencoder: a Barycentric View <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peijie Qiu, Wenhui Zhu, Sayantan Kumar, Xiwen Chen, Xiaotong Sun, Jin Yang, Abolfazl Razi, Yalin Wang, Aristeidis Sotiras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple signal modalities, such as vision and sounds, are naturally present
in real-world phenomena. Recently, there has been growing interest in learning
generative models, in particular variational autoencoder (VAE), to for
multimodal representation learning especially in the case of missing
modalities. The primary goal of these models is to learn a modality-invariant
and modality-specific representation that characterizes information across
multiple modalities. Previous attempts at multimodal VAEs approach this mainly
through the lens of experts, aggregating unimodal inference distributions with
a product of experts (PoE), a mixture of experts (MoE), or a combination of
both. In this paper, we provide an alternative generic and theoretical
formulation of multimodal VAE through the lens of barycenter. We first show
that PoE and MoE are specific instances of barycenters, derived by minimizing
the asymmetric weighted KL divergence to unimodal inference distributions. Our
novel formulation extends these two barycenters to a more flexible choice by
considering different types of divergences. In particular, we explore the
Wasserstein barycenter defined by the 2-Wasserstein distance, which better
preserves the geometry of unimodal distributions by capturing both
modality-specific and modality-invariant representations compared to KL
divergence. Empirical studies on three multimodal benchmarks demonstrated the
effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movable Antenna Array Aided Ultra Reliable Covert Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Wang, Guojie Hu, Xiaoling Hu, Xingbo Lu, Yuzhen Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we construct a framework of the movable antenna (MA) aided
covert communication shielded by the general noise uncertainty for the first
time. According to the analysis performance on the derived closed-form
expressions of the sum of the probabilities of the detection errors and the
communication outage probability, the perfect covertness and the ultra
reliability can be achieved by adjusting the antenna position in the MA array.
Then, we formulate the communication covertness maximization problem with the
constraints of the ultra reliability and the independent discrete movable
position to optimize the transmitter's parameter. With the maximal ratio
transmitting (MRT) design for the beamforming, we solve the closed-form optimal
information transmit power and design a lightweight discrete projected gradient
descent (DPGD) algorithm to determine the optimal antenna position. The
numerical results show that the optimal achievable covertness and the feasible
region of the steering angle with the MA array is significant larger than the
one with the fixed-position antenna (FPA) array.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>has been presented in IEEE GLOBECOM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Hybrid Sketching for $\ell_2$-Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neophytos Charalambides, Arya Mazumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear algebraic operations are ubiquitous in engineering applications, and
arise often in a variety of fields including statistical signal processing and
machine learning. With contemporary large datasets, to perform linear algebraic
methods and regression tasks, it is necessary to resort to both distributed
computations as well as data compression. In this paper, we study
\textit{distributed} $\ell_2$-subspace embeddings, a common technique used to
efficiently perform linear regression. In our setting, data is distributed
across multiple computing nodes and a goal is to minimize communication between
the nodes and the coordinator in the distributed centralized network, while
maintaining the geometry of the dataset. Furthermore, there is also the concern
of keeping the data private and secure from potential adversaries. In this
work, we address these issues through randomized sketching, where the key idea
is to apply distinct sketching matrices on the local datasets. A novelty of
this work is that we also consider \textit{hybrid sketching}, \textit{i.e.} a
second sketch is applied on the aggregated locally sketched datasets, for
enhanced embedding results. One of the main takeaways of this work is that by
hybrid sketching, we can interpolate between the trade-offs that arise in
off-the-shelf sketching matrices. That is, we can obtain gains in terms of
embedding dimension or multiplication time. Our embedding arguments are also
justified numerically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coprime Bivariate Bicycle Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10001v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10001v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Wang, Frank Mueller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work (1) proposes a novel numerical algorithm to accelerate the search
process for good Bivariate Bicycle (BB) codes and (2) defines a new subclass of
BB codes suitable for quantum error correction. The proposed acceleration
search algorithm reduces the search space by excluding some equivalent codes
from the search space, as well as setting thresholds to drop bad codes at an
early stage. A number of new BB codes found by this algorithm are reported. The
proposed subclass of BB codes employs coprimes to construct groups via
polynomials as the basis for the BB code, rather than using the standard BB
codes with unconstrained constructors. In contrast to vanilla BB codes, where
parameters remain unknown prior to code discovery, the rate of the proposed
code can be determined beforehand by specifying a factor polynomial as an input
to the numerical search algorithm. Using this coprime BB construction, we found
a number of surprisingly short to medium-length codes that were previously
unknown.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Overhead Channel Estimation via 3D Extrapolation for TDD mmWave
  Massive MIMO Systems Under High-Mobility Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binggui Zhou, Xi Yang, Shaodan Ma, Feifei Gao, Guanghua Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In time division duplexing (TDD) millimeter wave (mmWave) massive
multiple-input multiple-output (MIMO) systems, downlink channel state
information (CSI) can be obtained from uplink channel estimation thanks to
channel reciprocity. However, under high-mobility scenarios, frequent uplink
channel estimation is needed due to channel aging. Additionally, large amounts
of antennas and subcarriers result in high-dimensional CSI matrices,
aggravating pilot training overhead. To address this, we propose a three-domain
(3D) channel extrapolation framework across spatial, frequency, and temporal
domains. First, considering the effectiveness of traditional knowledge-driven
channel estimation methods and the marginal effects of pilots in the spatial
and frequency domains, a knowledge-and-data driven spatial-frequency channel
extrapolation network (KDD-SFCEN) is proposed for uplink channel estimation via
joint spatial-frequency channel extrapolation to reduce spatial-frequency
domain pilot overhead. Then, leveraging channel reciprocity and temporal
dependencies, we propose a temporal uplink-downlink channel extrapolation
network (TUDCEN) powered by generative artificial intelligence for slot-level
channel extrapolation, aiming to reduce the tremendous temporal domain pilot
overhead caused by high mobility. Numerical results demonstrate the superiority
of the proposed framework in significantly reducing the pilot training overhead
by 16 times and improving the system's spectral efficiency under high-mobility
scenarios compared with state-of-the-art channel estimation/extrapolation
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures, 3 tables. Accepted by IEEE Transactions on
  Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A physics-compliant diagonal representation for wireless channels
  parametrized by beyond-diagonal reconfigurable intelligent surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp del Hougne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The parametrization of wireless channels by so-called "beyond-diagonal
reconfigurable intelligent surfaces" (BD-RIS) is mathematically characterized
by a matrix whose off-diagonal entries are partially or fully populated.
Physically, this corresponds to tunable coupling mechanisms between the RIS
elements that originate from the RIS control circuit. Here, we derive a
physics-compliant diagonal representation for BD-RIS-parametrized channels. We
recognize that any RIS control circuit can always be separated into its static
parts (SLC) and a set of tunable individual loads (IL). Therefore, a
BD-RIS-parametrized channel results from the chain cascade of three systems: i)
radio environment (RE), ii) SLC, and iii) IL. RE and SLC are static
non-diagonal systems whose cascade K is terminated by the tunable diagonal
system IL. This physics-compliant representation in terms of K and IL is
directly analogous to that for conventional ("diagonal") RIS (D-RIS).
Therefore, scenarios with BD-RIS can also readily be captured by the
physics-compliant coupled-dipole model PhysFad, as we show. In addition,
physics-compliant algorithms for system-level optimization with D-RIS can be
directly applied to scenarios with BD-RIS. We demonstrate this important
implication of our conceptual finding in a case study on end-to-end channel
estimation and optimization in a BD-RIS-parametrized rich-scattering
environment. Our case study is the first experimentally grounded system-level
optimization for BD-RIS: We obtain the characteristics of RE and IL from
experimental measurements and a commercial PIN diode, respectively. Altogether,
our physics-compliant diagonal representation for BD-RIS enables a paradigm
shift in how practitioners in wireless communications and signal processing
implement system-level optimizations for BD-RIS because it enables them to
directly apply existing physics-compliant D-RIS algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, submitted to an IEEE Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Analysis of Federated Learning with Arbitrary Client
  Participation <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13648v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13648v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqiang Wang, Mingyue Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) faces challenges of intermittent client availability
and computation/communication efficiency. As a result, only a small subset of
clients can participate in FL at a given time. It is important to understand
how partial client participation affects convergence, but most existing works
have either considered idealized participation patterns or obtained results
with non-zero optimality error for generic patterns. In this paper, we provide
a unified convergence analysis for FL with arbitrary client participation. We
first introduce a generalized version of federated averaging (FedAvg) that
amplifies parameter updates at an interval of multiple FL rounds. Then, we
present a novel analysis that captures the effect of client participation in a
single term. By analyzing this term, we obtain convergence upper bounds for a
wide range of participation patterns, including both non-stochastic and
stochastic cases, which match either the lower bound of stochastic gradient
descent (SGD) or the state-of-the-art results in specific settings. We also
discuss various insights, recommendations, and experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at NeurIPS 2022. This latest version includes a minor fix
  of Step (a) in Equation (C.15) in the proof, which only affects a numerical
  constant in the learning rate choice for the theory. The convergence bounds
  expressed in $\mathcal{O}(\cdot)$ as well as all the main findings and
  conclusions remain the same</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 6G: The Intelligent Network of Everything 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.09398v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.09398v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harri Pennanen, Tuomo Hänninen, Oskari Tervo, Antti Tölli, Matti Latva-aho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global 6G vision has taken its shape after years of international
research and development efforts. This work culminated in ITU-R Recommendation
on "IMT-2030 Framework". While the definition phase of technological
requirements is currently ongoing, 3GPP standardization process on 6G networks
is expected to start in 2025 and worldwide commercialization around 2029-2030.
This article serves as a comprehensive guide to 6G by providing an overall
vision, a contemporary survey of the main literature, and an informative
tutorial-type presentation style. In our vision, 6G will be based on three
fundamental elements: wireless, artificial intelligence, and Internet of
Everything. Consequently, 6G can ultimately become the Intelligent Network of
Everything while serving as an enabling platform for the next major disruption
in mobile communication, called mobile intelligence. The potential of mobile
intelligence is that anything can be made connected, intelligent, and aware of
its environment. This will revolutionize the way how devices, systems, and
applications are designed; how they operate and interact with humans and each
other; and how they can be used for the benefit of people, society, and the
world in general. After high-level visioning, the main details of 6G are
discussed, including fundamental elements, disruptive applications, key use
cases, main performance requirements, potential technologies, and defining
features. A special focus is given to a comprehensive set of potential 6G
technologies, each of which is introduced in a tutorial manner. Finally, we
speculate on what comes after 6G and sketch the first high-level vision of 7G.
All in all, the objective of this article is to provide a thorough guide to 6G
in order to serve as a source of knowledge and inspiration for further research
and development work in academia, industry, and standardization bodies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Access, 101 pages, 50 figures, 16
  tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secure Wireless Communications via Frequency Diverse Arrays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenqiao Cheng, Chongjun Ouyang, Xingqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel frequency diverse array (FDA)-assisted secure transmission framework
is proposed, which leverages additional frequency offsets to enhance physical
layer security. Specifically, an FDA-assisted wiretap channel is considered,
where the transmit beamforming and frequency offsets at each antenna are
jointly optimized. A novel alternating optimization-based method is introduced
to address the non-convex problem of secure transmission, focusing on
minimizing transmit power and maximizing the secrecy rate. Numerical results
are provided to demonstrate the superiority of the FDA-based framework compared
to systems employing traditional phased array antennas in secure transmission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting NOMA Transmissions in Multi-UAV-assisted Wireless Networks:
  From Aerial-RIS to Mode-switching UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songhan Zhao, Shimin Gong, Bo Gu, Lanhua Li, Bin Lyu, Dinh Thai Hoang, Changyan Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider an aerial reconfigurable intelligent surface
(ARIS)-assisted wireless network, where multiple unmanned aerial vehicles
(UAVs) collect data from ground users (GUs) by using the non-orthogonal
multiple access (NOMA) method. The ARIS provides enhanced channel
controllability to improve the NOMA transmissions and reduce the co-channel
interference among UAVs. We also propose a novel dual-mode switching scheme,
where each UAV equipped with both an ARIS and a radio frequency (RF)
transceiver can adaptively perform passive reflection or active transmission.
We aim to maximize the overall network throughput by jointly optimizing the
UAVs' trajectory planning and operating modes, the ARIS's passive beamforming,
and the GUs' transmission control strategies. We propose an optimization-driven
hierarchical deep reinforcement learning (O-HDRL) method to decompose it into a
series of subproblems. Specifically, the multi-agent deep deterministic policy
gradient (MADDPG) adjusts the UAVs' trajectory planning and mode switching
strategies, while the passive beamforming and transmission control strategies
are tackled by the optimization methods. Numerical results reveal that the
O-HDRL efficiently improves the learning stability and reward performance
compared to the benchmark methods. Meanwhile, the dual-mode switching scheme is
verified to achieve a higher throughput performance compared to the fixed ARIS
scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movable Antenna Array Aided Ultra Reliable Covert Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Wang, Guojie Hu, Xiaoling Hu, Xingbo Lu, Yuzhen Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we construct a framework of the movable antenna (MA) aided
covert communication shielded by the general noise uncertainty for the first
time. According to the analysis performance on the derived closed-form
expressions of the sum of the probabilities of the detection errors and the
communication outage probability, the perfect covertness and the ultra
reliability can be achieved by adjusting the antenna position in the MA array.
Then, we formulate the communication covertness maximization problem with the
constraints of the ultra reliability and the independent discrete movable
position to optimize the transmitter's parameter. With the maximal ratio
transmitting (MRT) design for the beamforming, we solve the closed-form optimal
information transmit power and design a lightweight discrete projected gradient
descent (DPGD) algorithm to determine the optimal antenna position. The
numerical results show that the optimal achievable covertness and the feasible
region of the steering angle with the MA array is significant larger than the
one with the fixed-position antenna (FPA) array.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>has been presented in IEEE GLOBECOM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Source Heterogeneous SoCs for AI: The PULP Platform Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Conti, Angelo Garofalo, Davide Rossi, Giuseppe Tagliavini, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since 2013, the PULP (Parallel Ultra-Low Power) Platform project has been one
of the most active and successful initiatives in designing research IPs and
releasing them as open-source. Its portfolio now ranges from processor cores to
network-on-chips, peripherals, SoC templates, and full hardware accelerators.
In this article, we focus on the PULP experience designing heterogeneous AI
acceleration SoCs - an endeavour encompassing SoC architecture definition;
development, verification, and integration of acceleration IPs; front- and
back-end VLSI design; testing; development of AI deployment software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprinted submitted to IEEE Solid-State Circuits Magazine</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative ISAC-empowered Low-Altitude Economy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Tang, Yiming Yu, Cunhua Pan, Hong Ren, Dongming Wang, Jiangzhou Wang, Xiaohu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a cooperative integrated sensing and communication (ISAC)
scheme for the low-altitude sensing scenario, aiming at estimating the
parameters of the unmanned aerial vehicles (UAVs) and enhancing the sensing
performance via cooperation. The proposed scheme consists of two stages. In
Stage I, we formulate the monostatic parameter estimation problem via using a
tensor decomposition model. By leveraging the Vandermonde structure of the
factor matrix, a spatial smoothing tensor decomposition scheme is introduced to
estimate the UAVs' parameters. To further reduce the computational complexity,
we design a reduced-dimensional (RD) angle of arrival (AoA) estimation
algorithm based on generalized Rayleigh quotient (GRQ). In Stage II, the
positions and true velocities of the UAVs are determined through the data
fusion across multiple base stations (BSs). Specifically, we first develop a
false removing minimum spanning tree (MST)-based data association method to
accurately match the BSs' parameter estimations to the same UAV. Then, a Pareto
optimality method and a residual weighting scheme are developed to facilitate
the position and velocity estimation, respectively. We further extend our
approach to the dual-polarized system. Simulation results validate the
effectiveness of the proposed schemes in comparison to the conventional
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-Timescale Design for AP Mode Selection of Cooperative ISAC Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichu Ren, Cunhua Pan, Hong Ren, Dongming Wang, Lexi Xu, Jiangzhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an emerging technology, cooperative bi-static integrated sensing and
communication (ISAC) is promising to achieve high-precision sensing, high-rate
communication as well as self-interference (SI) avoidance. This paper
investigates the two-timescale design for access point (AP) mode selection to
realize the full potential of the cooperative bi-static ISAC network with low
system overhead, where the beamforming at the APs is adapted to the
rapidly-changing instantaneous channel state information (CSI), while the AP
mode is adapted to the slowly-changing statistical CSI. We first apply the
minimum mean square error (MMSE) estimator to estimate the channel between the
APs and the channels from the APs to the user equipments (UEs). Then we adopt
the low-complexity maximum ratio transmission (MRT) beamforming and the maximum
ratio combining (MRC) detector, and derive the closed-form expressions of the
communication rate and the sensing signal-to-interference-plus-noise-ratio
(SINR). We formulate a non-convex integer optimization problem to maximize the
minimum sensing SINR under the communication quality of service (QoS)
constraints. McCormick envelope relaxation and successive convex approximation
(SCA) techniques are applied to solve the challenging non-convex integer
optimization problem. Simulation results validate the closed-form expressions
and prove the convergence and effectiveness of the proposed AP mode selection
scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource Allocation for ISAC Networks with Application to Target
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Wang, Luis F. Abanto-Leon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future 6G networks are expected to empower communication systems by
integrating sensing capabilities, resulting in integrated sensing and
communication (ISAC) systems. However, this integration may exacerbate the data
traffic congestion in existing communication systems due to limited resources.
Therefore, the resources of ISAC systems must be carefully allocated to ensure
high performance. Given the increasing demands for both sensing and
communication services, current methods are inadequate for tracking targets
frequently in every frame while simultaneously communicating with users. To
address this gap, this work formulates an optimization problem that jointly
allocates resources in the time, frequency, power, and spatial domains for
targets and users, accounting for the movement of targets and time-varying
communication channels. Specifically, we minimize the trace of posterior
Cram\'er-Rao bound for target tracking subject to communication throughput and
resource allocation constraints. To solve this non-convex problem, we develop a
block coordinate descent (BCD) algorithm based on the penalty method,
successive convex approximation (SCA), and one-dimensional search. Simulation
results demonstrate the validity of the proposed algorithm and the performance
trade-off between sensing and communication.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Hybrid Sketching for $\ell_2$-Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neophytos Charalambides, Arya Mazumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear algebraic operations are ubiquitous in engineering applications, and
arise often in a variety of fields including statistical signal processing and
machine learning. With contemporary large datasets, to perform linear algebraic
methods and regression tasks, it is necessary to resort to both distributed
computations as well as data compression. In this paper, we study
\textit{distributed} $\ell_2$-subspace embeddings, a common technique used to
efficiently perform linear regression. In our setting, data is distributed
across multiple computing nodes and a goal is to minimize communication between
the nodes and the coordinator in the distributed centralized network, while
maintaining the geometry of the dataset. Furthermore, there is also the concern
of keeping the data private and secure from potential adversaries. In this
work, we address these issues through randomized sketching, where the key idea
is to apply distinct sketching matrices on the local datasets. A novelty of
this work is that we also consider \textit{hybrid sketching}, \textit{i.e.} a
second sketch is applied on the aggregated locally sketched datasets, for
enhanced embedding results. One of the main takeaways of this work is that by
hybrid sketching, we can interpolate between the trade-offs that arise in
off-the-shelf sketching matrices. That is, we can obtain gains in terms of
embedding dimension or multiplication time. Our embedding arguments are also
justified numerically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design of an improved microstrip antenna operating at a frequency band
  of 28GHz 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. O. Zakariyya, B. O. Sadiq, R. A. Alao, J. A. Adesina, E. Obi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The design of an improved microstrip antenna operating in the 28 GHz
frequency spectrum is the main goal of this work. The design used a Roger RT
5880 LZ substrate with a thickness and permittivity of 0.762mm and 1.96,
respectively. The antenna was simulated in CST Microwave Studio. As the antenna
feed, a quarter-wave transformer was used to provide an impedance match of 50
ohms. To improve the antenna's performance, a Ushaped element was added to the
ground plane. The antenna resonated at 28 GHz frequency, according to
simulation data, with a return loss of -21.4 dB, VSWR of 1.18, bandwidth of
2.026 GHz, and gain of 8.19 dB. The proposed antenna exhibits a performance
improvement in terms of gain and bandwidth due to the addition of U-shaped
element when benchmarked with existing designs in the literature work
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ E-Sort: Empowering End-to-end Neural Network for Multi-channel Spike
  Sorting with Transfer Learning and Fast Post-processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntao Han, Shiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decoding extracellular recordings is a crucial task in electrophysiology and
brain-computer interfaces. Spike sorting, which distinguishes spikes and their
putative neurons from extracellular recordings, becomes computationally
demanding with the increasing number of channels in modern neural probes. To
address the intensive workload and complex neuron interactions, we propose
E-Sort, an end-to-end neural network-based spike sorter with transfer learning
and parallelizable post-processing. Our framework reduces the required number
of annotated spikes for training by 44% compared to training from scratch,
achieving up to 25.68% higher accuracy. Additionally, our novel post-processing
algorithm is compatible with deep learning frameworks, making E-Sort
significantly faster than state-of-the-art spike sorters. On synthesized
Neuropixels recordings, E-Sort achieves comparable accuracy with Kilosort4
while sorting 50 seconds of data in only 1.32 seconds. Our method demonstrates
robustness across various probe geometries, noise levels, and drift conditions,
offering a substantial improvement in both accuracy and runtime efficiency
compared to existing spike sorters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Overhead Channel Estimation via 3D Extrapolation for TDD mmWave
  Massive MIMO Systems Under High-Mobility Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binggui Zhou, Xi Yang, Shaodan Ma, Feifei Gao, Guanghua Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In time division duplexing (TDD) millimeter wave (mmWave) massive
multiple-input multiple-output (MIMO) systems, downlink channel state
information (CSI) can be obtained from uplink channel estimation thanks to
channel reciprocity. However, under high-mobility scenarios, frequent uplink
channel estimation is needed due to channel aging. Additionally, large amounts
of antennas and subcarriers result in high-dimensional CSI matrices,
aggravating pilot training overhead. To address this, we propose a three-domain
(3D) channel extrapolation framework across spatial, frequency, and temporal
domains. First, considering the effectiveness of traditional knowledge-driven
channel estimation methods and the marginal effects of pilots in the spatial
and frequency domains, a knowledge-and-data driven spatial-frequency channel
extrapolation network (KDD-SFCEN) is proposed for uplink channel estimation via
joint spatial-frequency channel extrapolation to reduce spatial-frequency
domain pilot overhead. Then, leveraging channel reciprocity and temporal
dependencies, we propose a temporal uplink-downlink channel extrapolation
network (TUDCEN) powered by generative artificial intelligence for slot-level
channel extrapolation, aiming to reduce the tremendous temporal domain pilot
overhead caused by high mobility. Numerical results demonstrate the superiority
of the proposed framework in significantly reducing the pilot training overhead
by 16 times and improving the system's spectral efficiency under high-mobility
scenarios compared with state-of-the-art channel estimation/extrapolation
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures, 3 tables. Accepted by IEEE Transactions on
  Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A physics-compliant diagonal representation for wireless channels
  parametrized by beyond-diagonal reconfigurable intelligent surfaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp del Hougne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The parametrization of wireless channels by so-called "beyond-diagonal
reconfigurable intelligent surfaces" (BD-RIS) is mathematically characterized
by a matrix whose off-diagonal entries are partially or fully populated.
Physically, this corresponds to tunable coupling mechanisms between the RIS
elements that originate from the RIS control circuit. Here, we derive a
physics-compliant diagonal representation for BD-RIS-parametrized channels. We
recognize that any RIS control circuit can always be separated into its static
parts (SLC) and a set of tunable individual loads (IL). Therefore, a
BD-RIS-parametrized channel results from the chain cascade of three systems: i)
radio environment (RE), ii) SLC, and iii) IL. RE and SLC are static
non-diagonal systems whose cascade K is terminated by the tunable diagonal
system IL. This physics-compliant representation in terms of K and IL is
directly analogous to that for conventional ("diagonal") RIS (D-RIS).
Therefore, scenarios with BD-RIS can also readily be captured by the
physics-compliant coupled-dipole model PhysFad, as we show. In addition,
physics-compliant algorithms for system-level optimization with D-RIS can be
directly applied to scenarios with BD-RIS. We demonstrate this important
implication of our conceptual finding in a case study on end-to-end channel
estimation and optimization in a BD-RIS-parametrized rich-scattering
environment. Our case study is the first experimentally grounded system-level
optimization for BD-RIS: We obtain the characteristics of RE and IL from
experimental measurements and a commercial PIN diode, respectively. Altogether,
our physics-compliant diagonal representation for BD-RIS enables a paradigm
shift in how practitioners in wireless communications and signal processing
implement system-level optimizations for BD-RIS because it enables them to
directly apply existing physics-compliant D-RIS algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, submitted to an IEEE Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Power of Data Tsunami: A Comprehensive <span class="highlight-title">Survey</span> on Data
  Assessment and Selection for Instruction Tuning of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02085v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02085v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulei Qin, Yuncheng Yang, Pengcheng Guo, Gang Li, Hang Shao, Yuchen Shi, Zihan Xu, Yun Gu, Ke Li, Xing Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning plays a critical role in aligning large language models
(LLMs) with human preference. Despite the vast amount of open instruction
datasets, naively training a LLM on all existing instructions may not be
optimal and practical. To pinpoint the most beneficial datapoints, data
assessment and selection methods have been proposed in the fields of natural
language processing (NLP) and deep learning. However, under the context of
instruction tuning, there still exists a gap in knowledge on what kind of data
evaluation metrics can be employed and how they can be integrated into the
selection mechanism. To bridge this gap, we present a comprehensive review on
existing literature of data assessment and selection especially for instruction
tuning of LLMs. We systematically categorize all applicable methods into
quality-based, diversity-based, and importance-based ones where a unified,
fine-grained taxonomy is structured. For each category, representative methods
are elaborated to describe the landscape of relevant research. In addition,
comparison between the latest methods is conducted on their officially reported
results to provide in-depth discussions on their limitations. Finally, we
summarize the open challenges and propose the promosing avenues for future
studies. All related contents are available at
https://github.com/yuleiqin/fantastic-data-engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR with Survey Certificate, review, survey, 37 pages, 5
  figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-28T00:00:00Z">2024-12-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Cryptography and Security <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Anomaly Detection System Based on Generative Classifiers for
  Controller Area Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunheng Zhao, Stefano Longari, Michele Carminati, Pierluigi Pisu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As electronic systems become increasingly complex and prevalent in modern
vehicles, securing onboard networks is crucial, particularly as many of these
systems are safety-critical. Researchers have demonstrated that modern vehicles
are susceptible to various types of attacks, enabling attackers to gain control
and compromise safety-critical electronic systems. Consequently, several
Intrusion Detection Systems (IDSs) have been proposed in the literature to
detect such cyber-attacks on vehicles. This paper introduces a novel generative
classifier-based Intrusion Detection System (IDS) designed for anomaly
detection in automotive networks, specifically focusing on the Controller Area
Network (CAN). Leveraging variational Bayes, our proposed IDS utilizes a deep
latent variable model to construct a causal graph for conditional
probabilities. An auto-encoder architecture is utilized to build the classifier
to estimate conditional probabilities, which contribute to the final prediction
probabilities through Bayesian inference. Comparative evaluations against
state-of-the-art IDSs on a public Car-hacking dataset highlight our proposed
classifier's superior performance in improving detection accuracy and F1-score.
The proposed IDS demonstrates its efficacy by outperforming existing models
with limited training data, providing enhanced security assurance for
automotive systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How To Think About End-To-End Encryption and AI: Training, Processing,
  Disclosure, and Consent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mallory Knodel, Andrés Fábrega, Daniella Ferrari, Jacob Leiken, Betty Li Hou, Derek Yen, Sam de Alfaro, Kyunghyun Cho, Sunoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end encryption (E2EE) has become the gold standard for securing
communications, bringing strong confidentiality and privacy guarantees to
billions of users worldwide. However, the current push towards widespread
integration of artificial intelligence (AI) models, including in E2EE systems,
raises some serious security concerns. This work performs a critical
examination of the (in)compatibility of AI models and E2EE applications. We
explore this on two fronts: (1) the integration of AI "assistants" within E2EE
applications, and (2) the use of E2EE data for training AI models. We analyze
the potential security implications of each, and identify conflicts with the
security guarantees of E2EE. Then, we analyze legal implications of integrating
AI models in E2EE applications, given how AI integration can undermine the
confidentiality that E2EE promises. Finally, we offer a list of detailed
recommendations based on our technical and legal analyses, including: technical
design choices that must be prioritized to uphold E2EE security; how service
providers must accurately represent E2EE security; and best practices for the
default behavior of AI features and for requesting user consent. We hope this
paper catalyzes an informed conversation on the tensions that arise between the
brisk deployment of AI and the security offered by E2EE, and guides the
responsible development of new AI features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Validity of Traditional Vulnerability Scoring Systems for
  Adversarial Attacks against LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atmane Ayoub Mansour Bahar, Ahmad Samer Wazan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research investigates the effectiveness of established vulnerability
metrics, such as the Common Vulnerability Scoring System (CVSS), in evaluating
attacks against Large Language Models (LLMs), with a focus on Adversarial
Attacks (AAs). The study explores the influence of both general and specific
metric factors in determining vulnerability scores, providing new perspectives
on potential enhancements to these metrics.
  This study adopts a quantitative approach, calculating and comparing the
coefficient of variation of vulnerability scores across 56 adversarial attacks
on LLMs. The attacks, sourced from various research papers, and obtained
through online databases, were evaluated using multiple vulnerability metrics.
Scores were determined by averaging the values assessed by three distinct LLMs.
The results indicate that existing scoring-systems yield vulnerability scores
with minimal variation across different attacks, suggesting that many of the
metric factors are inadequate for assessing adversarial attacks on LLMs. This
is particularly true for context-specific factors or those with predefined
value sets, such as those in CVSS. These findings support the hypothesis that
current vulnerability metrics, especially those with rigid values, are limited
in evaluating AAs on LLMs, highlighting the need for the development of more
flexible, generalized metrics tailored to such attacks.
  This research offers a fresh analysis of the effectiveness and applicability
of established vulnerability metrics, particularly in the context of
Adversarial Attacks on Large Language Models, both of which have gained
significant attention in recent years. Through extensive testing and
calculations, the study underscores the limitations of these metrics and opens
up new avenues for improving and refining vulnerability assessment frameworks
specifically tailored for LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>101 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hades: Homomorphic Augmented Decryption for Efficient Symbol-comparison
  -- A Database's Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Outsourced databases powered by fully homomorphic encryption (FHE) offer the
promise of secure data processing on untrusted cloud servers. A crucial aspect
of database functionality, and one that has remained challenging to integrate
efficiently within FHE schemes, is the ability to perform comparisons on
encrypted data. Such comparisons are fundamental for various database
operations, including building indexes for efficient data retrieval and
executing range queries to select data within specific intervals. While
traditional approaches like Order-Preserving Encryption (OPE) could enable
comparisons, they are fundamentally incompatible with FHE without significantly
increasing ciphertext size, thereby exacerbating the inherent performance
overhead of FHE and further hindering its practical deployment. This paper
introduces HADES, a novel cryptographic framework that enables efficient and
secure comparisons directly on FHE ciphertexts without any ciphertext
expansion. Based on the Ring Learning with Errors (RLWE) problem, HADES
provides CPA-security and incorporates perturbation-aware encryption to
mitigate frequency-analysis attacks. Implemented using OpenFHE, HADES supports
both integer and floating-point operations, demonstrating practical performance
on real-world datasets and outperforming state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Semantic Federated Learning Enabled Industrial Edge Network
  for Fire Surveillance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Dong, Yubo Peng, Feibo Jiang, Kezhi Wang, Kun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In fire surveillance, Industrial Internet of Things (IIoT) devices require
transmitting large monitoring data frequently, which leads to huge consumption
of spectrum resources. Hence, we propose an Industrial Edge Semantic Network
(IESN) to allow IIoT devices to send warnings through Semantic communication
(SC). Thus, we should consider (1) Data privacy and security. (2) SC model
adaptation for heterogeneous devices. (3) Explainability of semantics.
Therefore, first, we present an eXplainable Semantic Federated Learning (XSFL)
to train the SC model, thus ensuring data privacy and security. Then, we
present an Adaptive Client Training (ACT) strategy to provide a specific SC
model for each device according to its Fisher information matrix, thus
overcoming the heterogeneity. Next, an Explainable SC (ESC) mechanism is
designed, which introduces a leakyReLU-based activation mapping to explain the
relationship between the extracted semantics and monitoring data. Finally,
simulation results demonstrate the effectiveness of XSFL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Unlearning with Gradient Descent and Conflict Mitigation <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zibin Pan, Zhichao Wang, Chi Li, Kaiyan Zheng, Boqi Wang, Xiaoying Tang, Junhua Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has received much attention in recent years. However,
although clients are not required to share their data in FL, the global model
itself can implicitly remember clients' local data. Therefore, it's necessary
to effectively remove the target client's data from the FL global model to ease
the risk of privacy leakage and implement ``the right to be forgotten".
Federated Unlearning (FU) has been considered a promising way to remove data
without full retraining. But the model utility easily suffers significant
reduction during unlearning due to the gradient conflicts. Furthermore, when
conducting the post-training to recover the model utility, the model is prone
to move back and revert what has already been unlearned. To address these
issues, we propose Federated Unlearning with Orthogonal Steepest Descent
(FedOSD). We first design an unlearning Cross-Entropy loss to overcome the
convergence issue of the gradient ascent. A steepest descent direction for
unlearning is then calculated in the condition of being non-conflicting with
other clients' gradients and closest to the target client's gradient. This
benefits to efficiently unlearn and mitigate the model utility reduction. After
unlearning, we recover the model utility by maintaining the achievement of
unlearning. Finally, extensive experiments in several FL scenarios verify that
FedOSD outperforms the SOTA FU algorithms in terms of unlearning and model
utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the Proceedings of the 39th AAAI Conference on
  Artificial Intelligence (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Watermarked LLMs be Identified by Users via Crafted Prompts? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiwei Liu, Sheng Guan, Yiming Liu, Leyi Pan, Yifei Zhang, Liancheng Fang, Lijie Wen, Philip S. Yu, Xuming Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text watermarking for Large Language Models (LLMs) has made significant
progress in detecting LLM outputs and preventing misuse. Current watermarking
techniques offer high detectability, minimal impact on text quality, and
robustness to text editing. However, current researches lack investigation into
the imperceptibility of watermarking techniques in LLM services. This is
crucial as LLM providers may not want to disclose the presence of watermarks in
real-world scenarios, as it could reduce user willingness to use the service
and make watermarks more vulnerable to attacks. This work is the first to
investigate the imperceptibility of watermarked LLMs. We design an
identification algorithm called Water-Probe that detects watermarks through
well-designed prompts to the LLM. Our key motivation is that current
watermarked LLMs expose consistent biases under the same watermark key,
resulting in similar differences across prompts under different watermark keys.
Experiments show that almost all mainstream watermarking algorithms are easily
identified with our well-designed prompts, while Water-Probe demonstrates a
minimal false positive rate for non-watermarked LLMs. Finally, we propose that
the key to enhancing the imperceptibility of watermarked LLMs is to increase
the randomness of watermark key selection. Based on this, we introduce the
Water-Bag strategy, which significantly improves watermark imperceptibility by
merging multiple watermark keys.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 5 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Next-Gen Interconnection Systems with Compute Express Link: a
  Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Xinkui Zhao, Guanjie Cheng, Yuesheng Xu, Shuiguang Deng, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interconnection is crucial for computing systems. However, the current
interconnection performance between processors and devices, such as memory
devices and accelerators, significantly lags behind their computing
performance, severely limiting the overall performance. To address this
challenge, Intel proposes Compute Express Link (CXL), an open industry-standard
interconnection. With memory semantics, CXL offers low-latency, scalable, and
coherent interconnection between processors and devices. This paper introduces
recent advances in CXL-based interconnection systems with memory semantics. We
classify the existing research into three categories: Pooling Memory,
Distributed Shared Memory, and Unified Memory. Pooling Memory interconnects
processors and memory, aims to address memory wall challenge. Distributed
shared memory interconnects processors across nodes, aims to synchronize the
cluster. Unified memory interconnects processors and accelerators, aims to
enhance collaboration in heterogeneous computing systems. Finally, we discuss
the future research and envision memory-centric computing with CXL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Cache Freshness for Emerging Real-Time Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Mao, Rishabh Iyer, Scott Shenker, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Caching is widely used in industry to improve application performance by
reducing data-access latency and taking the load off the backend
infrastructure. TTLs have become the de-facto mechanism used to keep cached
data reasonably fresh (i.e., not too out of date with the backend). However,
the emergence of real-time applications requires tighter data freshness, which
is impractical to achieve with TTLs. We discuss why this is the case, and
propose a simple yet effective adaptive policy to achieve the desired
freshness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HotNets '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Unlearning with Gradient Descent and Conflict Mitigation <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zibin Pan, Zhichao Wang, Chi Li, Kaiyan Zheng, Boqi Wang, Xiaoying Tang, Junhua Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has received much attention in recent years. However,
although clients are not required to share their data in FL, the global model
itself can implicitly remember clients' local data. Therefore, it's necessary
to effectively remove the target client's data from the FL global model to ease
the risk of privacy leakage and implement ``the right to be forgotten".
Federated Unlearning (FU) has been considered a promising way to remove data
without full retraining. But the model utility easily suffers significant
reduction during unlearning due to the gradient conflicts. Furthermore, when
conducting the post-training to recover the model utility, the model is prone
to move back and revert what has already been unlearned. To address these
issues, we propose Federated Unlearning with Orthogonal Steepest Descent
(FedOSD). We first design an unlearning Cross-Entropy loss to overcome the
convergence issue of the gradient ascent. A steepest descent direction for
unlearning is then calculated in the condition of being non-conflicting with
other clients' gradients and closest to the target client's gradient. This
benefits to efficiently unlearn and mitigate the model utility reduction. After
unlearning, we recover the model utility by maintaining the achievement of
unlearning. Finally, extensive experiments in several FL scenarios verify that
FedOSD outperforms the SOTA FU algorithms in terms of unlearning and model
utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the Proceedings of the 39th AAAI Conference on
  Artificial Intelligence (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contention-Aware Microservice Deployment in Collaborative Mobile Edge
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlei Ge, Yang Li, Xing Zhang, Yukun Sun, Yunji Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As an emerging computing paradigm, mobile edge computing (MEC) provides
processing capabilities at the network edge, aiming to reduce latency and
improve user experience. Meanwhile, the advancement of containerization
technology facilitates the deployment of microservice-based applications via
edge node collaboration, ensuring highly efficient service delivery. However,
existing research overlooks the resource contention among microservices in MEC.
This neglect potentially results in inadequate resources for microservices
constituting latency-sensitive applications, leading to increased response time
and ultimately compromising quality of service (QoS). To solve this problem, we
propose the Contention-Aware Multi-Application Microservice Deployment (CAMD)
algorithm for collaborative MEC, balancing rapid response for applications with
low-latency requirements and overall processing efficiency. The CAMD algorithm
decomposes the overall deployment problem into manageable sub-problems, each
focusing on a single microservice, then employs a heuristic approach to
optimize these sub-problems, and ultimately arrives at an optimized deployment
scheme through an iterative process. Finally, the superiority of the proposed
algorithm is evidenced through intensive experiments and comparison with
baseline algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Calibre: Towards Fair and Accurate Personalized Federated Learning with
  Self-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sijia Chen, Ningxin Su, Baochun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of personalized federated learning, existing approaches train
a global model to extract transferable representations, based on which any
client could train personalized models with a limited number of data samples.
Self-supervised learning is considered a promising direction as the global
model it produces is generic and facilitates personalization for all clients
fairly. However, when data is heterogeneous across clients, the global model
trained using SSL is unable to learn high-quality personalized models. In this
paper, we show that when the global model is trained with SSL without
modifications, its produced representations have fuzzy class boundaries. As a
result, personalized learning within each client produces models with low
accuracy. In order to improve SSL towards better accuracy without sacrificing
its advantage in fairness, we propose Calibre, a new personalized federated
learning framework designed to calibrate SSL representations by maintaining a
suitable balance between more generic and more client-specific representations.
Calibre is designed based on theoretically-sound properties, and introduces (1)
a client-specific prototype loss as an auxiliary training objective; and (2) an
aggregation algorithm guided by such prototypes across clients. Our
experimental results in an extensive array of non-i.i.d.~settings show that
Calibre achieves state-of-the-art performance in terms of both mean accuracy
and fairness across clients. Code repo:
https://github.com/TL-System/plato/tree/main/examples/ssl/calibre.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICDCS camera-ready paper, Code repo:
  https://github.com/TL-System/plato/tree/main/examples/ssl/calibre</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Parameter-Efficient Federated Fine-Tuning on Heterogeneous
  Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Liu, Yunming Liao, Hongli Xu, Yang Xu, Jianchun Liu, Chen Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated fine-tuning (FedFT) has been proposed to fine-tune the pre-trained
language models in a distributed manner. However, there are two critical
challenges for efficient FedFT in practical applications, i.e., resource
constraints and system heterogeneity. Existing works rely on
parameter-efficient fine-tuning methods, e.g., low-rank adaptation (LoRA), but
with major limitations. Herein, based on the inherent characteristics of FedFT,
we observe that LoRA layers with higher ranks added close to the output help to
save resource consumption while achieving comparable fine-tuning performance.
Then we propose a novel LoRA-based FedFT framework, termed LEGEND, which faces
the difficulty of determining the number of LoRA layers (called, LoRA depth)
and the rank of each LoRA layer (called, rank distribution). We analyze the
coupled relationship between LoRA depth and rank distribution, and design an
efficient LoRA configuration algorithm for heterogeneous devices, thereby
promoting fine-tuning efficiency. Extensive experiments are conducted on a
physical platform with 80 commercial devices. The results show that LEGEND can
achieve a speedup of 1.5-2.8$\times$ and save communication costs by about
42.3% when achieving the target accuracy, compared to the advanced solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Robust Federated Learning Framework for Undependable Devices at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Wang, Jianchun Liu, Hongli Xu, Chunming Qiao, Huarong Deng, Qiuye Zheng, Jiantao Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a federated learning (FL) system, many devices, such as smartphones, are
often undependable (e.g., frequently disconnected from WiFi) during training.
Existing FL frameworks always assume a dependable environment and exclude
undependable devices from training, leading to poor model performance and
resource wastage. In this paper, we propose FLUDE to effectively deal with
undependable environments. First, FLUDE assesses the dependability of devices
based on the probability distribution of their historical behaviors (e.g., the
likelihood of successfully completing training). Based on this assessment,
FLUDE adaptively selects devices with high dependability for training. To
mitigate resource wastage during the training phase, FLUDE maintains a model
cache on each device, aiming to preserve the latest training state for later
use in case local training on an undependable device is interrupted. Moreover,
FLUDE proposes a staleness-aware strategy to judiciously distribute the global
model to a subset of devices, thus significantly reducing resource wastage
while maintaining model performance. We have implemented FLUDE on two physical
platforms with 120 smartphones and NVIDIA Jetson devices. Extensive
experimental results demonstrate that FLUDE can effectively improve model
performance and resource efficiency of FL training in undependable
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Caesar: A Low-deviation Compression Approach for Efficient Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Yan, Jianchun Liu, Hongli Xu, Liusheng Huang, Jiantao Gong, Xudong Liu, Kun Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compression is an efficient way to relieve the tremendous communication
overhead of federated learning (FL) systems. However, for the existing works,
the information loss under compression will lead to unexpected model/gradient
deviation for the FL training, significantly degrading the training
performance, especially under the challenges of data heterogeneity and model
obsolescence. To strike a delicate trade-off between model accuracy and traffic
cost, we propose Caesar, a novel FL framework with a low-deviation compression
approach. For the global model download, we design a greedy method to optimize
the compression ratio for each device based on the staleness of the local
model, ensuring a precise initial model for local training. Regarding the local
gradient upload, we utilize the device's local data properties (\ie, sample
volume and label distribution) to quantify its local gradient's importance,
which then guides the determination of the gradient compression ratio. Besides,
with the fine-grained batch size optimization, Caesar can significantly
diminish the devices' idle waiting time under the synchronized barrier. We have
implemented Caesar on two physical platforms with 40 smartphones and 80 NVIDIA
Jetson devices. Extensive results show that Caesar can reduce the traffic costs
by about 25.54%$\thicksim$37.88% compared to the compression-based baselines
with the same target accuracy, while incurring only a 0.68% degradation in
final test accuracy relative to the full-precision communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 27 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delayed Random Partial Gradient Averaging for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a distributed machine learning paradigm that
enables multiple clients to train a shared model collaboratively while
preserving privacy. However, the scaling of real-world FL systems is often
limited by two communication bottlenecks:(a) while the increasing computing
power of edge devices enables the deployment of large-scale Deep Neural
Networks (DNNs), the limited bandwidth constraints frequent transmissions over
large DNNs; and (b) high latency cost greatly degrades the performance of FL.
In light of these bottlenecks, we propose a Delayed Random Partial Gradient
Averaging (DPGA) to enhance FL. Under DPGA, clients only share partial local
model gradients with the server. The size of the shared part in a local model
is determined by the update rate, which is coarsely initialized and
subsequently refined over the temporal dimension. Moreover, DPGA largely
reduces the system run time by enabling computation in parallel with
communication. We conduct experiments on non-IID CIFAR-10/100 to demonstrate
the efficacy of our method.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Operating Systems <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Cache Freshness for Emerging Real-Time Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Mao, Rishabh Iyer, Scott Shenker, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Caching is widely used in industry to improve application performance by
reducing data-access latency and taking the load off the backend
infrastructure. TTLs have become the de-facto mechanism used to keep cached
data reasonably fresh (i.e., not too out of date with the backend). However,
the emergence of real-time applications requires tighter data freshness, which
is impractical to achieve with TTLs. We discuss why this is the case, and
propose a simple yet effective adaptive policy to achieve the desired
freshness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>HotNets '24</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid <span class="highlight-title">Quantum</span>-Classical Autoencoder Framework for End-to-End
  Communication Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolun Zhang, Gan Zheng, Nguyen Van Huynh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of quantum machine learning to
End-to-End (E2E) communication systems in wireless fading scenarios. We
introduce a novel hybrid quantum-classical autoencoder architecture that
combines parameterized quantum circuits with classical deep neural networks
(DNNs). Specifically, we propose a hybrid quantum-classical autoencoder (QAE)
framework to optimize the E2E communication system. Our results demonstrate the
feasibility of the proposed hybrid system, and reveal that it is the first work
that can achieve comparable block error rate (BLER) performance to classical
DNN-based and conventional channel coding schemes, while significantly reducing
the number of trainable parameters. Additionally, the proposed QAE exhibits
steady and superior BLER convergence over the classical autoencoder baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Achieving Full-Bandwidth Sensing Performance with Partial Bandwidth
  Allocation for ISAC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Xiao, Zhiwen Zhou, Qianglong Dai, Yong Zeng, Fei Yang, Yan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter studies an uplink integrated sensing and communication (ISAC)
system using discrete Fourier transform spread orthogonal frequency division
multiplexing (DFT-s-OFDM) transmission. We try to answer the following
fundamental question: With only a fractional bandwidth allocated to the user
with sensing task, can the same delay resolution and unambiguous range be
achieved as if all bandwidth were allocated to it? We affirmatively answer the
question by proposing a novel two-stage delay estimation (TSDE) method that
exploits the following facts: without increasing the allocated bandwidth,
higher delay resolution can be achieved via distributed subcarrier allocation
compared to its collocated counterpart, while there is a trade-off between
delay resolution and unambiguous range by varying the decimation factor of
subcarriers. Therefore, the key idea of the proposed TSDE method is to first
perform coarse delay estimation with collocated subcarriers to achieve a large
unambiguous range, and then use distributed subcarriers with optimized
decimation factor to enhance delay resolution while avoiding delay ambiguity.
Our analysis shows that the proposed TSDE method can achieve the full-bandwidth
delay resolution and unambiguous range, by using only at most half of the full
bandwidth, provided that the channel delay spread is less than half of the
unambiguous range. Numerical results show the superiority of the proposed
method over the conventional method with collocated subcarriers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Semantic Federated Learning Enabled Industrial Edge Network
  for Fire Surveillance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Dong, Yubo Peng, Feibo Jiang, Kezhi Wang, Kun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In fire surveillance, Industrial Internet of Things (IIoT) devices require
transmitting large monitoring data frequently, which leads to huge consumption
of spectrum resources. Hence, we propose an Industrial Edge Semantic Network
(IESN) to allow IIoT devices to send warnings through Semantic communication
(SC). Thus, we should consider (1) Data privacy and security. (2) SC model
adaptation for heterogeneous devices. (3) Explainability of semantics.
Therefore, first, we present an eXplainable Semantic Federated Learning (XSFL)
to train the SC model, thus ensuring data privacy and security. Then, we
present an Adaptive Client Training (ACT) strategy to provide a specific SC
model for each device according to its Fisher information matrix, thus
overcoming the heterogeneity. Next, an Explainable SC (ESC) mechanism is
designed, which introduces a leakyReLU-based activation mapping to explain the
relationship between the extracted semantics and monitoring data. Finally,
simulation results demonstrate the effectiveness of XSFL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Overview of Cellular ISAC for Low-Altitude UAV: New Opportunities and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Song, Yong Zeng, Yuhang Yang, Zixiang Ren, Gaoyuan Cheng, Xiaoli Xu, Jie Xu, Shi Jin, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-altitude unmanned aerial vehicles (UAVs) are expected to play an
important role in future wireless networks, either as aerial base stations
(BSs) or aerial users connected to the cellular network. In addition,
integrated sensing and communication (ISAC) has been identified as one of the
six usage scenarios for the forthcoming sixth-generation (6G) mobile networks,
aimed at improving network functionalities and realizing situational awareness
of the physical world. While most existing research efforts focus on
terrestrial two-dimensional (2D) communication and sensing, UAV as an aerial
platform offers a new degree of freedom for designing three-dimensional (3D)
air-ground (AG) ISAC networks. In this article, we provide an overview of
cellular-connected UAV ISAC, by elaborating the UAV's roles as a target to be
sensed and as an aerial anchor to provide sensing functionality, respectively.
In particular, we pay attention to the network coverage issue and topics
specific to UAV networking, emphasizing the new opportunities as well as unique
challenges to be addressed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Zak-<span class="highlight-title">OTFS</span> with Interleaved Pilots to Extend the Region of Predictable
  Operation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinu Jayachandran, Imran Ali Khan, Saif Khan Mohammed, <span class="highlight-author">Ronny Hadani</span>, Ananthanarayanan Chockalingam, Robert Calderbank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When the delay period of the Zak-OTFS carrier is greater than the delay
spread of the channel, and the Doppler period of the carrier is greater than
the Doppler spread of the channel, the effective channel filter taps can simply
be read off from the response to a single pilot carrier waveform. The
input-output (I/O) relation can then be reconstructed for a sampled system that
operates under finite duration and bandwidth constraints. We introduce a
framework for pilot design in the delay-Doppler (DD) domain which makes it
possible to support users with very different delay-Doppler characteristics
when it is not possible to choose a single delay and Doppler period to support
all users. The method is to interleave single pilots in the DD domain, and to
choose the pilot spacing so that the I/O relation can be reconstructed by
solving a small linear system of equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear Complementary dual codes and Linear Complementary pairs of AG
  codes in function fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05845v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05845v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alonso S. Castellanos, Adler V. Marques, Luciane Quoos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, linear complementary pairs (LCP) of codes and linear
complementary dual (LCD) codes have gained significant attention due to their
applications in coding theory and cryptography. In this work, we construct
explicit LCPs of codes and LCD codes from function fields of genus $g \geq 1$.
To accomplish this, we present pairs of suitable divisors giving rise to
non-special divisors of degree $g-1$ in the function field. The results are
applied in constructing LCPs of algebraic geometry codes and LCD algebraic
geometry (AG) codes in Kummer extensions, hyperelliptic function fields, and
elliptic curves.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Quantum</span> state testing beyond the polarizing regime and <span class="highlight-title">quantum</span>
  triangular discrimination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01952v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01952v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complexity class Quantum Statistical Zero-Knowledge ($\mathsf{QSZK}$)
captures computational difficulties of the time-bounded quantum state testing
problem with respect to the trace distance, known as the Quantum State
Distinguishability Problem (QSDP) introduced by Watrous (FOCS 2002). However,
QSDP is in $\mathsf{QSZK}$ merely within the constant polarizing regime,
similar to its classical counterpart shown by Sahai and Vadhan (JACM 2003) due
to the polarization lemma (error reduction for SDP).
  Recently, Berman, Degwekar, Rothblum, and Vasudevan (TCC 2019) extended the
$\mathsf{SZK}$ containment for SDP beyond the polarizing regime via the
time-bounded distribution testing problems with respect to the triangular
discrimination and the Jensen-Shannon divergence. Our work introduces proper
quantum analogs for these problems by defining quantum counterparts for
triangular discrimination. We investigate whether the quantum analogs behave
similarly to their classical counterparts and examine the limitations of
existing approaches to polarization regarding quantum distances. These new
$\mathsf{QSZK}$-complete problems improve $\mathsf{QSZK}$ containments for QSDP
beyond the polarizing regime and establish a simple $\mathsf{QSZK}$-hardness
for the quantum entropy difference problem (QEDP) defined by Ben-Aroya,
Schwartz, and Ta-Shma (ToC 2010). Furthermore, we prove that QSDP with some
exponentially small errors is in $\mathsf{PP}$, while the same problem without
error is in $\mathsf{NQP}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages. v4: added a polarization lemma for QTD, and minor changes.
  v3: added a simple QSZK-hardness proof for QEDP, updated a correct version of
  Theorem 5.1(2), and improved presentation. v2: minor changes</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Optimization Driven Link SINR Assurance in RIS-assisted Indoor
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cao Vien Phung, Max Franke, Ehsan Tohidi, June Heinemann, Andre Drummond, Stefan Schmid, Slawomir Stanczak, Admela Jukan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future smart factories are expected to deploy applications over
high-performance indoor wireless channels in the millimeter-wave (mmWave)
bands, which on the other hand are susceptible to high path losses and Line-of
Sight (LoS) blockages. Low-cost Reconfigurable Intelligent Surfaces (RISs) can
provide great opportunities in such scenarios, due to its ability to alleviate
LoS link blockages. In this paper, we formulate a combinatorial optimization
problem, solved with Integer Linear Programming (ILP) to optimally maintain
connectivity by solving the problem of allocating RIS to robots in a wireless
indoor network. Our model exploits the characteristic of nulling interference
from RISs by tuning RIS reflection coefficients. We further consider
Quality-of-Service (QoS) at receivers in terms of
Signal-to-Interference-plus-Noise Ratio (SINR) and connection outages due to
insufficient transmission quality service. Numerical results for optimal
solutions and heuristics show the benefits of optimally deploying RISs by
providing continuous connectivity through SINR, which significantly reduces
outages due to link quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is uploaded here for research community, thus it is for
  non-commercial purposes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine-Learning Enabled Multidimensional Data Utilization in
  Multi-resonance Biosensors: A Pathway to Enhanced Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majid Aalizadeh, Morteza Azmoudeh Afshar, Xudong Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel framework is proposed that combines multi-resonance biosensors with
machine learning (ML) to significantly enhance the accuracy of parameter
prediction in biosensing. Unlike traditional single-resonance systems, which
are limited to one-dimensional datasets, this approach leverages
multi-dimensional data generated by a custom-designed nanostructure, a periodic
array of silicon nanorods with a triangular cross-section over an aluminum
reflector. High bulk sensitivity values are achieved for this multi-resonant
structure, with certain resonant peaks reaching up to 1706 nm/RIU. The
predictive power of multiple resonant peaks from transverse magnetic (TM) and
transverse electric (TE) polarizations is evaluated using Ridge Regression
modeling. Systematic analysis reveals that incorporating multiple resonances
yields up to three orders of magnitude improvement in refractive index
detection precision compared to single-peak analyses. This precision
enhancement is achieved without modifications to the biosensor hardware,
highlighting the potential of data-centric strategies in biosensing. The
findings establish a new paradigm in biosensing, demonstrating that the synergy
between multi-resonance data acquisition and ML-based analysis can
significantly enhance detection accuracy. This study provides a scalable
pathway for advancing high-precision biosensing technologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages total. References are before supplementary information at
  page 19. Supplementary information are placed after references at the end of
  the manuscript</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid <span class="highlight-title">Quantum</span>-Classical Autoencoder Framework for End-to-End
  Communication Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bolun Zhang, Gan Zheng, Nguyen Van Huynh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the application of quantum machine learning to
End-to-End (E2E) communication systems in wireless fading scenarios. We
introduce a novel hybrid quantum-classical autoencoder architecture that
combines parameterized quantum circuits with classical deep neural networks
(DNNs). Specifically, we propose a hybrid quantum-classical autoencoder (QAE)
framework to optimize the E2E communication system. Our results demonstrate the
feasibility of the proposed hybrid system, and reveal that it is the first work
that can achieve comparable block error rate (BLER) performance to classical
DNN-based and conventional channel coding schemes, while significantly reducing
the number of trainable parameters. Additionally, the proposed QAE exhibits
steady and superior BLER convergence over the classical autoencoder baseline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frames and vertex-frequency representations in graph fractional Fourier
  domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linbo Shang, Zhichao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertex-frequency analysis, particularly the windowed graph Fourier transform
(WGFT), is a significant challenge in graph signal processing. Tight frame
theories is known for its low computational complexity in signal
reconstruction, while fractional order methods shine at unveil more detailed
structural characteristics of graph signals. In the graph fractional Fourier
domain, we introduce multi-windowed graph fractional Fourier frames (MWGFRFF)
to facilitate the construction of tight frames. This leads to developing the
multi-windowed graph fractional Fourier transform (MWGFRFT), enabling novel
vertex-frequency analysis methods. A reconstruction formula is derived, along
with results concerning dual and tight frames. To enhance computational
efficiency, a fast MWGFRFT (FMWGFRFT) algorithm is proposed. Furthermore, we
define shift multi-windowed graph fractional Fourier frames (SMWGFRFF) and
their associated transform (SMWGFRFT), exploring their dual and tight frames.
Experimental results indicate that FMWGFRFT and SMWGFRFT excel in extracting
vertex-frequency features in the graph fractional Fourier domain, with their
combined use optimizing analytical performance. Applications in signal anomaly
detection demonstrate the advantages of FMWGFRFT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bird Vocalization Embedding Extraction Using Self-Supervised
  Disentangled Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runwu Shi, Katsutoshi Itoyama, Kazuhiro Nakadai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the extraction of the bird vocalization embedding from
the whole song level using disentangled representation learning (DRL). Bird
vocalization embeddings are necessary for large-scale bioacoustic tasks, and
self-supervised methods such as Variational Autoencoder (VAE) have shown their
performance in extracting such low-dimensional embeddings from vocalization
segments on the note or syllable level. To extend the processing level to the
entire song instead of cutting into segments, this paper regards each
vocalization as the generalized and discriminative part and uses two encoders
to learn these two parts. The proposed method is evaluated on the Great Tits
dataset according to the clustering performance, and the results outperform the
compared pre-trained models and vanilla VAE. Finally, this paper analyzes the
informative part of the embedding, further compresses its dimension, and
explains the disentangled performance of bird vocalizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented on Vocal Interactivity in-and-between Humans, Animals and
  Robots (VIHAR 2024),
  https://vihar-2024.vihar.org/assets/VIHAR_2024_proceedings.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ASE: Practical Acoustic Speed Estimation Beyond Doppler via Sound
  Diffusion Field 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Lyu, Chenshu Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Passive human speed estimation plays a critical role in acoustic sensing.
Despite extensive study, existing systems, however, suffer from various
limitations: First, previous acoustic speed estimation exploits Doppler
Frequency Shifts (DFS) created by moving targets and relies on microphone
arrays, making them only capable of sensing the radial speed within a
constrained distance. Second, the channel measurement rate proves inadequate to
estimate high moving speeds. To overcome these issues, we present ASE, an
accurate and robust Acoustic Speed Estimation system on a single commodity
microphone. We model the sound propagation from a unique perspective of the
acoustic diffusion field, and infer the speed from the acoustic spatial
distribution, a completely different way of thinking about speed estimation
beyond prior DFS-based approaches. We then propose a novel Orthogonal
Time-Delayed Multiplexing (OTDM) scheme for acoustic channel estimation at a
high rate that was previously infeasible, making it possible to estimate high
speeds. We further develop novel techniques for motion detection and signal
enhancement to deliver a robust and practical system. We implement and evaluate
ASE through extensive real-world experiments. Our results show that ASE
reliably tracks walking speed, independently of target location and direction,
with a mean error of 0.13 m/s, a reduction of 2.5x from DFS, and a detection
rate of 97.4% for large coverage, e.g., free walking in a 4m $\times$ 4m room.
We believe ASE pushes acoustic speed estimation beyond the conventional
DFS-based paradigm and will inspire exciting research in acoustic sensing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Achieving Full-Bandwidth Sensing Performance with Partial Bandwidth
  Allocation for ISAC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Xiao, Zhiwen Zhou, Qianglong Dai, Yong Zeng, Fei Yang, Yan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter studies an uplink integrated sensing and communication (ISAC)
system using discrete Fourier transform spread orthogonal frequency division
multiplexing (DFT-s-OFDM) transmission. We try to answer the following
fundamental question: With only a fractional bandwidth allocated to the user
with sensing task, can the same delay resolution and unambiguous range be
achieved as if all bandwidth were allocated to it? We affirmatively answer the
question by proposing a novel two-stage delay estimation (TSDE) method that
exploits the following facts: without increasing the allocated bandwidth,
higher delay resolution can be achieved via distributed subcarrier allocation
compared to its collocated counterpart, while there is a trade-off between
delay resolution and unambiguous range by varying the decimation factor of
subcarriers. Therefore, the key idea of the proposed TSDE method is to first
perform coarse delay estimation with collocated subcarriers to achieve a large
unambiguous range, and then use distributed subcarriers with optimized
decimation factor to enhance delay resolution while avoiding delay ambiguity.
Our analysis shows that the proposed TSDE method can achieve the full-bandwidth
delay resolution and unambiguous range, by using only at most half of the full
bandwidth, provided that the channel delay spread is less than half of the
unambiguous range. Numerical results show the superiority of the proposed
method over the conventional method with collocated subcarriers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amplitude-to-Phase Conversion in Injection-Locked CMOS Ring Oscillators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaowen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Injection-locked ring oscillators (ILROs) are extensively employed for
multi-phase clock generation in wireline and optical links. However, existing
injection-locking theorems primarily rely on linearized phase-domain or
nonlinear time-domain models, which fail to account for amplitude-to-phase
conversion effects inherent in ILROs. This paper introduces an enhanced
analytical model based on an extension of Adler's equation, explicitly
incorporating amplitude-to-phase conversion. Simulation results demonstrate
strong alignment with the proposed analytical predictions, validating the
model's accuracy in capturing the locking range and phasor relationships.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Calibrated Dual Contrasting for Annotation-Efficient Bacteria Raman
  Spectroscopy Clustering and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiming Yao, Wei Luo, Tao Zhou, Ang Gao, Xue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Raman scattering is based on molecular vibration spectroscopy and provides a
powerful technology for pathogenic bacteria diagnosis using the unique
molecular fingerprint information of a substance. The integration of deep
learning technology has significantly improved the efficiency and accuracy of
intelligent Raman spectroscopy (RS) recognition. However, the current RS
recognition methods based on deep neural networks still require the annotation
of a large amount of spectral data, which is labor-intensive. This paper
presents a novel annotation-efficient Self-Calibrated Dual Contrasting (SCDC)
method for RS recognition that operates effectively with few or no annotation.
Our core motivation is to represent the spectrum from two different
perspectives in two distinct subspaces: embedding and category. The embedding
perspective captures instance-level information, while the category perspective
reflects category-level information. Accordingly, we have implemented a dual
contrastive learning approach from two perspectives to obtain discriminative
representations, which are applicable for Raman spectroscopy recognition under
both unsupervised and semi-supervised learning conditions. Furthermore, a
self-calibration mechanism is proposed to enhance robustness. Validation of the
identification task on three large-scale bacterial Raman spectroscopy datasets
demonstrates that our SCDC method achieves robust recognition performance with
very few (5$\%$ or 10$\%$) or no annotations, highlighting the potential of the
proposed method for biospectral identification in annotation-efficient clinical
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language
  and Speaker Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji-Hoon Kim, Hong-Sun Yang, Yoon-Cheol Ju, Il-Hwan Kim, Byeong-Yeol Kim, Joon Son Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this work is to generate natural speech in multiple languages
while maintaining the same speaker identity, a task known as cross-lingual
speech synthesis. A key challenge of cross-lingual speech synthesis is the
language-speaker entanglement problem, which causes the quality of
cross-lingual systems to lag behind that of intra-lingual systems. In this
paper, we propose CrossSpeech++, which effectively disentangles language and
speaker information and significantly improves the quality of cross-lingual
speech synthesis. To this end, we break the complex speech generation pipeline
into two simple components: language-dependent and speaker-dependent
generators. The language-dependent generator produces linguistic variations
that are not biased by specific speaker attributes. The speaker-dependent
generator models acoustic variations that characterize speaker identity. By
handling each type of information in separate modules, our method can
effectively disentangle language and speaker representation. We conduct
extensive experiments using various metrics, and demonstrate that CrossSpeech++
achieves significant improvements in cross-lingual speech synthesis,
outperforming existing methods by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Random Sampling of Diffused Graph Signals with Sparse Inputs on
  Vertex Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingcheng Lai, Li Chai, Jinming Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sampling of graph signals has recently drawn much attention due to the
wide applications of graph signal processing. While a lot of efficient methods
and interesting results have been reported to the sampling of band-limited or
smooth graph signals, few research has been devoted to non-smooth graph
signals, especially to sparse graph signals, which are also of importance in
many practical applications. This paper addresses the random sampling of
non-smooth graph signals generated by diffusion of sparse inputs. We aim to
present a solid theoretical analysis on the random sampling of diffused sparse
graph signals, which can be parallel to that of band-limited graph signals, and
thus present a sufficient condition to the number of samples ensuring the
unique recovery for uniform random sampling. Then, we focus on two classes of
widely used binary graph models, and give explicit and tighter estimations on
the sampling numbers ensuring unique recovery. We also propose an adaptive
variable-density sampling strategy to provide a better performance than uniform
random sampling. Finally, simulation experiments are presented to validate the
effectiveness of the theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied AI-empowered Low Altitude Economy: Integrated Sensing,
  Communications, Computation, and Control (ISC3) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoqi Yang, Yong Chen, Jiacheng Wang, Geng Sun, Dusit Niyato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low altitude economy (LAE) holds immense potential to drive urban development
across various sectors. However, LAE also faces challenges in data collection
and processing efficiency, flight control precision, and network performance.
The challenges could be solved by realizing an integration of sensing,
communications, computation, and control (ISC3) for LAE. In this regard,
embodied artificial intelligence (EAI), with its unique perception, planning,
and decision-making capabilities, offers a promising solution to realize ISC3.
Specifically, this paper investigates an application of EAI into ISC3 to
support LAE, exploring potential research focuses, solutions, and case study.
We begin by outlining rationales and benefits of introducing EAI into LAE,
followed by reviewing research directions and solutions for EAI in ISC3. We
then propose a framework of an EAI-enabled ISC3 for LAE. The framework's
effectiveness is evaluated through a case study of express delivery utilizing
an EAI-enabled UAV. Finally, we discuss several future research directions for
advancing EAI-enabled LAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Movable-Element STARS for Wireless Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Zhao, Quan Zhou, Xidong Mu, Kaiquan Cai, Yanbo Zhu, Yuanwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel movable-element enabled simultaneously transmitting and reflecting
surface (ME-STARS) communication system is proposed, where ME-STARS elements
positions can be adjusted to enhance the degress-of-freedom for transmission
and reflection. For each ME-STARS operating protocols, namely energy-splitting
(ES), mode switching (MS), and time switching (TS), a weighted sum rate (WSR)
maximization problem is formulated to jointly optimize the active beamforming
at the base station (BS) as well as the elements positions and passive
beamforming at the ME-STARS. An alternative optimization (AO)-based iterative
algorithm is developed to decompose the original non-convex problem into three
subproblems. Specifically, the gradient descent algorithm is employed for
solving the ME-STARS element position optimization subproblem, and the weighted
minimum mean square error and the successive convex approximation methods are
invoked for solving the active and passive beamforming subproblems,
respectively. It is further demonstrated that the proposed AO algorithm for ES
can be extended to solve the problems for MS and TS. Numerical results unveil
that: 1) the ME-STARS can significantly improve the WSR compared to the STARS
with fixed position elements and the conventional reconfigurable intelligent
surface with movable elements, thanks to the extra spatial-domain diversity and
the higher flexibility in beamforming; and 2) the performance gain of ME-STARS
is significant in the scenarios with larger number of users or more scatterers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobileNetV2: A lightweight classification model for home-based sleep
  apnea screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Pan, Yanxuan Yu, Jilun Ye, Xu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study proposes a novel lightweight neural network model leveraging
features extracted from electrocardiogram (ECG) and respiratory signals for
early OSA screening. ECG signals are used to generate feature spectrograms to
predict sleep stages, while respiratory signals are employed to detect
sleep-related breathing abnormalities. By integrating these predictions, the
method calculates the apnea-hypopnea index (AHI) with enhanced accuracy,
facilitating precise OSA diagnosis.
  The method was validated on three publicly available sleep apnea databases:
the Apnea-ECG database, the UCDDB dataset, and the MIT-BIH Polysomnographic
database. Results showed an overall OSA detection accuracy of 0.978,
highlighting the model's robustness. Respiratory event classification achieved
an accuracy of 0.969 and an area under the receiver operating characteristic
curve (ROC-AUC) of 0.98. For sleep stage classification, in UCDDB dataset, the
ROC-AUC exceeded 0.85 across all stages, with recall for Sleep reaching 0.906
and specificity for REM and Wake states at 0.956 and 0.937, respectively.
  This study underscores the potential of integrating lightweight neural
networks with multi-signal analysis for accurate, portable, and cost-effective
OSA screening, paving the way for broader adoption in home-based and wearable
health monitoring systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Calibration Model for Low-cost Sensor in Fine-grained Time
  series <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokho Ahn, Hyungjin Kim, Sungbok Shin, Young-Duk Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise measurements from sensors are crucial, but data is usually collected
from low-cost, low-tech systems, which are often inaccurate. Thus, they require
further calibrations. To that end, we first identify three requirements for
effective calibration under practical low-tech sensor conditions. Based on the
requirements, we develop a model called TESLA, Transformer for effective sensor
calibration utilizing logarithmic-binned attention. TESLA uses a
high-performance deep learning model, Transformers, to calibrate and capture
non-linear components. At its core, it employs logarithmic binning to minimize
attention complexity. TESLA achieves consistent real-time calibration, even
with longer sequences and finer-grained time series in hardware-constrained
systems. Experiments show that TESLA outperforms existing novel deep learning
and newly crafted linear models in accuracy, calibration speed, and energy
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Random Sampling of Diffused Graph Signals with Sparse Inputs on
  Vertex Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingcheng Lai, Li Chai, Jinming Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sampling of graph signals has recently drawn much attention due to the
wide applications of graph signal processing. While a lot of efficient methods
and interesting results have been reported to the sampling of band-limited or
smooth graph signals, few research has been devoted to non-smooth graph
signals, especially to sparse graph signals, which are also of importance in
many practical applications. This paper addresses the random sampling of
non-smooth graph signals generated by diffusion of sparse inputs. We aim to
present a solid theoretical analysis on the random sampling of diffused sparse
graph signals, which can be parallel to that of band-limited graph signals, and
thus present a sufficient condition to the number of samples ensuring the
unique recovery for uniform random sampling. Then, we focus on two classes of
widely used binary graph models, and give explicit and tighter estimations on
the sampling numbers ensuring unique recovery. We also propose an adaptive
variable-density sampling strategy to provide a better performance than uniform
random sampling. Finally, simulation experiments are presented to validate the
effectiveness of the theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures. arXiv admin note: text overlap with
  arXiv:1612.09565 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Zak-<span class="highlight-title">OTFS</span> with Interleaved Pilots to Extend the Region of Predictable
  Operation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinu Jayachandran, Imran Ali Khan, Saif Khan Mohammed, <span class="highlight-author">Ronny Hadani</span>, Ananthanarayanan Chockalingam, Robert Calderbank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When the delay period of the Zak-OTFS carrier is greater than the delay
spread of the channel, and the Doppler period of the carrier is greater than
the Doppler spread of the channel, the effective channel filter taps can simply
be read off from the response to a single pilot carrier waveform. The
input-output (I/O) relation can then be reconstructed for a sampled system that
operates under finite duration and bandwidth constraints. We introduce a
framework for pilot design in the delay-Doppler (DD) domain which makes it
possible to support users with very different delay-Doppler characteristics
when it is not possible to choose a single delay and Doppler period to support
all users. The method is to interleave single pilots in the DD domain, and to
choose the pilot spacing so that the I/O relation can be reconstructed by
solving a small linear system of equations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Approximate Wave-Number Domain Expression for Near-Field XL-array
  Channel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11446v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11446v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Xing, Yuxiang Zhang, Jianhua Zhang, Huixin Xu, Guangyi Liu, Qixing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Extremely large-scale array (XL-array) technology advances and carrier
frequency rises, the near-field effects in communication are intensifying. In
near-field conditions, channels exhibit a diffusion phenomenon in the angular
domain, existing research indicates that this phenomenon can be leveraged for
efficient parameter estimation and beam training. However, the channel model in
angular domain lacks closed-form analysis, making the time complexity of the
corresponding algorithm high. To address this issue, this paper analyzes the
near-field diffusion effect in the wave-number domain, where the wave-number
domain can be viewed as the continuous form of the angular domain. A
closed-form approximate wave-number domain expression is proposed, based on the
Principle of Stationary Phase. Subsequently, we derive a simplified expression
for the case where the user distance is much larger than the array aperture,
which is more concise. Subsequently, we verify the accuracy of the proposed
approximate expression through simulations and demonstrate its effectiveness
using a beam training example. Results indicate that the beam training scheme,
improved by the wave-number domain approximation model, can effectively
estimate near-field user parameters and perform beam training using far-field
DFT codebooks. Moreover, its performance surpasses that of existing DFT
codebook-based beam training methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Finite-Time Adaptive Fuzzy Tracking Control for Nonlinear State
  Constrained Pure-Feedback Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju Wu, Tong Wang, Min Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the finite-time adaptive fuzzy tracking control
problem for a class of pure-feedback system with full-state constraints. With
the help of Mean-Value Theorem, the pure-feedback nonlinear system is
transformed into strict-feedback case. By employing finite-time-stable like
function and state transformation for output tracking error, the output
tracking error converges to a predefined set in a fixed finite interval. To
tackle the problem of state constraints, integral Barrier Lyapunov functions
are utilized to guarantee that the state variables remain within the prescribed
constraints with feasibility check. Fuzzy logic systems are utilized to
approximate the unknown nonlinear functions. In addition, all the signals in
the closed-loop system are guaranteed to be semi-global ultimately uniformly
bounded. Finally, two simulation examples are given to show the effectiveness
of the proposed control strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>typos checked and corrected in 'Introduction'</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Semi-Autonomous Robotic Arm Manipulation Operator Intention
  Detection from Force Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Alharthi, Ozan Tokatli, Erwin Lopez, Guido Herrmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In hazardous environments like nuclear facilities, robotic systems are
essential for executing tasks that would otherwise expose humans to dangerous
radiation levels, which pose severe health risks and can be fatal. However,
many operations in the nuclear environment require teleoperating robots,
resulting in a significant cognitive load on operators as well as physical
strain over extended periods of time. To address this challenge, we propose
enhancing the teleoperation system with an assistive model capable of
predicting operator intentions and dynamically adapting to their needs. The
machine learning model processes robotic arm force data, analyzing
spatiotemporal patterns to accurately detect the ongoing task before its
completion. To support this approach, we collected a diverse dataset from
teleoperation experiments involving glovebox tasks in nuclear applications.
This dataset encompasses heterogeneous spatiotemporal data captured from the
teleoperation system. We employ a hybrid Convolutional Neural Network (CNN) and
Long Short-Term Memory (LSTM) model to learn and forecast operator intentions
based on the spatiotemporal data. By accurately predicting these intentions,
the robot can execute tasks more efficiently and effectively, requiring minimal
input from the operator. Our experiments validated the model using the dataset,
focusing on tasks such as radiation surveys and object grasping. The proposed
approach demonstrated an F1-score of 89% for task classification and an
F1-score of 86% classification forecasted operator intentions over a 5-second
window. These results highlight the potential of our method to improve the
safety, precision, and efficiency of robotic operations in hazardous
environments, thereby significantly reducing human radiation exposure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Fuzzy Tracking Control for Nonlinear State Constrained
  Pure-Feedback Systems With Input Delay via Dynamic Surface Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ju Wu, Tong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This brief constructs the adaptive backstepping control scheme for a class of
pure-feedback systems with input delay and full state constraints. With the
help of Mean Value Theorem, the pure-feedback system is transformed into
strict-feedback one. Barrier Lyapunov functions are employed to guarantee all
of the states remain constrained within predefined sets. By introducing the
Pade approximation method and corresponding intermediate, the impact generated
by input delay on the output tracking performance of the system can be
eliminated. Furthermore, a low-pass filter driven by a newly-defined control
input, is employed to generate the actual control input, which facilitates the
design of backstepping control. To approximate the unknown functions with a
desired level of accuracy, the fuzzy logic systems (FLSs) are utilized by
choosing appropriate fuzzy rules, logics and so on. The minimal learning
parameter (MLP) technique is employed to decrease the number of nodes and
parameters in FLSs, and dynamic surface control (DSC) technique is leveraged to
avoid so-called "explosion of complexity". Moreover, smooth robust compensators
are introduced to circumvent the influences of external disturbance and
approximation errors. By stability analysis, it is proved that all of signals
in the closed-loop system are semi-globally ultimately uniform bounded, and the
tracking error can be within a arbitrary small neighbor of origin via selecting
appropriate parameters of controllers. Finally, the results of numerical
illustration are provided to demonstrate the effectiveness of the designed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>typos checked and corrected, redo literature review to update
  introduction part</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BlueME: Robust Underwater Robot-to-Robot Communication Using Compact
  Magnetoelectric Antennas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehron Talebi, Sultan Mahmud, Adam Khalifa, Md Jahidul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the design, development, and experimental validation of BlueME, a
compact magnetoelectric (ME) antenna array system for underwater robot-to-robot
communication. BlueME employs ME antennas operating at their natural mechanical
resonance frequency to efficiently transmit and receive very-low-frequency
(VLF) electromagnetic signals underwater. We outline the design, simulation,
fabrication, and integration of the proposed system on low-power embedded
platforms focusing on portable and scalable applications. For performance
evaluation, we deployed BlueME on an autonomous surface vehicle (ASV) and a
remotely operated vehicle (ROV) in open-water field trials. Our tests
demonstrate that BlueME maintains reliable signal transmission at distances
beyond 200 meters while consuming only 1 watt of power. Field trials show that
the system operates effectively in challenging underwater conditions such as
turbidity, obstacles, and multipath interference -- that generally affect
acoustics and optics. Our analysis also examines the impact of complete
submersion on system performance and identifies key deployment considerations.
This work represents the first practical underwater deployment of ME antennas
outside the laboratory, and implements the largest VLF ME array system to date.
BlueME demonstrates significant potential for marine robotics and automation in
multi-robot cooperative systems and remote sensor networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EEG Right & Left Voluntary Hand Movement-based Virtual Brain-Computer
  Interfacing Keyboard Using Hybrid Deep Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biplov Paneru, Bipul Thapa, Bishwash Paneru, Sanjog Chhetri Sapkota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-machine interfaces (BMIs), particularly those based on
electroencephalography (EEG), offer promising solutions for assisting
individuals with motor disabilities. However, challenges in reliably
interpreting EEG signals for specific tasks, such as simulating keystrokes,
persist due to the complexity and variability of brain activity. Current
EEG-based BMIs face limitations in adaptability, usability, and robustness,
especially in applications like virtual keyboards, as traditional
machine-learning models struggle to handle high-dimensional EEG data
effectively. To address these gaps, we developed an EEG-based BMI system
capable of accurately identifying voluntary keystrokes, specifically leveraging
right and left voluntary hand movements. Using a publicly available EEG
dataset, the signals were pre-processed with band-pass filtering, segmented
into 22-electrode arrays, and refined into event-related potential (ERP)
windows, resulting in a 19x200 feature array categorized into three classes:
resting state (0), 'd' key press (1), and 'l' key press (2). Our approach
employs a hybrid neural network architecture with BiGRU-Attention as the
proposed model for interpreting EEG signals, achieving superior test accuracy
of 90% and a mean accuracy of 91% in 10-fold stratified cross-validation. This
performance outperforms traditional ML methods like Support Vector Machines
(SVMs) and Naive Bayes, as well as advanced architectures such as Transformers,
CNN-Transformer hybrids, and EEGNet. Finally, the BiGRU-Attention model is
integrated into a real-time graphical user interface (GUI) to simulate and
predict keystrokes from brain activity. Our work demonstrates how deep learning
can advance EEG-based BMI systems by addressing the challenges of signal
interpretation and classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic GOSPA: A Metric for Performance Evaluation of Multi-Object
  Filters with Uncertainties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Xia, Ángel F. García-Fernández, Johan Karlsson, Ting Yuan, Kuo-Chu Chang, Lennart Svensson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This correspondence presents a probabilistic generalization of the
Generalized Optimal Sub-Pattern Assignment (GOSPA) metric, termed P-GOSPA. The
GOSPA metric is widely used to evaluate the distance between finite sets,
particularly in multi-object estimation applications. The P-GOSPA extends GOSPA
into the space of multi-Bernoulli densities, incorporating inherent uncertainty
in probabilistic multi-object representations. Additionally, P-GOSPA retains
the interpretability of GOSPA, such as its decomposition into localization,
missed detection, and false detection errors in a sound and meaningful manner.
Examples and simulations are provided to demonstrate the efficacy of the
proposed P-GOSPA metric.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-27T00:00:00Z">2024-12-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Cryptography and Security <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreStega: A Plug-and-Play Method for Boosting Imperceptibility and
  Capacity in Generative Linguistic Steganography for Real-World Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaiyi Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linguistic steganography embeds secret information in seemingly innocent
texts, safeguarding privacy in surveillance environments. Generative linguistic
steganography leverages the probability distribution of language models (LMs)
and applies steganographic algorithms to generate stego tokens, gaining
attention with recent Large Language Model (LLM) advancements. To enhance
security, researchers develop distribution-preserving stego algorithms to
minimize the gap between stego sampling and LM sampling. However, the reliance
on language model distributions, coupled with deviations from real-world cover
texts, results in insufficient imperceptibility when facing steganalysis
detectors in real-world scenarios. Moreover, LLM distributions tend to be more
deterministic, resulting in reduced entropy and, consequently, lower embedding
capacity. In this paper, we propose FreStega, a plug-and-play method to
reconstruct the distribution of language models used for generative linguistic
steganography. FreStega dynamically adjusts token probabilities from the
language model at each step of stegotext auto-regressive generation, leveraging
both sequential and spatial dimensions. In sequential adjustment, the
temperature is dynamically adjusted based on instantaneous entropy, enhancing
the diversity of stego texts and boosting embedding capacity. In the spatial
dimension, the distribution is aligned with guidance from the target domain
corpus, closely mimicking real cover text in the target domain. By reforming
the distribution, FreStega enhances the imperceptibility of stego text in
practical scenarios and improves steganographic capacity by 15.41\%, all
without compromising the quality of the generated text. FreStega serves as a
plug-and-play remedy to enhance the imperceptibility and embedding capacity of
existing distribution-preserving steganography methods in real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Let Watermarks Speak: A Robust and Unforgeable Watermark for Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minhao Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking is an effective way to trace model-generated content. Current
watermark methods cannot resist forgery attacks, such as a deceptive claim that
the model-generated content is a response to a fabricated prompt. None of them
can be made unforgeable without degrading robustness.
  Unforgeability demands that the watermarked output is not only detectable but
also verifiable for integrity, indicating whether it has been modified. This
underscores the necessity and significance of a multi-bit watermarking scheme.
  Recent works try to build multi-bit scheme based on existing zero-bit
watermarking scheme, but they either degrades the robustness or brings a
significant computational burden. We aim to design a novel single-bit watermark
scheme, which provides the ability to embed 2 different watermark signals.
  This paper's main contribution is that we are the first to propose an
undetectable, robust, single-bit watermarking scheme. It has a comparable
robustness to the most advanced zero-bit watermarking schemes. Then we
construct a multi-bit watermarking scheme to use the hash value of prompt or
the newest generated content as the watermark signals, and embed them into the
following content, which guarantees the unforgeability.
  Additionally, we provide sufficient experiments on some popular language
models, while the other advanced methods with provable guarantees do not often
provide. The results show that our method is practically effective and robust.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for
  Large Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) exhibit impressive potential across
various tasks but also face significant privacy risks, limiting their practical
applications. Current researches on privacy assessment for LVLMs is limited in
scope, with gaps in both assessment dimensions and privacy categories. To
bridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for
evaluating the privacy preservation capabilities of LVLMs in terms of privacy
awareness and leakage. Privacy awareness measures the model's ability to
recognize the privacy sensitivity of input data, while privacy leakage assesses
the risk of the model unintentionally disclosing privacy information in its
output. We design a range of sub-tasks to thoroughly evaluate the model's
privacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of
personal privacy, 15 categories of trade secrets, and 18 categories of state
secrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the
privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.
Our results reveal that current LVLMs generally pose a high risk of
facilitating privacy breaches, with vulnerabilities varying across personal
privacy, trade secret, and state secret.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Engorgio Prompt Makes Large Language Model Babble on 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianshuo Dong, Ziyuan Zhang, Qingjie Zhang, Han Qiu, Tianwei Zhang, Hao Wang, Hewu Li, Qi Li, Chao Zhang, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auto-regressive large language models (LLMs) have yielded impressive
performance in many real-world tasks. However, the new paradigm of these LLMs
also exposes novel threats. In this paper, we explore their vulnerability to
inference cost attacks, where a malicious user crafts Engorgio prompts to
intentionally increase the computation cost and latency of the inference
process. We design Engorgio, a novel methodology, to efficiently generate
adversarial Engorgio prompts to affect the target LLM's service availability.
Engorgio has the following two technical contributions. (1) We employ a
parameterized distribution to track LLMs' prediction trajectory. (2) Targeting
the auto-regressive nature of LLMs' inference process, we propose novel loss
functions to stably suppress the appearance of the <EOS> token, whose
occurrence will interrupt the LLM's generation process. We conduct extensive
experiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B.
The results show that Engorgio prompts can successfully induce LLMs to generate
abnormally long outputs (i.e., roughly 2-13$\times$ longer to reach 90%+ of the
output length limit) in a white-box scenario and our real-world experiment
demonstrates Engergio's threat to LLM service with limited computing resources.
The code is accessible at https://github.com/jianshuod/Engorgio-prompt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Standard-Deviation-Inspired Regularization for Improving Adversarial
  Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olukorede Fakorede, Modeste Atsague, Jin Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial Training (AT) has been demonstrated to improve the robustness of
deep neural networks (DNNs) against adversarial attacks. AT is a min-max
optimization procedure where in adversarial examples are generated to train a
more robust DNN. The inner maximization step of AT increases the losses of
inputs with respect to their actual classes. The outer minimization involves
minimizing the losses on the adversarial examples obtained from the inner
maximization. This work proposes a standard-deviation-inspired (SDI)
regularization term to improve adversarial robustness and generalization. We
argue that the inner maximization in AT is similar to minimizing a modified
standard deviation of the model's output probabilities. Moreover, we suggest
that maximizing this modified standard deviation can complement the outer
minimization of the AT framework. To support our argument, we experimentally
show that the SDI measure can be used to craft adversarial examples.
Additionally, we demonstrate that combining the SDI regularization term with
existing AT variants enhances the robustness of DNNs against stronger attacks,
such as CW and Auto-attack, and improves generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outfox: a Packet Format for a Layered Mixnet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfredo Rial, Ania M. Piotrowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Outfox, a packet format based on layered encryption that is
suitable for mixnets in which all paths have the same length and where all mix
nodes are associated with a single layer. Outfox is a variant of the packet
format Sphinx that removes unnecessary padding and optimizes the computation
cost of packet processing by halving the number of public key operations
performed by mix nodes. Outfox uses a KEM scheme as a building block and is
quantum-safe when instantiated with a quantum-safe KEM scheme. To analyze the
security of Outfox, we describe an ideal functionality for a layered replyable
mixnet that requires reply-request indistinguishability, and a construction
based on Outfox that realizes our ideal functionality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Convergence of DP-SGD with Adaptive Clipping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Egor Shulgin, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic Gradient Descent (SGD) with gradient clipping is a powerful
technique for enabling differentially private optimization. Although prior
works extensively investigated clipping with a constant threshold, private
training remains highly sensitive to threshold selection, which can be
expensive or even infeasible to tune. This sensitivity motivates the
development of adaptive approaches, such as quantile clipping, which have
demonstrated empirical success but lack a solid theoretical understanding. This
paper provides the first comprehensive convergence analysis of SGD with
quantile clipping (QC-SGD). We demonstrate that QC-SGD suffers from a bias
problem similar to constant-threshold clipped SGD but show how this can be
mitigated through a carefully designed quantile and step size schedule. Our
analysis reveals crucial relationships between quantile selection, step size,
and convergence behavior, providing practical guidelines for parameter
selection. We extend these results to differentially private optimization,
establishing the first theoretical guarantees for DP-QC-SGD. Our findings
provide theoretical foundations for widely used adaptive clipping heuristic and
highlight open avenues for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gröbner Basis Cryptanalysis of Ciminion and Hydra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.05040v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.05040v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Johann Steiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ciminion and Hydra are two recently introduced symmetric key Pseudo-Random
Functions for Multi-Party Computation applications. For efficiency both
primitives utilize quadratic permutations at round level. Therefore, polynomial
system solving-based attacks pose a serious threat to these primitives. For
Ciminion, we construct a quadratic degree reverse lexicographic (DRL) Gr\"obner
basis for the iterated polynomial model via linear transformations. With the
Gr\"obner basis we can simplify cryptanalysis since we do not need to impose
genericity assumptions anymore to derive complexity estimations. For Hydra,
with the help of a computer algebra program like SageMath we construct a DRL
Gr\"obner basis for the iterated model via linear transformations and a linear
change of coordinates. In the Hydra proposal it was claimed that $r_\mathcal{H}
= 31$ rounds are sufficient to provide $128$ bits of security against Gr\"obner
basis attacks for an ideal adversary with $\omega = 2$. However, via our Hydra
Gr\"obner basis standard term order conversion to a lexicographic (LEX)
Gr\"obner basis requires just $126$ bits with $\omega = 2$. Moreover, via a
dedicated polynomial system solving technique up to $r_\mathcal{H} = 33$ rounds
can be attacked below $128$ bits for an ideal adversary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GenDFIR: Advancing Cyber Incident Timeline Analysis Through Retrieval
  Augmented Generation and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02572v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02572v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatma Yasmine Loumachi, Mohamed Chahine Ghanem, Mohamed Amine Ferrag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cyber timeline analysis, or forensic timeline analysis, is crucial in Digital
Forensics and Incident Response (DFIR). It examines artefacts and events
particularly timestamps and metadata to detect anomalies, establish
correlations, and reconstruct incident timelines. Traditional methods rely on
structured artefacts, such as logs and filesystem metadata, using specialised
tools for evidence identification and feature extraction. This paper introduces
GenDFIR, a framework leveraging large language models (LLMs), specifically
Llama 3.1 8B in zero shot mode, integrated with a Retrieval-Augmented
Generation (RAG) agent. Incident data is preprocessed into a structured
knowledge base, enabling the RAG agent to retrieve relevant events based on
user prompts. The LLM interprets this context, offering semantic enrichment.
Tested on synthetic data in a controlled environment, results demonstrate
GenDFIR's reliability and robustness, showcasing LLMs potential to automate
timeline analysis and advance threat detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages V5.3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Mathematical Framework for the Problem of Security for Cognition in
  Neurotechnology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07945v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07945v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryce Allen Bagley, Claudia K Petritsch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement in neurotechnology in recent years has created an
emerging critical intersection between neurotechnology and security.
Implantable devices, non-invasive monitoring, and non-invasive therapies all
carry with them the prospect of violating the privacy and autonomy of
individuals' cognition. A growing number of scientists and physicians have made
calls to address this issue, but applied efforts have been relatively limited.
A major barrier hampering scientific and engineering efforts to address these
security issues is the lack of a clear means of describing and analyzing
relevant problems. In this paper we develop Cognitive Neurosecurity, a
mathematical framework which enables such description and analysis by drawing
on methods and results from multiple fields. We demonstrate certain statistical
properties which have significant implications for Cognitive Neurosecurity, and
then present descriptions of the algorithmic problems faced by attackers
attempting to violate privacy and autonomy, and defenders attempting to
obstruct such attempts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Agent Collaboration in Incident Response with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00652v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00652v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zefang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incident response (IR) is a critical aspect of cybersecurity, requiring rapid
decision-making and coordinated efforts to address cyberattacks effectively.
Leveraging large language models (LLMs) as intelligent agents offers a novel
approach to enhancing collaboration and efficiency in IR scenarios. This paper
explores the application of LLM-based multi-agent collaboration using the
Backdoors & Breaches framework, a tabletop game designed for cybersecurity
training. We simulate real-world IR dynamics through various team structures,
including centralized, decentralized, and hybrid configurations. By analyzing
agent interactions and performance across these setups, we provide insights
into optimizing multi-agent collaboration for incident response. Our findings
highlight the potential of LLMs to enhance decision-making, improve
adaptability, and streamline IR processes, paving the way for more effective
and coordinated responses to cyber threats.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02946v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02946v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dillon Bowen, Brendan Murphy, Will Cai, David Khachaturov, Adam Gleave, Kellin Pelrine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs produce harmful and undesirable behavior when trained on poisoned
datasets that contain a small fraction of corrupted or harmful data. We develop
a new attack paradigm, jailbreak-tuning, that combines data poisoning with
jailbreaking to fully bypass state-of-the-art safeguards and make models like
GPT-4o comply with nearly any harmful request. Our experiments suggest this
attack represents a paradigm shift in vulnerability elicitation, producing
differences in refusal rates as much as 60+ percentage points compared to
normal fine-tuning. Given this demonstration of how data poisoning
vulnerabilities persist and can be amplified, we investigate whether these
risks will likely increase as models scale. We evaluate three threat models -
malicious fine-tuning, imperfect data curation, and intentional data
contamination - across 24 frontier LLMs ranging from 1.5 to 72 billion
parameters. Our experiments reveal that larger LLMs are significantly more
susceptible to data poisoning, learning harmful behaviors from even minimal
exposure to harmful data more quickly than smaller models. These findings
underscore the need for leading AI companies to thoroughly red team fine-tuning
APIs before public release and to develop more robust safeguards against data
poisoning, particularly as models continue to scale in size and capability.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Distributed, Parallel, and Cluster Computing <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric Freeze-Tag Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharareh Alipour, Kajal Baghestani, Mahdis Mirzaei, Soroush Sahraei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the Freeze-Tag Problem (FTP), introduced by Arkin et al. (SODA'02),
where the objective is to activate a group of n robots, starting from a single
initially active robot. Robots are positioned in $\mathbb{R}^d$, and once
activated, they move at a constant speed to wake up others. The goal is to
minimize the time required to activate the last robot, known as the makespan.
We establish new upper bounds for the makespan under the $l_1$ and $l_2$ norms
in $\mathbb{R}^2$ and $\mathbb{R}^3$. Specifically, we improve the previous
upper bound for $(\mathbb{R}^2, l_2)$ from $7.07r$ (Bonichon et al., DISC'24)
to $5.064r$. For $(\mathbb{R}^3, l_1)$, we derive a makespan bound of $13r$,
which translates to $22.52r$ for $(\mathbb{R}^3, l_2)$. Here, $r$ denotes the
maximum distance of any robot from the initially active robot under the given
norm. To our knowledge, these are the first makespan bounds for FTP in
$\mathbb{R}^3$. Additionally, we show that the maximum makespan for $n$ robots
is not necessarily achieved when robots are equally distributed along the
boundary in $(\mathbb{R}^2, l_2)$. We further investigate FTP in
$(\mathbb{R}^3, l_2)$ for specific configurations where robots lie on a
boundary, providing insights into practical scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymmetrical Reciprocity-based Federated Learning for Resolving
  Disparities in Medical Diagnosis <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geographic health disparities pose a pressing global challenge, particularly
in underserved regions of low- and middle-income nations. Addressing this issue
requires a collaborative approach to enhance healthcare quality, leveraging
support from medically more developed areas. Federated learning emerges as a
promising tool for this purpose. However, the scarcity of medical data and
limited computation resources in underserved regions make collaborative
training of powerful machine learning models challenging. Furthermore, there
exists an asymmetrical reciprocity between underserved and developed regions.
To overcome these challenges, we propose a novel cross-silo federated learning
framework, named FedHelp, aimed at alleviating geographic health disparities
and fortifying the diagnostic capabilities of underserved regions.
Specifically, FedHelp leverages foundational model knowledge via one-time API
access to guide the learning process of underserved small clients, addressing
the challenge of insufficient data. Additionally, we introduce a novel
asymmetric dual knowledge distillation module to manage the issue of asymmetric
reciprocity, facilitating the exchange of necessary knowledge between developed
large clients and underserved small clients. We validate the effectiveness and
utility of FedHelp through extensive experiments on both medical image
classification and segmentation tasks. The experimental results demonstrate
significant performance improvement compared to state-of-the-art baselines,
particularly benefiting clients in underserved regions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Jiaqi Wang and Ziyi Yin equally contributed to this paper. This paper
  has been accepted by KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Download from an External Data Source in Faulty Majority
  Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We extend the study of retrieval problems in distributed networks, focusing
on improving the efficiency and resilience of protocols in the \emph{Data
Retrieval (DR) Model}. The DR Model consists of a complete network (i.e., a
clique) with $k$ peers, up to $\beta k$ of which may be Byzantine (for $\beta
\in [0, 1)$), and a trusted \emph{External Data Source} comprising an array $X$
of $n$ bits ($n \gg k$) that the peers can query. Additionally, the peers can
also send messages to each other. In this work, we focus on the Download
problem that requires all peers to learn $X$. Our primary goal is to minimize
the maximum number of queries made by any honest peer and additionally optimize
time.
  We begin with a randomized algorithm for the Download problem that achieves
optimal query complexity up to a logarithmic factor. For the stronger dynamic
adversary that can change the set of Byzantine peers from one round to the
next, we achieve the optimal time complexity in peer-to-peer communication but
with larger messages. In broadcast communication where all peers (including
Byzantine peers) are required to send the same message to all peers, with
larger messages, we achieve almost optimal time and query complexities for a
dynamic adversary. Finally, in a more relaxed crash fault model, where peers
stop responding after crashing, we address the Download problem in both
synchronous and asynchronous settings. Using a deterministic protocol, we
obtain nearly optimal results for both query complexity and message sizes in
these scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adrenaline: Adaptive Rendering Optimization System for Scalable Cloud
  Gaming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Heo, Ketan Bhardwaj, Ada Gavrilovska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud gaming requires a low-latency network connection, making it a prime
candidate for being hosted at the network edge. However, an edge server is
provisioned with a fixed compute capacity, causing an issue for multi-user
service and resulting in users having to wait before they can play when the
server is occupied. In this work, we present a new insight that when a user's
network condition results in use of lossy compression, the end-to-end visual
quality more degrades for frames of high rendering quality, wasting the
server's computing resources. We leverage this observation to build Adrenaline,
a new system which adaptively optimizes the game rendering qualities by
considering the user-side visual quality and server-side rendering cost. The
rendering quality optimization of Adrenaline is done via a scoring mechanism
quantifying the effectiveness of server resource usage on the user-side gaming
quality. Our open-sourced implementation of Adrenaline demonstrates easy
integration with modern game engines. In our evaluations, Adrenaline achieves
up to 24% higher service quality and 2x more users served with the same
resource footprint compared to other baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 13 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Large Language Model Acceleration based on KV Cache
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized a wide range of domains such
as natural language processing, computer vision, and multi-modal tasks due to
their ability to comprehend context and perform logical reasoning. However, the
computational and memory demands of LLMs, particularly during inference, pose
significant challenges when scaling them to real-world, long-context, and
real-time applications. Key-Value (KV) cache management has emerged as a
critical optimization technique for accelerating LLM inference by reducing
redundant computations and improving memory utilization. This survey provides a
comprehensive overview of KV cache management strategies for LLM acceleration,
categorizing them into token-level, model-level, and system-level
optimizations. Token-level strategies include KV cache selection, budget
allocation, merging, quantization, and low-rank decomposition, while
model-level optimizations focus on architectural innovations and attention
mechanisms to enhance KV reuse. System-level approaches address memory
management, scheduling, and hardware-aware designs to improve efficiency across
diverse computing environments. Additionally, the survey provides an overview
of both text and multimodal datasets and benchmarks used to evaluate these
strategies. By presenting detailed taxonomies and comparative analyses, this
work aims to offer useful insights for researchers and practitioners to support
the development of efficient and scalable KV cache management techniques,
contributing to the practical deployment of LLMs in real-world applications.
The curated paper list for KV cache management is in:
\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pilot-<span class="highlight-title">Quantum</span>: A <span class="highlight-title">Quantum</span>-HPC Middleware for Resource, Workload and Task
  Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pradeep Mantha, Florian J. Kiwit, Nishant Saurabh, Shantenu Jha, Andre Luckow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As quantum hardware continues to scale, managing the heterogeneity of
resources and applications -- spanning diverse quantum and classical hardware
and software frameworks -- becomes increasingly critical. Pilot-Quantum
addresses these challenges as a middleware designed to provide unified
application-level management of resources and workloads across hybrid
quantum-classical environments. It is built on a rigorous analysis of existing
quantum middleware systems and application execution patterns. It implements
the Pilot Abstraction conceptual model, originally developed for HPC, to manage
resources, workloads, and tasks. It is designed for quantum applications that
rely on task parallelism, including: (i) Hybrid algorithms, such as variational
approaches, and (ii) Circuit cutting systems, used to partition and execute
large quantum circuits. Pilot-Quantum facilitates seamless integration of
quantum processing units (QPUs), classical CPUs, and GPUs, while supporting
high-level programming frameworks like Qiskit and Pennylane. This enables users
to design and execute hybrid workflows across diverse computing resources
efficiently. The capabilities of Pilot-Quantum are demonstrated through
mini-applications -- simplified yet representative kernels focusing on critical
performance bottlenecks. We present several mini-apps, including circuit
execution across hardware and simulator platforms (e.g., IBM's Eagle QPU),
distributed state vector simulation, circuit cutting, and quantum machine
learning workflows, demonstrating significant scale (e.g., a 41-qubit
simulation on 256 GPUs) and speedups (e.g., 15x for QML, 3.5x for circuit
cutting).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLOR: Automated Repair of OpenMP Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utpal Bora, Saurabh Joshi, Gautam Muduganti, Ramakrishna Upadrasta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a technique for repairing data race errors in
parallel programs written in C/C++ and Fortran using the OpenMP API. Our
technique can also remove barriers that are deemed unnecessary for correctness.
We implement these ideas in our tool called LLOR, which takes a
language-independent approach to provide appropriate placements of
synchronization constructs to avoid data races. To the best of our knowledge,
LLOR is the only tool that can repair parallel programs that use the OpenMP
API. We showcase the capabilities of LLOR by performing extensive experiments
on 415 parallel programs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 1 algorithm, 2 figures, 26th International Conference on
  Verification Model Checking and Abstract Interpretation (VMCAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Starting Point for Dynamic Community Detection with Leiden Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11658v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11658v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhajit Sahu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world graphs often evolve over time, making community or cluster
detection a crucial task. In this technical report, we extend three dynamic
approaches - Naive-dynamic (ND), Delta-screening (DS), and Dynamic Frontier
(DF) - to our multicore implementation of the Leiden algorithm, known for its
high-quality community detection. Our experiments, conducted on a server with a
64-core AMD EPYC-7742 processor, show that ND, DS, and DF Leiden achieve
average speedups of 1.37x, 1.47x, and 1.98x on large graphs with random batch
updates, compared to the Static Leiden algorithm - while scaling at a rate of
1.6x for every doubling of threads. To our knowledge, this is the first attempt
to apply dynamic approaches to the Leiden algorithm. We hope these early
results pave the way for further development of dynamic approaches for evolving
graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures, 3 tables. arXiv admin note: substantial text
  overlap with arXiv:2404.19634</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained
  Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Zhan, Wenkuan Zhao, Yuanqing Li, Weijie Liu, Xiaoxi Zhang, Chee Wei Tan, Chuan Wu, Deke Guo, Xu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a collaborative machine learning approach that
enables multiple clients to train models without sharing their private data.
With the rise of deep learning, large-scale models have garnered significant
attention due to their exceptional performance. However, a key challenge in FL
is the limitation imposed by clients with constrained computational and
communication resources, which hampers the deployment of these large models.
The Mixture of Experts (MoE) architecture addresses this challenge with its
sparse activation property, which reduces computational workload and
communication demands during inference and updates. Additionally, MoE
facilitates better personalization by allowing each expert to specialize in
different subsets of the data distribution. To alleviate the communication
burdens between the server and clients, we propose FedMoE-DA, a new FL model
training framework that leverages the MoE architecture and incorporates a novel
domain-aware, fine-grained aggregation strategy to enhance the robustness,
personalizability, and communication efficiency simultaneously. Specifically,
the correlation between both intra-client expert models and inter-client data
heterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)
communication between clients for selective expert model synchronization, thus
significantly reducing the server-client transmissions. Experiments demonstrate
that our FedMoE-DA achieves excellent performance while reducing the
communication pressure on the server.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, accepted by The 20th International Conference on
  Mobility, Sensing and Networking (MSN 2024)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Theory <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfAlign: Inference-aware language model alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananth Balashankar, Ziteng Sun, Jonathan Berant, Jacob Eisenstein, Michael Collins, Adrian Hutter, Jong Lee, Chirag Nagpal, Flavien Prost, Aradhana Sinha, and Ananda Theertha Suresh, Ahmad Beirami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model alignment has become a critical step in training modern
generative language models. The goal of alignment is to finetune a reference
model such that the win rate of a sample from the aligned model over a sample
from the reference model is high, subject to a KL divergence constraint. Today,
we are increasingly using inference-time algorithms (e.g., Best-of-N,
controlled decoding, tree search) to decode from language models rather than
standard sampling. However, the alignment objective does not capture such
inference-time decoding procedures. We show that the existing alignment
framework is sub-optimal in view of such inference-time methods. We then modify
the alignment objective and propose a framework for inference-aware alignment
(IAPO). We prove that for any inference-time decoding algorithm, the optimal
solution that optimizes the inference-time win rate of the aligned policy
against the reference policy is the solution to the typical RLHF problem with a
transformation of the reward. This motivates us to provide the KL-regularized
calibrate-and-transform RL (CTRL) algorithm to solve this problem, which
involves a reward calibration step and a KL-regularized reward maximization
step with a transformation of the calibrated reward. We particularize our study
to two important inference-time strategies: best-of-N sampling and best-of-N
jailbreaking, where N responses are sampled from the model and the one with the
highest or lowest reward is selected. We propose specific transformations for
these strategies and demonstrate that our framework offers significant
improvements over existing state-of-the-art methods for language model
alignment. Empirically, we outperform baselines that are designed without
taking inference-time decoding into consideration by 8-12% and 4-9% on
inference-time win rates over the Anthropic helpfulness and harmlessness dialog
benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV-Enabled Secure ISAC Against Dual Eavesdropping Threats: Joint
  Beamforming and Trajectory Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianping Yao, Zeyu Yang, Zai Yang, Jie Xu, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study an unmanned aerial vehicle (UAV)-enabled secure
integrated sensing and communication (ISAC) system, where a UAV serves as an
aerial base station (BS) to simultaneously perform communication with a user
and detect a target on the ground, while a dual-functional eavesdropper
attempts to intercept the signals for both sensing and communication. Facing
the dual eavesdropping threats, we aim to enhance the average achievable
secrecy rate for the communication user by jointly designing the UAV trajectory
together with the transmit information and sensing beamforming, while
satisfying the requirements on sensing performance and sensing security, as
well as the UAV power and flight constraints. To address the non-convex nature
of the optimization problem, we employ the alternating optimization (AO)
strategy, jointly with the successive convex approximation (SCA) and
semidefinite relaxation (SDR) methods. Numerical results validate the proposed
approach, demonstrating its ability to achieve a high secrecy rate while
meeting the required sensing and security constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, submitted for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep ReLU networks -- injectivity capacity upper bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihailo Stojnic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study deep ReLU feed forward neural networks (NN) and their injectivity
abilities. The main focus is on \emph{precisely} determining the so-called
injectivity capacity. For any given hidden layers architecture, it is defined
as the minimal ratio between number of network's outputs and inputs which
ensures unique recoverability of the input from a realizable output. A strong
recent progress in precisely studying single ReLU layer injectivity properties
is here moved to a deep network level. In particular, we develop a program that
connects deep $l$-layer net injectivity to an $l$-extension of the $\ell_0$
spherical perceptrons, thereby massively generalizing an isomorphism between
studying single layer injectivity and the capacity of the so-called
(1-extension) $\ell_0$ spherical perceptrons discussed in [82]. \emph{Random
duality theory} (RDT) based machinery is then created and utilized to
statistically handle properties of the extended $\ell_0$ spherical perceptrons
and implicitly of the deep ReLU NNs. A sizeable set of numerical evaluations is
conducted as well to put the entire RDT machinery in practical use. From these
we observe a rapidly decreasing tendency in needed layers' expansions, i.e., we
observe a rapid \emph{expansion saturation effect}. Only $4$ layers of depth
are sufficient to closely approach level of no needed expansion -- a result
that fairly closely resembles observations made in practical experiments and
that has so far remained completely untouchable by any of the existing
mathematical methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-augmented Generation for GenAI-enabled Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunpu Tang, Ruichen Zhang, Yuxuan Yan, Qianqian Yang, Dusit Niyato, Xianbin Wang, Shiwen Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic communication (SemCom) is an emerging paradigm aiming at
transmitting only task-relevant semantic information to the receiver, which can
significantly improve communication efficiency. Recent advancements in
generative artificial intelligence (GenAI) have empowered GenAI-enabled SemCom
(GenSemCom) to further expand its potential in various applications. However,
current GenSemCom systems still face challenges such as semantic inconsistency,
limited adaptability to diverse tasks and dynamic environments, and the
inability to leverage insights from past transmission. Motivated by the success
of retrieval-augmented generation (RAG) in the domain of GenAI, this paper
explores the integration of RAG in GenSemCom systems. Specifically, we first
provide a comprehensive review of existing GenSemCom systems and the
fundamentals of RAG techniques. We then discuss how RAG can be integrated into
GenSemCom. Following this, we conduct a case study on semantic image
transmission using an RAG-enabled diffusion-based SemCom system, demonstrating
the effectiveness of the proposed integration. Finally, we outline future
directions for advancing RAG-enabled GenSemCom systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movable Antenna-Aided Near-Field Integrated Sensing and Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingze Ding, Zijian Zhou, Xiaodan Shao, Bingli Jiao, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated sensing and communication (ISAC) is emerging as a pivotal
technology for next-generation wireless networks. However, existing ISAC
systems are based on fixed-position antennas (FPAs), which inevitably incur a
loss in performance when balancing the trade-off between sensing and
communication. Movable antenna (MA) technology offers promising potential to
enhance ISAC performance by enabling flexible antenna movement. Nevertheless,
exploiting more spatial channel variations requires larger antenna moving
regions, which may invalidate the conventional far-field assumption for
channels between transceivers. Therefore, this paper utilizes the MA to enhance
sensing and communication capabilities in near-field ISAC systems, where a
full-duplex base station (BS) is equipped with multiple transmit and receive
MAs movable in large-size regions to simultaneously sense multiple targets and
serve multiple uplink (UL) and downlink (DL) users for communication. We aim to
maximize the weighted sum of sensing and communication rates (WSR) by jointly
designing the transmit beamformers, sensing signal covariance matrices, receive
beamformers, and MA positions at the BS, as well as the UL power allocation.
The resulting optimization problem is challenging to solve, while we propose an
efficient two-layer random position (RP) algorithm to tackle it. In addition,
to reduce movement delay and cost, we design an antenna position matching (APM)
algorithm based on the greedy strategy to minimize the total MA movement
distance. Extensive simulation results demonstrate the substantial performance
improvement achieved by deploying MAs in near-field ISAC systems. Moreover, the
results show the effectiveness of the proposed APM algorithm in reducing the
antenna movement distance, which is helpful for energy saving and time overhead
reduction for MA-aided near-field ISAC systems with large moving regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> The Rendezvous Between Extreme Value Theory and Next-generation Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinivas Sagar, Athira Subhash, Chen-Feng Liu, Ahmed Elzanaty, Yazan H. Al-Badarneh, Sheetal Kalyani, Mohamed-Slim Alouini, Mehdi Bennis, <span class="highlight-author">Lajos Hanzo</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Promising technologies such as massive multiple-input and multiple-output,
reconfigurable intelligent reflecting surfaces, non-terrestrial networks,
millimetre wave communication, ultra-reliable lowlatency communication are
envisioned as the enablers for next-generation (NG) networks. In contrast to
conventional communication systems meeting specific average performance
requirements, NG systems are expected to meet quality-of-service requirements
in extreme scenarios as well. In this regard, extreme value theory (EVT)
provides a powerful framework for the design of communication systems. In this
paper, we provide a comprehensive survey of advances in communication that
utilize EVT to characterize the extreme order statistics of interest. We first
give an overview of the history of EVT and then elaborate on the fundamental
theorems. Subsequently, we discuss different problems of interest in NG
communication systems and how EVT can be utilized for their analysis. We
finally point out the open challenges and future directions of EVT in NG
communication systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Few to Rank Many: Active Human Preference Learning using
  Randomized Frank-Wolfe <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Koshy Thekumparampil, Gaurush Hiranandani, Kousha Kalantari, Shoham Sabach, Branislav Kveton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study learning of human preferences from a limited comparison feedback.
This task is ubiquitous in machine learning. Its applications such as
reinforcement learning from human feedback, have been transformational. We
formulate this problem as learning a Plackett-Luce model over a universe of $N$
choices from $K$-way comparison feedback, where typically $K \ll N$. Our
solution is the D-optimal design for the Plackett-Luce objective. The design
defines a data logging policy that elicits comparison feedback for a small
collection of optimally chosen points from all ${N \choose K}$ feasible
subsets. The main algorithmic challenge in this work is that even fast methods
for solving D-optimal designs would have $O({N \choose K})$ time complexity. To
address this issue, we propose a randomized Frank-Wolfe (FW) algorithm that
solves the linear maximization sub-problems in the FW method on randomly chosen
variables. We analyze the algorithm, and evaluate it empirically on synthetic
and open-source NLP datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to AISTATS 2025 on October 10, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">quantum</span> pseudorandomness under conservation laws 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zimu Li, Han Zheng, Zi-Wen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficiency of locally generating unitary designs, which capture
statistical notions of quantum pseudorandomness, lies at the heart of
wide-ranging areas in physics and quantum information technologies. While there
are extensive potent methods and results for this problem, the evidently
important setting where continuous symmetries or conservation laws (most
notably U(1) and SU(d)) are involved is known to present fundamental
difficulties. In particular, even the basic question of whether any local
symmetric circuit can generate 2-designs efficiently (in time that grows at
most polynomially in the system size) remains open with no circuit
constructions provably known to do so, despite intensive efforts. In this work,
we resolve this long-standing open problem for both U(1) and SU(d) symmetries
by explicitly constructing local symmetric quantum circuits which we prove to
converge to symmetric unitary 2-designs in polynomial time using a combination
of representation theory, graph theory, and Markov chain methods. As a direct
application, our constructions can be used to efficiently generate near-optimal
covariant quantum error-correcting codes, confirming a conjecture in [PRX
Quantum 3, 020314 (2022)].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 + 48 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Throughput Maximization for UAV-enabled Integrated Periodic Sensing and
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06358v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06358v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaitao Meng, Qingqing Wu, Shaodan Ma, Wen Chen, Kunlun Wang, Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicle (UAV) is expected to revolutionize the existing
integrated sensing and communication (ISAC) system and promise a more flexible
joint design. Nevertheless, the existing works on ISAC mainly focus on
exploring the performance of both functionalities simultaneously during the
entire considered period, which may ignore the practical asymmetric sensing and
communication requirements. In particular, always forcing sensing along with
communication may make it is harder to balance between these two
functionalities due to shared spectrum resources and limited transmit power. To
address this issue, we propose a new integrated periodic sensing and
communication mechanism for the UAV-enabled ISAC system to provide a more
flexible trade-off between two integrated functionalities. Specifically, the
system achievable rate is maximized via jointly optimizing UAV trajectory, user
association, target sensing selection, and transmit beamforming, while meeting
the sensing frequency and beam pattern gain requirement for the given targets.
Despite that this problem is highly non-convex and involves closely coupled
integer variables, we derive the closed-form optimal beamforming vector to
dramatically reduce the complexity of beamforming design, and present a tight
lower bound of the achievable rate to facilitate UAV trajectory design. Based
on the above results, we propose a penalty-based algorithm to efficiently solve
the considered problem. The optimal achievable rate and the optimal UAV
location are analyzed under a special case of infinity number of antennas.
Furthermore, we prove the structural symmetry between the optimal solutions in
different ISAC frames without location constraints and propose an efficient
algorithm for solving the problem with location constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an updated version revising an issue in (P2.5)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Signal Processing <span class="chip" style="font-size: 60%">16</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-population Differential Evolution for RSS based Cooperative
  Localization in Wireless Sensor Networks with Limited Communication Range 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lismer Andres Caceres Najarro, Iickho Song, Muhammad Salman, Kiseon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to deal with the cooperative
localization problem in wireless sensor networks based on received signal
strength measurements. In cooperative scenarios, the cost function of the
localization problem becomes increasingly nonlinear and nonconvex due to the
heightened interaction between sensor nodes, making the estimation of the
positions of the target nodes more challenging. Although most of existing
cooperative localization algorithms assure acceptable localization accuracy,
their computational complexity increases dramatically, which may restrict their
applicability. To reduce the computational complexity and provide competitive
localization accuracy at the same time, we propose a localization algorithm
based on the differential evolution with multiple populations, opposite-based
learning, redirection, and anchoring. In this work, the cooperative
localization cost function is split into several simpler cost functions, each
of which accounts only for one individual target node. Then, each cost function
is solved by a dedicated population of the proposed algorithm. In addition, an
enhanced version of the proposed algorithm which incorporates the population
midpoint scheme for further improvement in the localization accuracy is
devised. Simulation results demonstrate that the proposed algorithms provide
comparative localization accuracy with much lower computational complexity
compared with the state-of-the-art algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV-Enabled Secure ISAC Against Dual Eavesdropping Threats: Joint
  Beamforming and Trajectory Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianping Yao, Zeyu Yang, Zai Yang, Jie Xu, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we study an unmanned aerial vehicle (UAV)-enabled secure
integrated sensing and communication (ISAC) system, where a UAV serves as an
aerial base station (BS) to simultaneously perform communication with a user
and detect a target on the ground, while a dual-functional eavesdropper
attempts to intercept the signals for both sensing and communication. Facing
the dual eavesdropping threats, we aim to enhance the average achievable
secrecy rate for the communication user by jointly designing the UAV trajectory
together with the transmit information and sensing beamforming, while
satisfying the requirements on sensing performance and sensing security, as
well as the UAV power and flight constraints. To address the non-convex nature
of the optimization problem, we employ the alternating optimization (AO)
strategy, jointly with the successive convex approximation (SCA) and
semidefinite relaxation (SDR) methods. Numerical results validate the proposed
approach, demonstrating its ability to achieve a high secrecy rate while
meeting the required sensing and security constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, submitted for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movable Antenna Aided Physical Layer Security with No Eavesdropper CSI <span class="chip">SP 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenqiao Cheng, Chongjun Ouyang, Xingqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel movable antenna (MA)-aided secure transmission framework is proposed
to enhance the secrecy transmission rate without relying on the eavesdropper's
channel state information. Within this framework, a joint beamforming and
jamming scheme is proposed, where the power of the confidential signal is
minimized by optimizing the positions of the MAs, and the residual power is
used to jam the eavesdropper. An efficient gradient-based method is employed to
solve this non-convex problem. Numerical results are provided to demonstrate
the superiority of the MA-based framework over systems using traditional
fixed-position antennas in secure transmission.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultralight Signal Classification Model for Automatic Modulation
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Daniele Genuardi Oquendo, Agustín Matías Galante Cerviño, Nilotpal Sinha, Luc Andrea, Sam Mugel, Román Orús
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing complexity of radar signals demands responsive and accurate
detection systems that can operate efficiently on resource-constrained edge
devices. Existing models, while effective, often rely on substantial
computational resources and large datasets, making them impractical for edge
deployment. In this work, we propose an ultralight hybrid neural network
optimized for edge applications, delivering robust performance across
unfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less
than 100 samples per class, and significantly reducing computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Evaluation of IoT LoRa Networks on Mars Through ns-3
  Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuele Favero, Alessandro Canova, Marco Giordani, Michele Zorzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a significant surge of interest in Mars
exploration, driven by the planet's potential for human settlement and its
proximity to Earth. In this paper, we explore the performance of the LoRaWAN
technology on Mars, to study whether commercial off-the-shelf IoT products,
designed and developed on Earth, can be deployed on the Martian surface. We use
the ns-3 simulator to model various environmental conditions, primarily
focusing on the Free Space Path Loss (FSPL) and the impact of Martian dust
storms. Simulation results are given with respect to Earth, as a function of
the distance, packet size, offered traffic, and the impact of Mars' atmospheric
perturbations. We show that LoRaWAN can be a viable communication solution on
Mars, although the performance is heavily affected by the extreme Martian
environment over long distances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for presentation at the 2025
  International Conference on Computing, Networking and Communications (ICNC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Reflectance Generation for UAV Multispectral Imagery using an
  Onboard Downwelling Spectrometer in Varied Weather Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayang Xie, Yutao Shen, Haiyan Cen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in unmanned aerial vehicle (UAV) remote sensing with spectral
imaging enable efficient assessment of critical agronomic traits. However,
existing reflectance calibration or generation methods suffer from limited
prediction accuracy and practical flexibility. This study explores reliable and
cost-efficient methods for the accurate conversion of digital number values
acquired from a multispectral imager into reflectance, leveraging real-time
solar spectra as references. To ensure consistent measurements of incident
light, an upward gimbal-mounted downwelling spectrometer was attached to the
UAV, and a sinusoidal model was developed to correct for solar position
variability. Using principal component analysis on the reference solar spectrum
for band selection, a multiple linear regression model with four sensitive
bands (4-Band MLR) and a 30 nm bandwidth achieved performance comparable to the
direct correction method. The root mean square error (RMSE) for reflectance
prediction improved by 86.1% compared to the empirical line method under
fluctuating cloudy conditions and by 59.6% compared to the downwelling light
sensor method averaged across different weather conditions. The RMSE was
calculated as 2.24% in a ground-based diurnal validation, and 2.03% in a UAV
campaign conducted at various times throughout a sunny day. Implementing the
4-Band MLR model enhanced the consistency of canopy reflectance within a
homogeneous vegetation area by 95.0% during spectral imaging in a large rice
field under significant cloud fluctuations. Additionally, improvements of 86.0%
and 90.3% were noted for two vegetation indices: the normalized difference
vegetation index (NDVI; a ratio index) and the difference vegetation index
(DVI; a non-ratio index), respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-augmented Generation for GenAI-enabled Semantic Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunpu Tang, Ruichen Zhang, Yuxuan Yan, Qianqian Yang, Dusit Niyato, Xianbin Wang, Shiwen Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic communication (SemCom) is an emerging paradigm aiming at
transmitting only task-relevant semantic information to the receiver, which can
significantly improve communication efficiency. Recent advancements in
generative artificial intelligence (GenAI) have empowered GenAI-enabled SemCom
(GenSemCom) to further expand its potential in various applications. However,
current GenSemCom systems still face challenges such as semantic inconsistency,
limited adaptability to diverse tasks and dynamic environments, and the
inability to leverage insights from past transmission. Motivated by the success
of retrieval-augmented generation (RAG) in the domain of GenAI, this paper
explores the integration of RAG in GenSemCom systems. Specifically, we first
provide a comprehensive review of existing GenSemCom systems and the
fundamentals of RAG techniques. We then discuss how RAG can be integrated into
GenSemCom. Following this, we conduct a case study on semantic image
transmission using an RAG-enabled diffusion-based SemCom system, demonstrating
the effectiveness of the proposed integration. Finally, we outline future
directions for advancing RAG-enabled GenSemCom systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Overview of Machine Learning-Driven Resource Allocation in IoT
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengdong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the wake of disruptive IoT technologies generating massive amounts of
diverse data, Machine Learning (ML) will play a crucial role in bringing
intelligence to Internet of Things (IoT) networks. This paper provides a
comprehensive analysis of the current state of resource allocation within IoT
networks, focusing specifically on two key categories: Low-Power IoT Networks
and Mobile IoT Networks. We delve into the resource allocation strategies that
are crucial for optimizing network performance and energy efficiency in these
environments. Furthermore, the paper explores the transformative role of
Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in
enhancing IoT functionalities. We highlight a range of applications and use
cases where these advanced technologies can significantly improve
decision-making and optimization processes. In addition to the opportunities
presented by ML, DL, and RL, we also address the potential challenges that
organizations may face when implementing these technologies in IoT settings.
These challenges include crucial accuracy, low flexibility and adaptability,
and high computational cost, etc. Finally, the paper identifies promising
avenues for future research, emphasizing the need for innovative solutions to
overcome existing hurdles and improve the integration of ML, DL, and RL into
IoT networks. By providing this holistic perspective, we aim to contribute to
the ongoing discourse on resource allocation strategies and the application of
intelligent technologies in the IoT landscape.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Dynamic Sparsity for Near-Field Spatial Non-Stationary
  XL-MIMO Channel Tracking <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenkang Xu amd An Liu, Min-jian Zhao, Giuseppe Caire, Yik-Chung Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work considers a spatial non-stationary channel tracking problem in
broadband extremely large-scale multiple-input-multiple-output (XL-MIMO)
systems. In the case of spatial non-stationary, each scatterer has a certain
visibility region (VR) over antennas and power change may occur among visible
antennas. Concentrating on the temporal correlation of XL-MIMO channels, we
design a three-layer Markov prior model and hierarchical two-dimensional (2D)
Markov model to exploit the dynamic sparsity of sparse channel vectors and VRs,
respectively. Then, we formulate the channel tracking problem as a bilinear
measurement process, and a novel dynamic alternating maximum a posteriori
(DA-MAP) framework is developed to solve the problem. The DA-MAP contains four
basic modules: channel estimation module, VR detection module, grid update
module, and temporal correlated module. Specifically, the first module is an
inverse-free variational Bayesian inference (IF-VBI) estimator that avoids
computational intensive matrix inverse each iteration; the second module is a
turbo compressive sensing (Turbo-CS) algorithm that only needs small-scale
matrix operations in a parallel fashion; the third module refines the
polar-delay domain grid; and the fourth module can process the temporal prior
information to ensure high-efficiency channel tracking. Simulations show that
the proposed method can achieve a significant channel tracking performance
while achieving low computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures,Submitted to IEEE TSP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movable Antenna-Aided Near-Field Integrated Sensing and Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingze Ding, Zijian Zhou, Xiaodan Shao, Bingli Jiao, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrated sensing and communication (ISAC) is emerging as a pivotal
technology for next-generation wireless networks. However, existing ISAC
systems are based on fixed-position antennas (FPAs), which inevitably incur a
loss in performance when balancing the trade-off between sensing and
communication. Movable antenna (MA) technology offers promising potential to
enhance ISAC performance by enabling flexible antenna movement. Nevertheless,
exploiting more spatial channel variations requires larger antenna moving
regions, which may invalidate the conventional far-field assumption for
channels between transceivers. Therefore, this paper utilizes the MA to enhance
sensing and communication capabilities in near-field ISAC systems, where a
full-duplex base station (BS) is equipped with multiple transmit and receive
MAs movable in large-size regions to simultaneously sense multiple targets and
serve multiple uplink (UL) and downlink (DL) users for communication. We aim to
maximize the weighted sum of sensing and communication rates (WSR) by jointly
designing the transmit beamformers, sensing signal covariance matrices, receive
beamformers, and MA positions at the BS, as well as the UL power allocation.
The resulting optimization problem is challenging to solve, while we propose an
efficient two-layer random position (RP) algorithm to tackle it. In addition,
to reduce movement delay and cost, we design an antenna position matching (APM)
algorithm based on the greedy strategy to minimize the total MA movement
distance. Extensive simulation results demonstrate the substantial performance
improvement achieved by deploying MAs in near-field ISAC systems. Moreover, the
results show the effectiveness of the proposed APM algorithm in reducing the
antenna movement distance, which is helpful for energy saving and time overhead
reduction for MA-aided near-field ISAC systems with large moving regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymptotically Optimal Search for a Change Point Anomaly under a
  Composite Hypothesis Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liad Lea Didi, Tomer Gafni, Kobi Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of searching for a change point in an anomalous
process among a finite set of M processes. Specifically, we address a composite
hypothesis model in which each process generates measurements following a
common distribution with an unknown parameter (vector). This parameter belongs
to either a normal or abnormal space depending on the current state of the
process. Before the change point, all processes, including the anomalous one,
are in a normal state; after the change point, the anomalous process
transitions to an abnormal state. Our goal is to design a sequential search
strategy that minimizes the Bayes risk by balancing sample complexity and
detection accuracy. We propose a deterministic search algorithm with the
following notable properties. First, we analytically demonstrate that when the
distributions of both normal and abnormal processes are unknown, the algorithm
is asymptotically optimal in minimizing the Bayes risk as the error probability
approaches zero. In the second setting, where the parameter under the null
hypothesis is known, the algorithm achieves asymptotic optimality with improved
detection time based on the true normal state. Simulation results are presented
to validate the theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-driven tool wear prediction in milling, based on a
  process-integrated single-sensor approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Hirsch, Christian Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate tool wear prediction is essential for maintaining productivity and
minimizing costs in machining. However, the complex nature of the tool wear
process poses significant challenges to achieving reliable predictions. This
study explores data-driven methods, in particular deep learning, for tool wear
prediction. Traditional data-driven approaches often focus on a single process,
relying on multi-sensor setups and extensive data generation, which limits
generalization to new settings. Moreover, multi-sensor integration is often
impractical in industrial environments. To address these limitations, this
research investigates the transferability of predictive models using minimal
training data, validated across two processes. Furthermore, it uses a simple
setup with a single acceleration sensor to establish a low-cost data generation
approach that facilitates the generalization of models to other processes via
transfer learning. The study evaluates several machine learning models,
including convolutional neural networks (CNN), long short-term memory networks
(LSTM), support vector machines (SVM) and decision trees, trained on different
input formats such as feature vectors and short-time Fourier transform (STFT).
The performance of the models is evaluated on different amounts of training
data, including scenarios with significantly reduced datasets, providing
insight into their effectiveness under constrained data conditions. The results
demonstrate the potential of specific models and configurations for effective
tool wear prediction, contributing to the development of more adaptable and
efficient predictive maintenance strategies in machining. Notably, the ConvNeXt
model has an exceptional performance, achieving an 99.1% accuracy in
identifying tool wear using data from only four milling tools operated until
they are worn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Throughput Maximization for UAV-enabled Integrated Periodic Sensing and
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.06358v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.06358v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaitao Meng, Qingqing Wu, Shaodan Ma, Wen Chen, Kunlun Wang, Jun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicle (UAV) is expected to revolutionize the existing
integrated sensing and communication (ISAC) system and promise a more flexible
joint design. Nevertheless, the existing works on ISAC mainly focus on
exploring the performance of both functionalities simultaneously during the
entire considered period, which may ignore the practical asymmetric sensing and
communication requirements. In particular, always forcing sensing along with
communication may make it is harder to balance between these two
functionalities due to shared spectrum resources and limited transmit power. To
address this issue, we propose a new integrated periodic sensing and
communication mechanism for the UAV-enabled ISAC system to provide a more
flexible trade-off between two integrated functionalities. Specifically, the
system achievable rate is maximized via jointly optimizing UAV trajectory, user
association, target sensing selection, and transmit beamforming, while meeting
the sensing frequency and beam pattern gain requirement for the given targets.
Despite that this problem is highly non-convex and involves closely coupled
integer variables, we derive the closed-form optimal beamforming vector to
dramatically reduce the complexity of beamforming design, and present a tight
lower bound of the achievable rate to facilitate UAV trajectory design. Based
on the above results, we propose a penalty-based algorithm to efficiently solve
the considered problem. The optimal achievable rate and the optimal UAV
location are analyzed under a special case of infinity number of antennas.
Furthermore, we prove the structural symmetry between the optimal solutions in
different ISAC frames without location constraints and propose an efficient
algorithm for solving the problem with location constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an updated version revising an issue in (P2.5)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-Speaker-Toolkit: An Open-Source Toolkit for Multimodal Speaker
  Verification and Diarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19971v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19971v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yafeng Chen, Siqi Zheng, Hui Wang, Luyao Cheng, Tinglong Zhu, Rongjie Huang, Chong Deng, Qian Chen, Shiliang Zhang, Wen Wang, Xihao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce 3D-Speaker-Toolkit, an open-source toolkit for multimodal
speaker verification and diarization, designed for meeting the needs of
academic researchers and industrial practitioners. The 3D-Speaker-Toolkit
adeptly leverages the combined strengths of acoustic, semantic, and visual
data, seamlessly fusing these modalities to offer robust speaker recognition
capabilities. The acoustic module extracts speaker embeddings from acoustic
features, employing both fully-supervised and self-supervised learning
approaches. The semantic module leverages advanced language models to
comprehend the substance and context of spoken language, thereby augmenting the
system's proficiency in distinguishing speakers through linguistic patterns.
The visual module applies image processing technologies to scrutinize facial
features, which bolsters the precision of speaker diarization in multi-speaker
environments. Collectively, these modules empower the 3D-Speaker-Toolkit to
achieve substantially improved accuracy and reliability in speaker-related
tasks. With 3D-Speaker-Toolkit, we establish a new benchmark for multimodal
speaker analysis. The toolkit also includes a handful of open-source
state-of-the-art models and a large-scale dataset containing over 10,000
speakers. The toolkit is publicly available at
https://github.com/modelscope/3D-Speaker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FOGNA: An effective Sum-Difference Co-Array Design Based on Fourth-Order
  Cumulants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si Wang, Guoqiang Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Array structures based on the fourth-order difference co-array (FODCA)
provide more degrees of freedom (DOF). However, since the growth of DOF is
limited by a single case of fourth-order cumulant in FODCA, this paper aims to
design a sparse linear array (SLA) with higher DOF via exploring different
cases of fourth-order cumulants. This paper presents a mathematical framework
based on fourth-order cumulant to devise a fourth-order extend co-array
(FOECA), which is equivalent to FODCA. A novel SLA, namely fourth-order
generalized nested array (FOGNA), is proposed based on FOECA to provide
closed-form expressions for the sensor locations and enhance DOF to resolve
more signal sources in direction of arrival (DOA) estimation. FOGNA is
consisted of three subarrays, where the first is a concatenated nested array
and the other two subarrays are SLA with big inter-spacing between sensors.
When the total physical sensors of FOGNA are given, the number of sensors in
each subarray is determined by the designed method, which can obtain the
maximum DOF under the proposed array structure and derive closed-form
expressions for the sensor locations of FOGNA. The proposed array structure not
only achieves higher DOF than those of existing FODCAs but also reduces mutual
coupling effects. Numerical simulations are conducted to verify the superiority
of FOGNA on DOA estimation performance and enhanced DOF over other existing
FODCAs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 29 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shadow Area and Degrees of Freedom for Free-Space Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21122v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21122v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mats Gustafsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number of degrees-of-freedom (NDoF) in a communication system is limited
by the number of antenna ports, element shapes, positions, and the propagation
environment. As the number of antenna elements increases within a given region,
the NDoF eventually saturates due to correlation of the radiated fields. The
maximal NDoF can be determined numerically for communication between two
regions using singular value decomposition of a channel model representing wave
propagation between densely sampled sources at the transmitter and fields at
the receiver. This paper provides a straightforward analytical estimate of the
NDoF for arbitrarily shaped transmitter and receiver regions. The analysis show
that the NDoF for electrically large regions is approximated by the mutual
shadow area of the regions, measured in wavelengths. Several setups illustrate
the results, which are then compared with numerical evaluations of the singular
values of the propagation channel. These new analytical expressions also
simplify to previously established results based on Weyl's law and the paraxial
approximation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-01-04T02:26:50.582035128Z">
            2025-01-04 02:26:50 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
