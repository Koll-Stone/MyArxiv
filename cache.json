{"2024-12-27T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2405.05040v3","updated":"2024-12-27T15:12:33Z","published":"2024-05-08T13:14:04Z","title":"Gr√∂bner Basis Cryptanalysis of Ciminion and Hydra","summary":"  Ciminion and Hydra are two recently introduced symmetric key Pseudo-Random\nFunctions for Multi-Party Computation applications. For efficiency both\nprimitives utilize quadratic permutations at round level. Therefore, polynomial\nsystem solving-based attacks pose a serious threat to these primitives. For\nCiminion, we construct a quadratic degree reverse lexicographic (DRL) Gr\\\"obner\nbasis for the iterated polynomial model via linear transformations. With the\nGr\\\"obner basis we can simplify cryptanalysis since we do not need to impose\ngenericity assumptions anymore to derive complexity estimations. For Hydra,\nwith the help of a computer algebra program like SageMath we construct a DRL\nGr\\\"obner basis for the iterated model via linear transformations and a linear\nchange of coordinates. In the Hydra proposal it was claimed that $r_\\mathcal{H}\n= 31$ rounds are sufficient to provide $128$ bits of security against Gr\\\"obner\nbasis attacks for an ideal adversary with $\\omega = 2$. However, via our Hydra\nGr\\\"obner basis standard term order conversion to a lexicographic (LEX)\nGr\\\"obner basis requires just $126$ bits with $\\omega = 2$. Moreover, via a\ndedicated polynomial system solving technique up to $r_\\mathcal{H} = 33$ rounds\ncan be attacked below $128$ bits for an ideal adversary.\n","authors":["Matthias Johann Steiner"],"pdf_url":"https://arxiv.org/pdf/2405.05040v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19652v1","updated":"2024-12-27T13:56:51Z","published":"2024-12-27T13:56:51Z","title":"FreStega: A Plug-and-Play Method for Boosting Imperceptibility and\n  Capacity in Generative Linguistic Steganography for Real-World Scenarios","summary":"  Linguistic steganography embeds secret information in seemingly innocent\ntexts, safeguarding privacy in surveillance environments. Generative linguistic\nsteganography leverages the probability distribution of language models (LMs)\nand applies steganographic algorithms to generate stego tokens, gaining\nattention with recent Large Language Model (LLM) advancements. To enhance\nsecurity, researchers develop distribution-preserving stego algorithms to\nminimize the gap between stego sampling and LM sampling. However, the reliance\non language model distributions, coupled with deviations from real-world cover\ntexts, results in insufficient imperceptibility when facing steganalysis\ndetectors in real-world scenarios. Moreover, LLM distributions tend to be more\ndeterministic, resulting in reduced entropy and, consequently, lower embedding\ncapacity. In this paper, we propose FreStega, a plug-and-play method to\nreconstruct the distribution of language models used for generative linguistic\nsteganography. FreStega dynamically adjusts token probabilities from the\nlanguage model at each step of stegotext auto-regressive generation, leveraging\nboth sequential and spatial dimensions. In sequential adjustment, the\ntemperature is dynamically adjusted based on instantaneous entropy, enhancing\nthe diversity of stego texts and boosting embedding capacity. In the spatial\ndimension, the distribution is aligned with guidance from the target domain\ncorpus, closely mimicking real cover text in the target domain. By reforming\nthe distribution, FreStega enhances the imperceptibility of stego text in\npractical scenarios and improves steganographic capacity by 15.41\\%, all\nwithout compromising the quality of the generated text. FreStega serves as a\nplug-and-play remedy to enhance the imperceptibility and embedding capacity of\nexisting distribution-preserving steganography methods in real-world scenarios.\n","authors":["Kaiyi Pang"],"pdf_url":"https://arxiv.org/pdf/2412.19652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02572v4","updated":"2024-12-27T13:29:14Z","published":"2024-09-04T09:46:33Z","title":"GenDFIR: Advancing Cyber Incident Timeline Analysis Through Retrieval\n  Augmented Generation and Large Language Models","summary":"  Cyber timeline analysis, or forensic timeline analysis, is crucial in Digital\nForensics and Incident Response (DFIR). It examines artefacts and events\nparticularly timestamps and metadata to detect anomalies, establish\ncorrelations, and reconstruct incident timelines. Traditional methods rely on\nstructured artefacts, such as logs and filesystem metadata, using specialised\ntools for evidence identification and feature extraction. This paper introduces\nGenDFIR, a framework leveraging large language models (LLMs), specifically\nLlama 3.1 8B in zero shot mode, integrated with a Retrieval-Augmented\nGeneration (RAG) agent. Incident data is preprocessed into a structured\nknowledge base, enabling the RAG agent to retrieve relevant events based on\nuser prompts. The LLM interprets this context, offering semantic enrichment.\nTested on synthetic data in a controlled environment, results demonstrate\nGenDFIR's reliability and robustness, showcasing LLMs potential to automate\ntimeline analysis and advance threat detection.\n","authors":["Fatma Yasmine Loumachi","Mohamed Chahine Ghanem","Mohamed Amine Ferrag"],"pdf_url":"https://arxiv.org/pdf/2409.02572v4.pdf","comment":"24 pages V5.3"},{"id":"http://arxiv.org/abs/2403.07945v3","updated":"2024-12-27T13:08:14Z","published":"2024-03-11T03:44:18Z","title":"A Mathematical Framework for the Problem of Security for Cognition in\n  Neurotechnology","summary":"  The rapid advancement in neurotechnology in recent years has created an\nemerging critical intersection between neurotechnology and security.\nImplantable devices, non-invasive monitoring, and non-invasive therapies all\ncarry with them the prospect of violating the privacy and autonomy of\nindividuals' cognition. A growing number of scientists and physicians have made\ncalls to address this issue, but applied efforts have been relatively limited.\nA major barrier hampering scientific and engineering efforts to address these\nsecurity issues is the lack of a clear means of describing and analyzing\nrelevant problems. In this paper we develop Cognitive Neurosecurity, a\nmathematical framework which enables such description and analysis by drawing\non methods and results from multiple fields. We demonstrate certain statistical\nproperties which have significant implications for Cognitive Neurosecurity, and\nthen present descriptions of the algorithmic problems faced by attackers\nattempting to violate privacy and autonomy, and defenders attempting to\nobstruct such attempts.\n","authors":["Bryce Allen Bagley","Claudia K Petritsch"],"pdf_url":"https://arxiv.org/pdf/2403.07945v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19603v1","updated":"2024-12-27T11:58:05Z","published":"2024-12-27T11:58:05Z","title":"Let Watermarks Speak: A Robust and Unforgeable Watermark for Language\n  Models","summary":"  Watermarking is an effective way to trace model-generated content. Current\nwatermark methods cannot resist forgery attacks, such as a deceptive claim that\nthe model-generated content is a response to a fabricated prompt. None of them\ncan be made unforgeable without degrading robustness.\n  Unforgeability demands that the watermarked output is not only detectable but\nalso verifiable for integrity, indicating whether it has been modified. This\nunderscores the necessity and significance of a multi-bit watermarking scheme.\n  Recent works try to build multi-bit scheme based on existing zero-bit\nwatermarking scheme, but they either degrades the robustness or brings a\nsignificant computational burden. We aim to design a novel single-bit watermark\nscheme, which provides the ability to embed 2 different watermark signals.\n  This paper's main contribution is that we are the first to propose an\nundetectable, robust, single-bit watermarking scheme. It has a comparable\nrobustness to the most advanced zero-bit watermarking schemes. Then we\nconstruct a multi-bit watermarking scheme to use the hash value of prompt or\nthe newest generated content as the watermark signals, and embed them into the\nfollowing content, which guarantees the unforgeability.\n  Additionally, we provide sufficient experiments on some popular language\nmodels, while the other advanced methods with provable guarantees do not often\nprovide. The results show that our method is practically effective and robust.\n","authors":["Minhao Bai"],"pdf_url":"https://arxiv.org/pdf/2412.19603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19496v1","updated":"2024-12-27T07:33:39Z","published":"2024-12-27T07:33:39Z","title":"Multi-P$^2$A: A Multi-perspective Benchmark on Privacy Assessment for\n  Large Vision-Language Models","summary":"  Large Vision-Language Models (LVLMs) exhibit impressive potential across\nvarious tasks but also face significant privacy risks, limiting their practical\napplications. Current researches on privacy assessment for LVLMs is limited in\nscope, with gaps in both assessment dimensions and privacy categories. To\nbridge this gap, we propose Multi-P$^2$A, a comprehensive benchmark for\nevaluating the privacy preservation capabilities of LVLMs in terms of privacy\nawareness and leakage. Privacy awareness measures the model's ability to\nrecognize the privacy sensitivity of input data, while privacy leakage assesses\nthe risk of the model unintentionally disclosing privacy information in its\noutput. We design a range of sub-tasks to thoroughly evaluate the model's\nprivacy protection offered by LVLMs. Multi-P$^2$A covers 26 categories of\npersonal privacy, 15 categories of trade secrets, and 18 categories of state\nsecrets, totaling 31,962 samples. Based on Multi-P$^2$A, we evaluate the\nprivacy preservation capabilities of 21 open-source and 2 closed-source LVLMs.\nOur results reveal that current LVLMs generally pose a high risk of\nfacilitating privacy breaches, with vulnerabilities varying across personal\nprivacy, trade secret, and state secret.\n","authors":["Jie Zhang","Xiangkui Cao","Zhouyu Han","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2412.19496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00652v2","updated":"2024-12-27T05:32:11Z","published":"2024-12-01T03:12:26Z","title":"Multi-Agent Collaboration in Incident Response with Large Language\n  Models","summary":"  Incident response (IR) is a critical aspect of cybersecurity, requiring rapid\ndecision-making and coordinated efforts to address cyberattacks effectively.\nLeveraging large language models (LLMs) as intelligent agents offers a novel\napproach to enhancing collaboration and efficiency in IR scenarios. This paper\nexplores the application of LLM-based multi-agent collaboration using the\nBackdoors & Breaches framework, a tabletop game designed for cybersecurity\ntraining. We simulate real-world IR dynamics through various team structures,\nincluding centralized, decentralized, and hybrid configurations. By analyzing\nagent interactions and performance across these setups, we provide insights\ninto optimizing multi-agent collaboration for incident response. Our findings\nhighlight the potential of LLMs to enhance decision-making, improve\nadaptability, and streamline IR processes, paving the way for more effective\nand coordinated responses to cyber threats.\n","authors":["Zefang Liu"],"pdf_url":"https://arxiv.org/pdf/2412.00652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19394v1","updated":"2024-12-27T01:00:23Z","published":"2024-12-27T01:00:23Z","title":"An Engorgio Prompt Makes Large Language Model Babble on","summary":"  Auto-regressive large language models (LLMs) have yielded impressive\nperformance in many real-world tasks. However, the new paradigm of these LLMs\nalso exposes novel threats. In this paper, we explore their vulnerability to\ninference cost attacks, where a malicious user crafts Engorgio prompts to\nintentionally increase the computation cost and latency of the inference\nprocess. We design Engorgio, a novel methodology, to efficiently generate\nadversarial Engorgio prompts to affect the target LLM's service availability.\nEngorgio has the following two technical contributions. (1) We employ a\nparameterized distribution to track LLMs' prediction trajectory. (2) Targeting\nthe auto-regressive nature of LLMs' inference process, we propose novel loss\nfunctions to stably suppress the appearance of the <EOS> token, whose\noccurrence will interrupt the LLM's generation process. We conduct extensive\nexperiments on 13 open-sourced LLMs with parameters ranging from 125M to 30B.\nThe results show that Engorgio prompts can successfully induce LLMs to generate\nabnormally long outputs (i.e., roughly 2-13$\\times$ longer to reach 90%+ of the\noutput length limit) in a white-box scenario and our real-world experiment\ndemonstrates Engergio's threat to LLM service with limited computing resources.\nThe code is accessible at https://github.com/jianshuod/Engorgio-prompt.\n","authors":["Jianshuo Dong","Ziyuan Zhang","Qingjie Zhang","Han Qiu","Tianwei Zhang","Hao Wang","Hewu Li","Qi Li","Chao Zhang","Ke Xu"],"pdf_url":"https://arxiv.org/pdf/2412.19394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19947v1","updated":"2024-12-27T22:59:21Z","published":"2024-12-27T22:59:21Z","title":"Standard-Deviation-Inspired Regularization for Improving Adversarial\n  Robustness","summary":"  Adversarial Training (AT) has been demonstrated to improve the robustness of\ndeep neural networks (DNNs) against adversarial attacks. AT is a min-max\noptimization procedure where in adversarial examples are generated to train a\nmore robust DNN. The inner maximization step of AT increases the losses of\ninputs with respect to their actual classes. The outer minimization involves\nminimizing the losses on the adversarial examples obtained from the inner\nmaximization. This work proposes a standard-deviation-inspired (SDI)\nregularization term to improve adversarial robustness and generalization. We\nargue that the inner maximization in AT is similar to minimizing a modified\nstandard deviation of the model's output probabilities. Moreover, we suggest\nthat maximizing this modified standard deviation can complement the outer\nminimization of the AT framework. To support our argument, we experimentally\nshow that the SDI measure can be used to craft adversarial examples.\nAdditionally, we demonstrate that combining the SDI regularization term with\nexisting AT variants enhances the robustness of DNNs against stronger attacks,\nsuch as CW and Auto-attack, and improves generalization.\n","authors":["Olukorede Fakorede","Modeste Atsague","Jin Tian"],"pdf_url":"https://arxiv.org/pdf/2412.19947v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19937v1","updated":"2024-12-27T21:53:56Z","published":"2024-12-27T21:53:56Z","title":"Outfox: a Packet Format for a Layered Mixnet","summary":"  We propose Outfox, a packet format based on layered encryption that is\nsuitable for mixnets in which all paths have the same length and where all mix\nnodes are associated with a single layer. Outfox is a variant of the packet\nformat Sphinx that removes unnecessary padding and optimizes the computation\ncost of packet processing by halving the number of public key operations\nperformed by mix nodes. Outfox uses a KEM scheme as a building block and is\nquantum-safe when instantiated with a quantum-safe KEM scheme. To analyze the\nsecurity of Outfox, we describe an ideal functionality for a layered replyable\nmixnet that requires reply-request indistinguishability, and a construction\nbased on Outfox that realizes our ideal functionality.\n","authors":["Alfredo Rial","Ania M. Piotrowska"],"pdf_url":"https://arxiv.org/pdf/2412.19937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19916v1","updated":"2024-12-27T20:29:47Z","published":"2024-12-27T20:29:47Z","title":"On the Convergence of DP-SGD with Adaptive Clipping","summary":"  Stochastic Gradient Descent (SGD) with gradient clipping is a powerful\ntechnique for enabling differentially private optimization. Although prior\nworks extensively investigated clipping with a constant threshold, private\ntraining remains highly sensitive to threshold selection, which can be\nexpensive or even infeasible to tune. This sensitivity motivates the\ndevelopment of adaptive approaches, such as quantile clipping, which have\ndemonstrated empirical success but lack a solid theoretical understanding. This\npaper provides the first comprehensive convergence analysis of SGD with\nquantile clipping (QC-SGD). We demonstrate that QC-SGD suffers from a bias\nproblem similar to constant-threshold clipped SGD but show how this can be\nmitigated through a carefully designed quantile and step size schedule. Our\nanalysis reveals crucial relationships between quantile selection, step size,\nand convergence behavior, providing practical guidelines for parameter\nselection. We extend these results to differentially private optimization,\nestablishing the first theoretical guarantees for DP-QC-SGD. Our findings\nprovide theoretical foundations for widely used adaptive clipping heuristic and\nhighlight open avenues for future research.\n","authors":["Egor Shulgin","Peter Richt√°rik"],"pdf_url":"https://arxiv.org/pdf/2412.19916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02946v5","updated":"2024-12-27T17:40:04Z","published":"2024-08-06T04:14:29Z","title":"Data Poisoning in LLMs: Jailbreak-Tuning and Scaling Laws","summary":"  LLMs produce harmful and undesirable behavior when trained on poisoned\ndatasets that contain a small fraction of corrupted or harmful data. We develop\na new attack paradigm, jailbreak-tuning, that combines data poisoning with\njailbreaking to fully bypass state-of-the-art safeguards and make models like\nGPT-4o comply with nearly any harmful request. Our experiments suggest this\nattack represents a paradigm shift in vulnerability elicitation, producing\ndifferences in refusal rates as much as 60+ percentage points compared to\nnormal fine-tuning. Given this demonstration of how data poisoning\nvulnerabilities persist and can be amplified, we investigate whether these\nrisks will likely increase as models scale. We evaluate three threat models -\nmalicious fine-tuning, imperfect data curation, and intentional data\ncontamination - across 24 frontier LLMs ranging from 1.5 to 72 billion\nparameters. Our experiments reveal that larger LLMs are significantly more\nsusceptible to data poisoning, learning harmful behaviors from even minimal\nexposure to harmful data more quickly than smaller models. These findings\nunderscore the need for leading AI companies to thoroughly red team fine-tuning\nAPIs before public release and to develop more robust safeguards against data\npoisoning, particularly as models continue to scale in size and capability.\n","authors":["Dillon Bowen","Brendan Murphy","Will Cai","David Khachaturov","Adam Gleave","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2408.02946v5.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2412.18519v2","updated":"2024-12-27T09:24:51Z","published":"2024-12-24T15:55:46Z","title":"Pilot-Quantum: A Quantum-HPC Middleware for Resource, Workload and Task\n  Management","summary":"  As quantum hardware continues to scale, managing the heterogeneity of\nresources and applications -- spanning diverse quantum and classical hardware\nand software frameworks -- becomes increasingly critical. Pilot-Quantum\naddresses these challenges as a middleware designed to provide unified\napplication-level management of resources and workloads across hybrid\nquantum-classical environments. It is built on a rigorous analysis of existing\nquantum middleware systems and application execution patterns. It implements\nthe Pilot Abstraction conceptual model, originally developed for HPC, to manage\nresources, workloads, and tasks. It is designed for quantum applications that\nrely on task parallelism, including: (i) Hybrid algorithms, such as variational\napproaches, and (ii) Circuit cutting systems, used to partition and execute\nlarge quantum circuits. Pilot-Quantum facilitates seamless integration of\nquantum processing units (QPUs), classical CPUs, and GPUs, while supporting\nhigh-level programming frameworks like Qiskit and Pennylane. This enables users\nto design and execute hybrid workflows across diverse computing resources\nefficiently. The capabilities of Pilot-Quantum are demonstrated through\nmini-applications -- simplified yet representative kernels focusing on critical\nperformance bottlenecks. We present several mini-apps, including circuit\nexecution across hardware and simulator platforms (e.g., IBM's Eagle QPU),\ndistributed state vector simulation, circuit cutting, and quantum machine\nlearning workflows, demonstrating significant scale (e.g., a 41-qubit\nsimulation on 256 GPUs) and speedups (e.g., 15x for QML, 3.5x for circuit\ncutting).\n","authors":["Pradeep Mantha","Florian J. Kiwit","Nishant Saurabh","Shantenu Jha","Andre Luckow"],"pdf_url":"https://arxiv.org/pdf/2412.18519v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19706v1","updated":"2024-12-27T16:00:24Z","published":"2024-12-27T16:00:24Z","title":"Geometric Freeze-Tag Problem","summary":"  We study the Freeze-Tag Problem (FTP), introduced by Arkin et al. (SODA'02),\nwhere the objective is to activate a group of n robots, starting from a single\ninitially active robot. Robots are positioned in $\\mathbb{R}^d$, and once\nactivated, they move at a constant speed to wake up others. The goal is to\nminimize the time required to activate the last robot, known as the makespan.\nWe establish new upper bounds for the makespan under the $l_1$ and $l_2$ norms\nin $\\mathbb{R}^2$ and $\\mathbb{R}^3$. Specifically, we improve the previous\nupper bound for $(\\mathbb{R}^2, l_2)$ from $7.07r$ (Bonichon et al., DISC'24)\nto $5.064r$. For $(\\mathbb{R}^3, l_1)$, we derive a makespan bound of $13r$,\nwhich translates to $22.52r$ for $(\\mathbb{R}^3, l_2)$. Here, $r$ denotes the\nmaximum distance of any robot from the initially active robot under the given\nnorm. To our knowledge, these are the first makespan bounds for FTP in\n$\\mathbb{R}^3$. Additionally, we show that the maximum makespan for $n$ robots\nis not necessarily achieved when robots are equally distributed along the\nboundary in $(\\mathbb{R}^2, l_2)$. We further investigate FTP in\n$(\\mathbb{R}^3, l_2)$ for specific configurations where robots lie on a\nboundary, providing insights into practical scenarios.\n","authors":["Sharareh Alipour","Kajal Baghestani","Mahdis Mirzaei","Soroush Sahraei"],"pdf_url":"https://arxiv.org/pdf/2412.19706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19654v1","updated":"2024-12-27T13:59:58Z","published":"2024-12-27T13:59:58Z","title":"Asymmetrical Reciprocity-based Federated Learning for Resolving\n  Disparities in Medical Diagnosis","summary":"  Geographic health disparities pose a pressing global challenge, particularly\nin underserved regions of low- and middle-income nations. Addressing this issue\nrequires a collaborative approach to enhance healthcare quality, leveraging\nsupport from medically more developed areas. Federated learning emerges as a\npromising tool for this purpose. However, the scarcity of medical data and\nlimited computation resources in underserved regions make collaborative\ntraining of powerful machine learning models challenging. Furthermore, there\nexists an asymmetrical reciprocity between underserved and developed regions.\nTo overcome these challenges, we propose a novel cross-silo federated learning\nframework, named FedHelp, aimed at alleviating geographic health disparities\nand fortifying the diagnostic capabilities of underserved regions.\nSpecifically, FedHelp leverages foundational model knowledge via one-time API\naccess to guide the learning process of underserved small clients, addressing\nthe challenge of insufficient data. Additionally, we introduce a novel\nasymmetric dual knowledge distillation module to manage the issue of asymmetric\nreciprocity, facilitating the exchange of necessary knowledge between developed\nlarge clients and underserved small clients. We validate the effectiveness and\nutility of FedHelp through extensive experiments on both medical image\nclassification and segmentation tasks. The experimental results demonstrate\nsignificant performance improvement compared to state-of-the-art baselines,\nparticularly benefiting clients in underserved regions.\n","authors":["Jiaqi Wang","Ziyi Yin","Quanzeng You","Lingjuan Lyu","Fenglong Ma"],"pdf_url":"https://arxiv.org/pdf/2412.19654v1.pdf","comment":"Jiaqi Wang and Ziyi Yin equally contributed to this paper. This paper\n  has been accepted by KDD 2025"},{"id":"http://arxiv.org/abs/2412.19649v1","updated":"2024-12-27T13:55:00Z","published":"2024-12-27T13:55:00Z","title":"Distributed Download from an External Data Source in Faulty Majority\n  Settings","summary":"  We extend the study of retrieval problems in distributed networks, focusing\non improving the efficiency and resilience of protocols in the \\emph{Data\nRetrieval (DR) Model}. The DR Model consists of a complete network (i.e., a\nclique) with $k$ peers, up to $\\beta k$ of which may be Byzantine (for $\\beta\n\\in [0, 1)$), and a trusted \\emph{External Data Source} comprising an array $X$\nof $n$ bits ($n \\gg k$) that the peers can query. Additionally, the peers can\nalso send messages to each other. In this work, we focus on the Download\nproblem that requires all peers to learn $X$. Our primary goal is to minimize\nthe maximum number of queries made by any honest peer and additionally optimize\ntime.\n  We begin with a randomized algorithm for the Download problem that achieves\noptimal query complexity up to a logarithmic factor. For the stronger dynamic\nadversary that can change the set of Byzantine peers from one round to the\nnext, we achieve the optimal time complexity in peer-to-peer communication but\nwith larger messages. In broadcast communication where all peers (including\nByzantine peers) are required to send the same message to all peers, with\nlarger messages, we achieve almost optimal time and query complexities for a\ndynamic adversary. Finally, in a more relaxed crash fault model, where peers\nstop responding after crashing, we address the Download problem in both\nsynchronous and asynchronous settings. Using a deterministic protocol, we\nobtain nearly optimal results for both query complexity and message sizes in\nthese scenarios.\n","authors":["John Augustine","Soumyottam Chatterjee","Valerie King","Manish Kumar","Shachar Meir","David Peleg"],"pdf_url":"https://arxiv.org/pdf/2412.19649v1.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2411.14590v2","updated":"2024-12-27T13:08:00Z","published":"2024-11-21T21:04:52Z","title":"LLOR: Automated Repair of OpenMP Programs","summary":"  In this paper, we present a technique for repairing data race errors in\nparallel programs written in C/C++ and Fortran using the OpenMP API. Our\ntechnique can also remove barriers that are deemed unnecessary for correctness.\nWe implement these ideas in our tool called LLOR, which takes a\nlanguage-independent approach to provide appropriate placements of\nsynchronization constructs to avoid data races. To the best of our knowledge,\nLLOR is the only tool that can repair parallel programs that use the OpenMP\nAPI. We showcase the capabilities of LLOR by performing extensive experiments\non 415 parallel programs.\n","authors":["Utpal Bora","Saurabh Joshi","Gautam Muduganti","Ramakrishna Upadrasta"],"pdf_url":"https://arxiv.org/pdf/2411.14590v2.pdf","comment":"23 pages, 1 algorithm, 2 figures, 26th International Conference on\n  Verification Model Checking and Abstract Interpretation (VMCAI 2025)"},{"id":"http://arxiv.org/abs/2405.11658v4","updated":"2024-12-27T11:56:37Z","published":"2024-05-19T20:10:55Z","title":"A Starting Point for Dynamic Community Detection with Leiden Algorithm","summary":"  Real-world graphs often evolve over time, making community or cluster\ndetection a crucial task. In this technical report, we extend three dynamic\napproaches - Naive-dynamic (ND), Delta-screening (DS), and Dynamic Frontier\n(DF) - to our multicore implementation of the Leiden algorithm, known for its\nhigh-quality community detection. Our experiments, conducted on a server with a\n64-core AMD EPYC-7742 processor, show that ND, DS, and DF Leiden achieve\naverage speedups of 1.37x, 1.47x, and 1.98x on large graphs with random batch\nupdates, compared to the Static Leiden algorithm - while scaling at a rate of\n1.6x for every doubling of threads. To our knowledge, this is the first attempt\nto apply dynamic approaches to the Leiden algorithm. We hope these early\nresults pave the way for further development of dynamic approaches for evolving\ngraphs.\n","authors":["Subhajit Sahu"],"pdf_url":"https://arxiv.org/pdf/2405.11658v4.pdf","comment":"18 pages, 11 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:2404.19634"},{"id":"http://arxiv.org/abs/2411.02115v2","updated":"2024-12-27T05:48:14Z","published":"2024-11-04T14:29:04Z","title":"FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained\n  Aggregation","summary":"  Federated learning (FL) is a collaborative machine learning approach that\nenables multiple clients to train models without sharing their private data.\nWith the rise of deep learning, large-scale models have garnered significant\nattention due to their exceptional performance. However, a key challenge in FL\nis the limitation imposed by clients with constrained computational and\ncommunication resources, which hampers the deployment of these large models.\nThe Mixture of Experts (MoE) architecture addresses this challenge with its\nsparse activation property, which reduces computational workload and\ncommunication demands during inference and updates. Additionally, MoE\nfacilitates better personalization by allowing each expert to specialize in\ndifferent subsets of the data distribution. To alleviate the communication\nburdens between the server and clients, we propose FedMoE-DA, a new FL model\ntraining framework that leverages the MoE architecture and incorporates a novel\ndomain-aware, fine-grained aggregation strategy to enhance the robustness,\npersonalizability, and communication efficiency simultaneously. Specifically,\nthe correlation between both intra-client expert models and inter-client data\nheterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P)\ncommunication between clients for selective expert model synchronization, thus\nsignificantly reducing the server-client transmissions. Experiments demonstrate\nthat our FedMoE-DA achieves excellent performance while reducing the\ncommunication pressure on the server.\n","authors":["Ziwei Zhan","Wenkuan Zhao","Yuanqing Li","Weijie Liu","Xiaoxi Zhang","Chee Wei Tan","Chuan Wu","Deke Guo","Xu Chen"],"pdf_url":"https://arxiv.org/pdf/2411.02115v2.pdf","comment":"8 pages, 5 figures, accepted by The 20th International Conference on\n  Mobility, Sensing and Networking (MSN 2024)"},{"id":"http://arxiv.org/abs/2412.19446v1","updated":"2024-12-27T04:25:32Z","published":"2024-12-27T04:25:32Z","title":"Adrenaline: Adaptive Rendering Optimization System for Scalable Cloud\n  Gaming","summary":"  Cloud gaming requires a low-latency network connection, making it a prime\ncandidate for being hosted at the network edge. However, an edge server is\nprovisioned with a fixed compute capacity, causing an issue for multi-user\nservice and resulting in users having to wait before they can play when the\nserver is occupied. In this work, we present a new insight that when a user's\nnetwork condition results in use of lossy compression, the end-to-end visual\nquality more degrades for frames of high rendering quality, wasting the\nserver's computing resources. We leverage this observation to build Adrenaline,\na new system which adaptively optimizes the game rendering qualities by\nconsidering the user-side visual quality and server-side rendering cost. The\nrendering quality optimization of Adrenaline is done via a scoring mechanism\nquantifying the effectiveness of server resource usage on the user-side gaming\nquality. Our open-sourced implementation of Adrenaline demonstrates easy\nintegration with modern game engines. In our evaluations, Adrenaline achieves\nup to 24% higher service quality and 2x more users served with the same\nresource footprint compared to other baselines.\n","authors":["Jin Heo","Ketan Bhardwaj","Ada Gavrilovska"],"pdf_url":"https://arxiv.org/pdf/2412.19446v1.pdf","comment":"15 pages, 13 figures, 5 tables"},{"id":"http://arxiv.org/abs/2412.19442v1","updated":"2024-12-27T04:17:57Z","published":"2024-12-27T04:17:57Z","title":"A Survey on Large Language Model Acceleration based on KV Cache\n  Management","summary":"  Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.\n","authors":["Haoyang Li","Yiming Li","Anxin Tian","Tianhao Tang","Zhanchao Xu","Xuejia Chen","Nicole Hu","Wei Dong","Qing Li","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2412.19442v1.pdf","comment":null}],"Information Theory":[{"id":"http://arxiv.org/abs/2412.19792v1","updated":"2024-12-27T18:45:36Z","published":"2024-12-27T18:45:36Z","title":"InfAlign: Inference-aware language model alignment","summary":"  Language model alignment has become a critical step in training modern\ngenerative language models. The goal of alignment is to finetune a reference\nmodel such that the win rate of a sample from the aligned model over a sample\nfrom the reference model is high, subject to a KL divergence constraint. Today,\nwe are increasingly using inference-time algorithms (e.g., Best-of-N,\ncontrolled decoding, tree search) to decode from language models rather than\nstandard sampling. However, the alignment objective does not capture such\ninference-time decoding procedures. We show that the existing alignment\nframework is sub-optimal in view of such inference-time methods. We then modify\nthe alignment objective and propose a framework for inference-aware alignment\n(IAPO). We prove that for any inference-time decoding algorithm, the optimal\nsolution that optimizes the inference-time win rate of the aligned policy\nagainst the reference policy is the solution to the typical RLHF problem with a\ntransformation of the reward. This motivates us to provide the KL-regularized\ncalibrate-and-transform RL (CTRL) algorithm to solve this problem, which\ninvolves a reward calibration step and a KL-regularized reward maximization\nstep with a transformation of the calibrated reward. We particularize our study\nto two important inference-time strategies: best-of-N sampling and best-of-N\njailbreaking, where N responses are sampled from the model and the one with the\nhighest or lowest reward is selected. We propose specific transformations for\nthese strategies and demonstrate that our framework offers significant\nimprovements over existing state-of-the-art methods for language model\nalignment. Empirically, we outperform baselines that are designed without\ntaking inference-time decoding into consideration by 8-12% and 4-9% on\ninference-time win rates over the Anthropic helpfulness and harmlessness dialog\nbenchmark datasets.\n","authors":["Ananth Balashankar","Ziteng Sun","Jonathan Berant","Jacob Eisenstein","Michael Collins","Adrian Hutter","Jong Lee","Chirag Nagpal","Flavien Prost","Aradhana Sinha","and Ananda Theertha Suresh","Ahmad Beirami"],"pdf_url":"https://arxiv.org/pdf/2412.19792v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19748v1","updated":"2024-12-27T17:15:20Z","published":"2024-12-27T17:15:20Z","title":"UAV-Enabled Secure ISAC Against Dual Eavesdropping Threats: Joint\n  Beamforming and Trajectory Design","summary":"  In this work, we study an unmanned aerial vehicle (UAV)-enabled secure\nintegrated sensing and communication (ISAC) system, where a UAV serves as an\naerial base station (BS) to simultaneously perform communication with a user\nand detect a target on the ground, while a dual-functional eavesdropper\nattempts to intercept the signals for both sensing and communication. Facing\nthe dual eavesdropping threats, we aim to enhance the average achievable\nsecrecy rate for the communication user by jointly designing the UAV trajectory\ntogether with the transmit information and sensing beamforming, while\nsatisfying the requirements on sensing performance and sensing security, as\nwell as the UAV power and flight constraints. To address the non-convex nature\nof the optimization problem, we employ the alternating optimization (AO)\nstrategy, jointly with the successive convex approximation (SCA) and\nsemidefinite relaxation (SDR) methods. Numerical results validate the proposed\napproach, demonstrating its ability to achieve a high secrecy rate while\nmeeting the required sensing and security constraints.\n","authors":["Jianping Yao","Zeyu Yang","Zai Yang","Jie Xu","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2412.19748v1.pdf","comment":"7 pages, 6 figures, submitted for possible publication"},{"id":"http://arxiv.org/abs/2412.19677v1","updated":"2024-12-27T14:57:40Z","published":"2024-12-27T14:57:40Z","title":"Deep ReLU networks -- injectivity capacity upper bounds","summary":"  We study deep ReLU feed forward neural networks (NN) and their injectivity\nabilities. The main focus is on \\emph{precisely} determining the so-called\ninjectivity capacity. For any given hidden layers architecture, it is defined\nas the minimal ratio between number of network's outputs and inputs which\nensures unique recoverability of the input from a realizable output. A strong\nrecent progress in precisely studying single ReLU layer injectivity properties\nis here moved to a deep network level. In particular, we develop a program that\nconnects deep $l$-layer net injectivity to an $l$-extension of the $\\ell_0$\nspherical perceptrons, thereby massively generalizing an isomorphism between\nstudying single layer injectivity and the capacity of the so-called\n(1-extension) $\\ell_0$ spherical perceptrons discussed in [82]. \\emph{Random\nduality theory} (RDT) based machinery is then created and utilized to\nstatistically handle properties of the extended $\\ell_0$ spherical perceptrons\nand implicitly of the deep ReLU NNs. A sizeable set of numerical evaluations is\nconducted as well to put the entire RDT machinery in practical use. From these\nwe observe a rapidly decreasing tendency in needed layers' expansions, i.e., we\nobserve a rapid \\emph{expansion saturation effect}. Only $4$ layers of depth\nare sufficient to closely approach level of no needed expansion -- a result\nthat fairly closely resembles observations made in practical experiments and\nthat has so far remained completely untouchable by any of the existing\nmathematical methodologies.\n","authors":["Mihailo Stojnic"],"pdf_url":"https://arxiv.org/pdf/2412.19677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19494v1","updated":"2024-12-27T07:30:01Z","published":"2024-12-27T07:30:01Z","title":"Retrieval-augmented Generation for GenAI-enabled Semantic Communications","summary":"  Semantic communication (SemCom) is an emerging paradigm aiming at\ntransmitting only task-relevant semantic information to the receiver, which can\nsignificantly improve communication efficiency. Recent advancements in\ngenerative artificial intelligence (GenAI) have empowered GenAI-enabled SemCom\n(GenSemCom) to further expand its potential in various applications. However,\ncurrent GenSemCom systems still face challenges such as semantic inconsistency,\nlimited adaptability to diverse tasks and dynamic environments, and the\ninability to leverage insights from past transmission. Motivated by the success\nof retrieval-augmented generation (RAG) in the domain of GenAI, this paper\nexplores the integration of RAG in GenSemCom systems. Specifically, we first\nprovide a comprehensive review of existing GenSemCom systems and the\nfundamentals of RAG techniques. We then discuss how RAG can be integrated into\nGenSemCom. Following this, we conduct a case study on semantic image\ntransmission using an RAG-enabled diffusion-based SemCom system, demonstrating\nthe effectiveness of the proposed integration. Finally, we outline future\ndirections for advancing RAG-enabled GenSemCom systems.\n","authors":["Shunpu Tang","Ruichen Zhang","Yuxuan Yan","Qianqian Yang","Dusit Niyato","Xianbin Wang","Shiwen Mao"],"pdf_url":"https://arxiv.org/pdf/2412.19494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.04893v2","updated":"2024-12-27T06:43:10Z","published":"2024-11-07T17:32:04Z","title":"Efficient quantum pseudorandomness under conservation laws","summary":"  The efficiency of locally generating unitary designs, which capture\nstatistical notions of quantum pseudorandomness, lies at the heart of\nwide-ranging areas in physics and quantum information technologies. While there\nare extensive potent methods and results for this problem, the evidently\nimportant setting where continuous symmetries or conservation laws (most\nnotably U(1) and SU(d)) are involved is known to present fundamental\ndifficulties. In particular, even the basic question of whether any local\nsymmetric circuit can generate 2-designs efficiently (in time that grows at\nmost polynomially in the system size) remains open with no circuit\nconstructions provably known to do so, despite intensive efforts. In this work,\nwe resolve this long-standing open problem for both U(1) and SU(d) symmetries\nby explicitly constructing local symmetric quantum circuits which we prove to\nconverge to symmetric unitary 2-designs in polynomial time using a combination\nof representation theory, graph theory, and Markov chain methods. As a direct\napplication, our constructions can be used to efficiently generate near-optimal\ncovariant quantum error-correcting codes, confirming a conjecture in [PRX\nQuantum 3, 020314 (2022)].\n","authors":["Zimu Li","Han Zheng","Zi-Wen Liu"],"pdf_url":"https://arxiv.org/pdf/2411.04893v2.pdf","comment":"9 + 48 pages"},{"id":"http://arxiv.org/abs/2412.19470v1","updated":"2024-12-27T05:45:35Z","published":"2024-12-27T05:45:35Z","title":"Movable Antenna-Aided Near-Field Integrated Sensing and Communication","summary":"  Integrated sensing and communication (ISAC) is emerging as a pivotal\ntechnology for next-generation wireless networks. However, existing ISAC\nsystems are based on fixed-position antennas (FPAs), which inevitably incur a\nloss in performance when balancing the trade-off between sensing and\ncommunication. Movable antenna (MA) technology offers promising potential to\nenhance ISAC performance by enabling flexible antenna movement. Nevertheless,\nexploiting more spatial channel variations requires larger antenna moving\nregions, which may invalidate the conventional far-field assumption for\nchannels between transceivers. Therefore, this paper utilizes the MA to enhance\nsensing and communication capabilities in near-field ISAC systems, where a\nfull-duplex base station (BS) is equipped with multiple transmit and receive\nMAs movable in large-size regions to simultaneously sense multiple targets and\nserve multiple uplink (UL) and downlink (DL) users for communication. We aim to\nmaximize the weighted sum of sensing and communication rates (WSR) by jointly\ndesigning the transmit beamformers, sensing signal covariance matrices, receive\nbeamformers, and MA positions at the BS, as well as the UL power allocation.\nThe resulting optimization problem is challenging to solve, while we propose an\nefficient two-layer random position (RP) algorithm to tackle it. In addition,\nto reduce movement delay and cost, we design an antenna position matching (APM)\nalgorithm based on the greedy strategy to minimize the total MA movement\ndistance. Extensive simulation results demonstrate the substantial performance\nimprovement achieved by deploying MAs in near-field ISAC systems. Moreover, the\nresults show the effectiveness of the proposed APM algorithm in reducing the\nantenna movement distance, which is helpful for energy saving and time overhead\nreduction for MA-aided near-field ISAC systems with large moving regions.\n","authors":["Jingze Ding","Zijian Zhou","Xiaodan Shao","Bingli Jiao","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.19470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.06358v3","updated":"2024-12-27T05:32:12Z","published":"2022-03-12T06:41:32Z","title":"Throughput Maximization for UAV-enabled Integrated Periodic Sensing and\n  Communication","summary":"  Unmanned aerial vehicle (UAV) is expected to revolutionize the existing\nintegrated sensing and communication (ISAC) system and promise a more flexible\njoint design. Nevertheless, the existing works on ISAC mainly focus on\nexploring the performance of both functionalities simultaneously during the\nentire considered period, which may ignore the practical asymmetric sensing and\ncommunication requirements. In particular, always forcing sensing along with\ncommunication may make it is harder to balance between these two\nfunctionalities due to shared spectrum resources and limited transmit power. To\naddress this issue, we propose a new integrated periodic sensing and\ncommunication mechanism for the UAV-enabled ISAC system to provide a more\nflexible trade-off between two integrated functionalities. Specifically, the\nsystem achievable rate is maximized via jointly optimizing UAV trajectory, user\nassociation, target sensing selection, and transmit beamforming, while meeting\nthe sensing frequency and beam pattern gain requirement for the given targets.\nDespite that this problem is highly non-convex and involves closely coupled\ninteger variables, we derive the closed-form optimal beamforming vector to\ndramatically reduce the complexity of beamforming design, and present a tight\nlower bound of the achievable rate to facilitate UAV trajectory design. Based\non the above results, we propose a penalty-based algorithm to efficiently solve\nthe considered problem. The optimal achievable rate and the optimal UAV\nlocation are analyzed under a special case of infinity number of antennas.\nFurthermore, we prove the structural symmetry between the optimal solutions in\ndifferent ISAC frames without location constraints and propose an efficient\nalgorithm for solving the problem with location constraints.\n","authors":["Kaitao Meng","Qingqing Wu","Shaodan Ma","Wen Chen","Kunlun Wang","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2203.06358v3.pdf","comment":"This is an updated version revising an issue in (P2.5)"},{"id":"http://arxiv.org/abs/2412.19438v1","updated":"2024-12-27T04:06:32Z","published":"2024-12-27T04:06:32Z","title":"The Rendezvous Between Extreme Value Theory and Next-generation Networks","summary":"  Promising technologies such as massive multiple-input and multiple-output,\nreconfigurable intelligent reflecting surfaces, non-terrestrial networks,\nmillimetre wave communication, ultra-reliable lowlatency communication are\nenvisioned as the enablers for next-generation (NG) networks. In contrast to\nconventional communication systems meeting specific average performance\nrequirements, NG systems are expected to meet quality-of-service requirements\nin extreme scenarios as well. In this regard, extreme value theory (EVT)\nprovides a powerful framework for the design of communication systems. In this\npaper, we provide a comprehensive survey of advances in communication that\nutilize EVT to characterize the extreme order statistics of interest. We first\ngive an overview of the history of EVT and then elaborate on the fundamental\ntheorems. Subsequently, we discuss different problems of interest in NG\ncommunication systems and how EVT can be utilized for their analysis. We\nfinally point out the open challenges and future directions of EVT in NG\ncommunication systems.\n","authors":["Srinivas Sagar","Athira Subhash","Chen-Feng Liu","Ahmed Elzanaty","Yazan H. Al-Badarneh","Sheetal Kalyani","Mohamed-Slim Alouini","Mehdi Bennis","Lajos Hanzo"],"pdf_url":"https://arxiv.org/pdf/2412.19438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19396v1","updated":"2024-12-27T01:10:17Z","published":"2024-12-27T01:10:17Z","title":"Comparing Few to Rank Many: Active Human Preference Learning using\n  Randomized Frank-Wolfe","summary":"  We study learning of human preferences from a limited comparison feedback.\nThis task is ubiquitous in machine learning. Its applications such as\nreinforcement learning from human feedback, have been transformational. We\nformulate this problem as learning a Plackett-Luce model over a universe of $N$\nchoices from $K$-way comparison feedback, where typically $K \\ll N$. Our\nsolution is the D-optimal design for the Plackett-Luce objective. The design\ndefines a data logging policy that elicits comparison feedback for a small\ncollection of optimally chosen points from all ${N \\choose K}$ feasible\nsubsets. The main algorithmic challenge in this work is that even fast methods\nfor solving D-optimal designs would have $O({N \\choose K})$ time complexity. To\naddress this issue, we propose a randomized Frank-Wolfe (FW) algorithm that\nsolves the linear maximization sub-problems in the FW method on randomly chosen\nvariables. We analyze the algorithm, and evaluate it empirically on synthetic\nand open-source NLP datasets.\n","authors":["Kiran Koshy Thekumparampil","Gaurush Hiranandani","Kousha Kalantari","Shoham Sabach","Branislav Kveton"],"pdf_url":"https://arxiv.org/pdf/2412.19396v1.pdf","comment":"Submitted to AISTATS 2025 on October 10, 2024"}],"Signal Processing":[{"id":"http://arxiv.org/abs/2412.19763v1","updated":"2024-12-27T17:45:10Z","published":"2024-12-27T17:45:10Z","title":"Multi-population Differential Evolution for RSS based Cooperative\n  Localization in Wireless Sensor Networks with Limited Communication Range","summary":"  This paper presents a novel approach to deal with the cooperative\nlocalization problem in wireless sensor networks based on received signal\nstrength measurements. In cooperative scenarios, the cost function of the\nlocalization problem becomes increasingly nonlinear and nonconvex due to the\nheightened interaction between sensor nodes, making the estimation of the\npositions of the target nodes more challenging. Although most of existing\ncooperative localization algorithms assure acceptable localization accuracy,\ntheir computational complexity increases dramatically, which may restrict their\napplicability. To reduce the computational complexity and provide competitive\nlocalization accuracy at the same time, we propose a localization algorithm\nbased on the differential evolution with multiple populations, opposite-based\nlearning, redirection, and anchoring. In this work, the cooperative\nlocalization cost function is split into several simpler cost functions, each\nof which accounts only for one individual target node. Then, each cost function\nis solved by a dedicated population of the proposed algorithm. In addition, an\nenhanced version of the proposed algorithm which incorporates the population\nmidpoint scheme for further improvement in the localization accuracy is\ndevised. Simulation results demonstrate that the proposed algorithms provide\ncomparative localization accuracy with much lower computational complexity\ncompared with the state-of-the-art algorithms.\n","authors":["Lismer Andres Caceres Najarro","Iickho Song","Muhammad Salman","Kiseon Kim"],"pdf_url":"https://arxiv.org/pdf/2412.19763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19748v1","updated":"2024-12-27T17:15:20Z","published":"2024-12-27T17:15:20Z","title":"UAV-Enabled Secure ISAC Against Dual Eavesdropping Threats: Joint\n  Beamforming and Trajectory Design","summary":"  In this work, we study an unmanned aerial vehicle (UAV)-enabled secure\nintegrated sensing and communication (ISAC) system, where a UAV serves as an\naerial base station (BS) to simultaneously perform communication with a user\nand detect a target on the ground, while a dual-functional eavesdropper\nattempts to intercept the signals for both sensing and communication. Facing\nthe dual eavesdropping threats, we aim to enhance the average achievable\nsecrecy rate for the communication user by jointly designing the UAV trajectory\ntogether with the transmit information and sensing beamforming, while\nsatisfying the requirements on sensing performance and sensing security, as\nwell as the UAV power and flight constraints. To address the non-convex nature\nof the optimization problem, we employ the alternating optimization (AO)\nstrategy, jointly with the successive convex approximation (SCA) and\nsemidefinite relaxation (SDR) methods. Numerical results validate the proposed\napproach, demonstrating its ability to achieve a high secrecy rate while\nmeeting the required sensing and security constraints.\n","authors":["Jianping Yao","Zeyu Yang","Zai Yang","Jie Xu","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2412.19748v1.pdf","comment":"7 pages, 6 figures, submitted for possible publication"},{"id":"http://arxiv.org/abs/2412.19656v1","updated":"2024-12-27T14:05:08Z","published":"2024-12-27T14:05:08Z","title":"Movable Antenna Aided Physical Layer Security with No Eavesdropper CSI","summary":"  A novel movable antenna (MA)-aided secure transmission framework is proposed\nto enhance the secrecy transmission rate without relying on the eavesdropper's\nchannel state information. Within this framework, a joint beamforming and\njamming scheme is proposed, where the power of the confidential signal is\nminimized by optimizing the positions of the MAs, and the residual power is\nused to jam the eavesdropper. An efficient gradient-based method is employed to\nsolve this non-convex problem. Numerical results are provided to demonstrate\nthe superiority of the MA-based framework over systems using traditional\nfixed-position antennas in secure transmission.\n","authors":["Zhenqiao Cheng","Chongjun Ouyang","Xingqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.19656v1.pdf","comment":"Accepted by IEEE ICASSP 2025"},{"id":"http://arxiv.org/abs/2412.19585v1","updated":"2024-12-27T11:03:26Z","published":"2024-12-27T11:03:26Z","title":"Ultralight Signal Classification Model for Automatic Modulation\n  Recognition","summary":"  The growing complexity of radar signals demands responsive and accurate\ndetection systems that can operate efficiently on resource-constrained edge\ndevices. Existing models, while effective, often rely on substantial\ncomputational resources and large datasets, making them impractical for edge\ndeployment. In this work, we propose an ultralight hybrid neural network\noptimized for edge applications, delivering robust performance across\nunfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less\nthan 100 samples per class, and significantly reducing computational overhead.\n","authors":["Alessandro Daniele Genuardi Oquendo","Agust√≠n Mat√≠as Galante Cervi√±o","Nilotpal Sinha","Luc Andrea","Sam Mugel","Rom√°n Or√∫s"],"pdf_url":"https://arxiv.org/pdf/2412.19585v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.19549v1","updated":"2024-12-27T09:40:29Z","published":"2024-12-27T09:40:29Z","title":"Performance Evaluation of IoT LoRa Networks on Mars Through ns-3\n  Simulations","summary":"  In recent years, there has been a significant surge of interest in Mars\nexploration, driven by the planet's potential for human settlement and its\nproximity to Earth. In this paper, we explore the performance of the LoRaWAN\ntechnology on Mars, to study whether commercial off-the-shelf IoT products,\ndesigned and developed on Earth, can be deployed on the Martian surface. We use\nthe ns-3 simulator to model various environmental conditions, primarily\nfocusing on the Free Space Path Loss (FSPL) and the impact of Martian dust\nstorms. Simulation results are given with respect to Earth, as a function of\nthe distance, packet size, offered traffic, and the impact of Mars' atmospheric\nperturbations. We show that LoRaWAN can be a viable communication solution on\nMars, although the performance is heavily affected by the extreme Martian\nenvironment over long distances.\n","authors":["Manuele Favero","Alessandro Canova","Marco Giordani","Michele Zorzi"],"pdf_url":"https://arxiv.org/pdf/2412.19549v1.pdf","comment":"This paper has been accepted for presentation at the 2025\n  International Conference on Computing, Networking and Communications (ICNC)"},{"id":"http://arxiv.org/abs/2412.19527v1","updated":"2024-12-27T08:41:46Z","published":"2024-12-27T08:41:46Z","title":"Real-time Reflectance Generation for UAV Multispectral Imagery using an\n  Onboard Downwelling Spectrometer in Varied Weather Conditions","summary":"  Advancements in unmanned aerial vehicle (UAV) remote sensing with spectral\nimaging enable efficient assessment of critical agronomic traits. However,\nexisting reflectance calibration or generation methods suffer from limited\nprediction accuracy and practical flexibility. This study explores reliable and\ncost-efficient methods for the accurate conversion of digital number values\nacquired from a multispectral imager into reflectance, leveraging real-time\nsolar spectra as references. To ensure consistent measurements of incident\nlight, an upward gimbal-mounted downwelling spectrometer was attached to the\nUAV, and a sinusoidal model was developed to correct for solar position\nvariability. Using principal component analysis on the reference solar spectrum\nfor band selection, a multiple linear regression model with four sensitive\nbands (4-Band MLR) and a 30 nm bandwidth achieved performance comparable to the\ndirect correction method. The root mean square error (RMSE) for reflectance\nprediction improved by 86.1% compared to the empirical line method under\nfluctuating cloudy conditions and by 59.6% compared to the downwelling light\nsensor method averaged across different weather conditions. The RMSE was\ncalculated as 2.24% in a ground-based diurnal validation, and 2.03% in a UAV\ncampaign conducted at various times throughout a sunny day. Implementing the\n4-Band MLR model enhanced the consistency of canopy reflectance within a\nhomogeneous vegetation area by 95.0% during spectral imaging in a large rice\nfield under significant cloud fluctuations. Additionally, improvements of 86.0%\nand 90.3% were noted for two vegetation indices: the normalized difference\nvegetation index (NDVI; a ratio index) and the difference vegetation index\n(DVI; a non-ratio index), respectively.\n","authors":["Jiayang Xie","Yutao Shen","Haiyan Cen"],"pdf_url":"https://arxiv.org/pdf/2412.19527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19494v1","updated":"2024-12-27T07:30:01Z","published":"2024-12-27T07:30:01Z","title":"Retrieval-augmented Generation for GenAI-enabled Semantic Communications","summary":"  Semantic communication (SemCom) is an emerging paradigm aiming at\ntransmitting only task-relevant semantic information to the receiver, which can\nsignificantly improve communication efficiency. Recent advancements in\ngenerative artificial intelligence (GenAI) have empowered GenAI-enabled SemCom\n(GenSemCom) to further expand its potential in various applications. However,\ncurrent GenSemCom systems still face challenges such as semantic inconsistency,\nlimited adaptability to diverse tasks and dynamic environments, and the\ninability to leverage insights from past transmission. Motivated by the success\nof retrieval-augmented generation (RAG) in the domain of GenAI, this paper\nexplores the integration of RAG in GenSemCom systems. Specifically, we first\nprovide a comprehensive review of existing GenSemCom systems and the\nfundamentals of RAG techniques. We then discuss how RAG can be integrated into\nGenSemCom. Following this, we conduct a case study on semantic image\ntransmission using an RAG-enabled diffusion-based SemCom system, demonstrating\nthe effectiveness of the proposed integration. Finally, we outline future\ndirections for advancing RAG-enabled GenSemCom systems.\n","authors":["Shunpu Tang","Ruichen Zhang","Yuxuan Yan","Qianqian Yang","Dusit Niyato","Xianbin Wang","Shiwen Mao"],"pdf_url":"https://arxiv.org/pdf/2412.19494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19478v1","updated":"2024-12-27T06:11:28Z","published":"2024-12-27T06:11:28Z","title":"An Overview of Machine Learning-Driven Resource Allocation in IoT\n  Networks","summary":"  In the wake of disruptive IoT technologies generating massive amounts of\ndiverse data, Machine Learning (ML) will play a crucial role in bringing\nintelligence to Internet of Things (IoT) networks. This paper provides a\ncomprehensive analysis of the current state of resource allocation within IoT\nnetworks, focusing specifically on two key categories: Low-Power IoT Networks\nand Mobile IoT Networks. We delve into the resource allocation strategies that\nare crucial for optimizing network performance and energy efficiency in these\nenvironments. Furthermore, the paper explores the transformative role of\nMachine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in\nenhancing IoT functionalities. We highlight a range of applications and use\ncases where these advanced technologies can significantly improve\ndecision-making and optimization processes. In addition to the opportunities\npresented by ML, DL, and RL, we also address the potential challenges that\norganizations may face when implementing these technologies in IoT settings.\nThese challenges include crucial accuracy, low flexibility and adaptability,\nand high computational cost, etc. Finally, the paper identifies promising\navenues for future research, emphasizing the need for innovative solutions to\novercome existing hurdles and improve the integration of ML, DL, and RL into\nIoT networks. By providing this holistic perspective, we aim to contribute to\nthe ongoing discourse on resource allocation strategies and the application of\nintelligent technologies in the IoT landscape.\n","authors":["Zhengdong Li"],"pdf_url":"https://arxiv.org/pdf/2412.19478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19475v1","updated":"2024-12-27T06:00:30Z","published":"2024-12-27T06:00:30Z","title":"Exploiting Dynamic Sparsity for Near-Field Spatial Non-Stationary\n  XL-MIMO Channel Tracking","summary":"  This work considers a spatial non-stationary channel tracking problem in\nbroadband extremely large-scale multiple-input-multiple-output (XL-MIMO)\nsystems. In the case of spatial non-stationary, each scatterer has a certain\nvisibility region (VR) over antennas and power change may occur among visible\nantennas. Concentrating on the temporal correlation of XL-MIMO channels, we\ndesign a three-layer Markov prior model and hierarchical two-dimensional (2D)\nMarkov model to exploit the dynamic sparsity of sparse channel vectors and VRs,\nrespectively. Then, we formulate the channel tracking problem as a bilinear\nmeasurement process, and a novel dynamic alternating maximum a posteriori\n(DA-MAP) framework is developed to solve the problem. The DA-MAP contains four\nbasic modules: channel estimation module, VR detection module, grid update\nmodule, and temporal correlated module. Specifically, the first module is an\ninverse-free variational Bayesian inference (IF-VBI) estimator that avoids\ncomputational intensive matrix inverse each iteration; the second module is a\nturbo compressive sensing (Turbo-CS) algorithm that only needs small-scale\nmatrix operations in a parallel fashion; the third module refines the\npolar-delay domain grid; and the fourth module can process the temporal prior\ninformation to ensure high-efficiency channel tracking. Simulations show that\nthe proposed method can achieve a significant channel tracking performance\nwhile achieving low computational overhead.\n","authors":["Wenkang Xu amd An Liu","Min-jian Zhao","Giuseppe Caire","Yik-Chung Wu"],"pdf_url":"https://arxiv.org/pdf/2412.19475v1.pdf","comment":"13 pages, 11 figures,Submitted to IEEE TSP"},{"id":"http://arxiv.org/abs/2412.19470v1","updated":"2024-12-27T05:45:35Z","published":"2024-12-27T05:45:35Z","title":"Movable Antenna-Aided Near-Field Integrated Sensing and Communication","summary":"  Integrated sensing and communication (ISAC) is emerging as a pivotal\ntechnology for next-generation wireless networks. However, existing ISAC\nsystems are based on fixed-position antennas (FPAs), which inevitably incur a\nloss in performance when balancing the trade-off between sensing and\ncommunication. Movable antenna (MA) technology offers promising potential to\nenhance ISAC performance by enabling flexible antenna movement. Nevertheless,\nexploiting more spatial channel variations requires larger antenna moving\nregions, which may invalidate the conventional far-field assumption for\nchannels between transceivers. Therefore, this paper utilizes the MA to enhance\nsensing and communication capabilities in near-field ISAC systems, where a\nfull-duplex base station (BS) is equipped with multiple transmit and receive\nMAs movable in large-size regions to simultaneously sense multiple targets and\nserve multiple uplink (UL) and downlink (DL) users for communication. We aim to\nmaximize the weighted sum of sensing and communication rates (WSR) by jointly\ndesigning the transmit beamformers, sensing signal covariance matrices, receive\nbeamformers, and MA positions at the BS, as well as the UL power allocation.\nThe resulting optimization problem is challenging to solve, while we propose an\nefficient two-layer random position (RP) algorithm to tackle it. In addition,\nto reduce movement delay and cost, we design an antenna position matching (APM)\nalgorithm based on the greedy strategy to minimize the total MA movement\ndistance. Extensive simulation results demonstrate the substantial performance\nimprovement achieved by deploying MAs in near-field ISAC systems. Moreover, the\nresults show the effectiveness of the proposed APM algorithm in reducing the\nantenna movement distance, which is helpful for energy saving and time overhead\nreduction for MA-aided near-field ISAC systems with large moving regions.\n","authors":["Jingze Ding","Zijian Zhou","Xiaodan Shao","Bingli Jiao","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.19470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.06358v3","updated":"2024-12-27T05:32:12Z","published":"2022-03-12T06:41:32Z","title":"Throughput Maximization for UAV-enabled Integrated Periodic Sensing and\n  Communication","summary":"  Unmanned aerial vehicle (UAV) is expected to revolutionize the existing\nintegrated sensing and communication (ISAC) system and promise a more flexible\njoint design. Nevertheless, the existing works on ISAC mainly focus on\nexploring the performance of both functionalities simultaneously during the\nentire considered period, which may ignore the practical asymmetric sensing and\ncommunication requirements. In particular, always forcing sensing along with\ncommunication may make it is harder to balance between these two\nfunctionalities due to shared spectrum resources and limited transmit power. To\naddress this issue, we propose a new integrated periodic sensing and\ncommunication mechanism for the UAV-enabled ISAC system to provide a more\nflexible trade-off between two integrated functionalities. Specifically, the\nsystem achievable rate is maximized via jointly optimizing UAV trajectory, user\nassociation, target sensing selection, and transmit beamforming, while meeting\nthe sensing frequency and beam pattern gain requirement for the given targets.\nDespite that this problem is highly non-convex and involves closely coupled\ninteger variables, we derive the closed-form optimal beamforming vector to\ndramatically reduce the complexity of beamforming design, and present a tight\nlower bound of the achievable rate to facilitate UAV trajectory design. Based\non the above results, we propose a penalty-based algorithm to efficiently solve\nthe considered problem. The optimal achievable rate and the optimal UAV\nlocation are analyzed under a special case of infinity number of antennas.\nFurthermore, we prove the structural symmetry between the optimal solutions in\ndifferent ISAC frames without location constraints and propose an efficient\nalgorithm for solving the problem with location constraints.\n","authors":["Kaitao Meng","Qingqing Wu","Shaodan Ma","Wen Chen","Kunlun Wang","Jun Li"],"pdf_url":"https://arxiv.org/pdf/2203.06358v3.pdf","comment":"This is an updated version revising an issue in (P2.5)"},{"id":"http://arxiv.org/abs/2403.19971v3","updated":"2024-12-27T02:48:43Z","published":"2024-03-29T04:42:12Z","title":"3D-Speaker-Toolkit: An Open-Source Toolkit for Multimodal Speaker\n  Verification and Diarization","summary":"  We introduce 3D-Speaker-Toolkit, an open-source toolkit for multimodal\nspeaker verification and diarization, designed for meeting the needs of\nacademic researchers and industrial practitioners. The 3D-Speaker-Toolkit\nadeptly leverages the combined strengths of acoustic, semantic, and visual\ndata, seamlessly fusing these modalities to offer robust speaker recognition\ncapabilities. The acoustic module extracts speaker embeddings from acoustic\nfeatures, employing both fully-supervised and self-supervised learning\napproaches. The semantic module leverages advanced language models to\ncomprehend the substance and context of spoken language, thereby augmenting the\nsystem's proficiency in distinguishing speakers through linguistic patterns.\nThe visual module applies image processing technologies to scrutinize facial\nfeatures, which bolsters the precision of speaker diarization in multi-speaker\nenvironments. Collectively, these modules empower the 3D-Speaker-Toolkit to\nachieve substantially improved accuracy and reliability in speaker-related\ntasks. With 3D-Speaker-Toolkit, we establish a new benchmark for multimodal\nspeaker analysis. The toolkit also includes a handful of open-source\nstate-of-the-art models and a large-scale dataset containing over 10,000\nspeakers. The toolkit is publicly available at\nhttps://github.com/modelscope/3D-Speaker.\n","authors":["Yafeng Chen","Siqi Zheng","Hui Wang","Luyao Cheng","Tinglong Zhu","Rongjie Huang","Chong Deng","Qian Chen","Shiliang Zhang","Wen Wang","Xihao Li"],"pdf_url":"https://arxiv.org/pdf/2403.19971v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18122v2","updated":"2024-12-27T02:12:37Z","published":"2024-12-24T03:13:45Z","title":"FOGNA: An effective Sum-Difference Co-Array Design Based on Fourth-Order\n  Cumulants","summary":"  Array structures based on the fourth-order difference co-array (FODCA)\nprovide more degrees of freedom (DOF). However, since the growth of DOF is\nlimited by a single case of fourth-order cumulant in FODCA, this paper aims to\ndesign a sparse linear array (SLA) with higher DOF via exploring different\ncases of fourth-order cumulants. This paper presents a mathematical framework\nbased on fourth-order cumulant to devise a fourth-order extend co-array\n(FOECA), which is equivalent to FODCA. A novel SLA, namely fourth-order\ngeneralized nested array (FOGNA), is proposed based on FOECA to provide\nclosed-form expressions for the sensor locations and enhance DOF to resolve\nmore signal sources in direction of arrival (DOA) estimation. FOGNA is\nconsisted of three subarrays, where the first is a concatenated nested array\nand the other two subarrays are SLA with big inter-spacing between sensors.\nWhen the total physical sensors of FOGNA are given, the number of sensors in\neach subarray is determined by the designed method, which can obtain the\nmaximum DOF under the proposed array structure and derive closed-form\nexpressions for the sensor locations of FOGNA. The proposed array structure not\nonly achieves higher DOF than those of existing FODCAs but also reduces mutual\ncoupling effects. Numerical simulations are conducted to verify the superiority\nof FOGNA on DOA estimation performance and enhanced DOF over other existing\nFODCAs.\n","authors":["Si Wang","Guoqiang Xiao"],"pdf_url":"https://arxiv.org/pdf/2412.18122v2.pdf","comment":"16 pages, 29 figures"},{"id":"http://arxiv.org/abs/2412.19392v1","updated":"2024-12-27T00:44:34Z","published":"2024-12-27T00:44:34Z","title":"Asymptotically Optimal Search for a Change Point Anomaly under a\n  Composite Hypothesis Model","summary":"  We address the problem of searching for a change point in an anomalous\nprocess among a finite set of M processes. Specifically, we address a composite\nhypothesis model in which each process generates measurements following a\ncommon distribution with an unknown parameter (vector). This parameter belongs\nto either a normal or abnormal space depending on the current state of the\nprocess. Before the change point, all processes, including the anomalous one,\nare in a normal state; after the change point, the anomalous process\ntransitions to an abnormal state. Our goal is to design a sequential search\nstrategy that minimizes the Bayes risk by balancing sample complexity and\ndetection accuracy. We propose a deterministic search algorithm with the\nfollowing notable properties. First, we analytically demonstrate that when the\ndistributions of both normal and abnormal processes are unknown, the algorithm\nis asymptotically optimal in minimizing the Bayes risk as the error probability\napproaches zero. In the second setting, where the parameter under the null\nhypothesis is known, the algorithm achieves asymptotic optimality with improved\ndetection time based on the true normal state. Simulation results are presented\nto validate the theoretical findings.\n","authors":["Liad Lea Didi","Tomer Gafni","Kobi Cohen"],"pdf_url":"https://arxiv.org/pdf/2412.19392v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.21122v2","updated":"2024-12-27T23:32:21Z","published":"2024-07-30T18:26:58Z","title":"Shadow Area and Degrees of Freedom for Free-Space Communication","summary":"  The number of degrees-of-freedom (NDoF) in a communication system is limited\nby the number of antenna ports, element shapes, positions, and the propagation\nenvironment. As the number of antenna elements increases within a given region,\nthe NDoF eventually saturates due to correlation of the radiated fields. The\nmaximal NDoF can be determined numerically for communication between two\nregions using singular value decomposition of a channel model representing wave\npropagation between densely sampled sources at the transmitter and fields at\nthe receiver. This paper provides a straightforward analytical estimate of the\nNDoF for arbitrarily shaped transmitter and receiver regions. The analysis show\nthat the NDoF for electrically large regions is approximated by the mutual\nshadow area of the regions, measured in wavelengths. Several setups illustrate\nthe results, which are then compared with numerical evaluations of the singular\nvalues of the propagation channel. These new analytical expressions also\nsimplify to previously established results based on Weyl's law and the paraxial\napproximation.\n","authors":["Mats Gustafsson"],"pdf_url":"https://arxiv.org/pdf/2407.21122v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19950v1","updated":"2024-12-27T23:10:32Z","published":"2024-12-27T23:10:32Z","title":"Data-driven tool wear prediction in milling, based on a\n  process-integrated single-sensor approach","summary":"  Accurate tool wear prediction is essential for maintaining productivity and\nminimizing costs in machining. However, the complex nature of the tool wear\nprocess poses significant challenges to achieving reliable predictions. This\nstudy explores data-driven methods, in particular deep learning, for tool wear\nprediction. Traditional data-driven approaches often focus on a single process,\nrelying on multi-sensor setups and extensive data generation, which limits\ngeneralization to new settings. Moreover, multi-sensor integration is often\nimpractical in industrial environments. To address these limitations, this\nresearch investigates the transferability of predictive models using minimal\ntraining data, validated across two processes. Furthermore, it uses a simple\nsetup with a single acceleration sensor to establish a low-cost data generation\napproach that facilitates the generalization of models to other processes via\ntransfer learning. The study evaluates several machine learning models,\nincluding convolutional neural networks (CNN), long short-term memory networks\n(LSTM), support vector machines (SVM) and decision trees, trained on different\ninput formats such as feature vectors and short-time Fourier transform (STFT).\nThe performance of the models is evaluated on different amounts of training\ndata, including scenarios with significantly reduced datasets, providing\ninsight into their effectiveness under constrained data conditions. The results\ndemonstrate the potential of specific models and configurations for effective\ntool wear prediction, contributing to the development of more adaptable and\nefficient predictive maintenance strategies in machining. Notably, the ConvNeXt\nmodel has an exceptional performance, achieving an 99.1% accuracy in\nidentifying tool wear using data from only four milling tools operated until\nthey are worn.\n","authors":["Eric Hirsch","Christian Friedrich"],"pdf_url":"https://arxiv.org/pdf/2412.19950v1.pdf","comment":"14 pages, 9 figures"}]},"2024-12-30T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2412.19652v2","updated":"2024-12-30T07:49:13Z","published":"2024-12-27T13:56:51Z","title":"FreStega: A Plug-and-Play Method for Boosting Imperceptibility and\n  Capacity in Generative Linguistic Steganography for Real-World Scenarios","summary":"  Linguistic steganography embeds secret information in seemingly innocent\ntexts, safeguarding privacy in surveillance environments. Generative linguistic\nsteganography leverages the probability distribution of language models (LMs)\nand applies steganographic algorithms to generate stego tokens, gaining\nattention with recent Large Language Model (LLM) advancements. To enhance\nsecurity, researchers develop distribution-preserving stego algorithms to\nminimize the gap between stego sampling and LM sampling. However, the reliance\non language model distributions, coupled with deviations from real-world cover\ntexts, results in insufficient imperceptibility when facing steganalysis\ndetectors in real-world scenarios. Moreover, LLM distributions tend to be more\ndeterministic, resulting in reduced entropy and, consequently, lower embedding\ncapacity. In this paper, we propose FreStega, a plug-and-play method to\nreconstruct the distribution of language models used for generative linguistic\nsteganography. FreStega dynamically adjusts token probabilities from the\nlanguage model at each step of stegotext auto-regressive generation, leveraging\nboth sequential and spatial dimensions. In sequential adjustment, the\ntemperature is dynamically adjusted based on instantaneous entropy, enhancing\nthe diversity of stego texts and boosting embedding capacity. In the spatial\ndimension, the distribution is aligned with guidance from the target domain\ncorpus, closely mimicking real cover text in the target domain. By reforming\nthe distribution, FreStega enhances the imperceptibility of stego text in\npractical scenarios and improves steganographic capacity by 15.41\\%, all\nwithout compromising the quality of the generated text. FreStega serves as a\nplug-and-play remedy to enhance the imperceptibility and embedding capacity of\nexisting distribution-preserving steganography methods in real-world scenarios.\n","authors":["Kaiyi Pang"],"pdf_url":"https://arxiv.org/pdf/2412.19652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21164v1","updated":"2024-12-30T18:43:21Z","published":"2024-12-30T18:43:21Z","title":"Adversarial Attack and Defense for LoRa Device Identification and\n  Authentication via Deep Learning","summary":"  LoRa provides long-range, energy-efficient communications in Internet of\nThings (IoT) applications that rely on Low-Power Wide-Area Network (LPWAN)\ncapabilities. Despite these merits, concerns persist regarding the security of\nLoRa networks, especially in situations where device identification and\nauthentication are imperative to secure the reliable access to the LoRa\nnetworks. This paper explores a deep learning (DL) approach to tackle these\nconcerns, focusing on two critical tasks, namely (i) identifying LoRa devices\nand (ii) classifying them to legitimate and rogue devices. Deep neural networks\n(DNNs), encompassing both convolutional and feedforward neural networks, are\ntrained for these tasks using actual LoRa signal data. In this setting, the\nadversaries may spoof rogue LoRa signals through the kernel density estimation\n(KDE) method based on legitimate device signals that are received by the\nadversaries. Two cases are considered, (i) training two separate classifiers,\none for each of the two tasks, and (ii) training a multi-task classifier for\nboth tasks. The vulnerabilities of the resulting DNNs to manipulations in input\nsamples are studied in form of untargeted and targeted adversarial attacks\nusing the Fast Gradient Sign Method (FGSM). Individual and common perturbations\nare considered against single-task and multi-task classifiers for the LoRa\nsignal analysis. To provide resilience against such attacks, a defense approach\nis presented by increasing the robustness of classifiers with adversarial\ntraining. Results quantify how vulnerable LoRa signal classification tasks are\nto adversarial attacks and emphasize the need to fortify IoT applications\nagainst these subtle yet effective threats.\n","authors":["Yalin E. Sagduyu","Tugba Erpek"],"pdf_url":"https://arxiv.org/pdf/2412.21164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21123v1","updated":"2024-12-30T17:52:02Z","published":"2024-12-30T17:52:02Z","title":"ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language\n  Modeling Exploitation","summary":"  As large language models (LLMs) increasingly depend on web-scraped datasets,\nconcerns over unauthorized use of copyrighted or personal content for training\nhave intensified. Despite regulations such as the General Data Protection\nRegulation (GDPR), data owners still have limited control over the use of their\ncontent in model training. To address this, we propose ExpShield, a proactive\nself-guard mechanism that empowers content owners to embed invisible\nperturbations into their text, limiting data misuse in LLMs training without\naffecting readability. This preemptive approach enables data owners to protect\nsensitive content directly, without relying on a third-party to perform\ndefense. Starting from the random perturbation, we demonstrate the rationale\nfor using perturbation to conceal protected content. We further enhance the\nefficiency by identifying memorization triggers and creating pitfalls to\ndiverge the model memorization in a more focused way. To validate our defense's\neffectiveness, we propose a novel metric of instance exploitation which\ncaptures the individual risk raised by model training. The experimental results\nvalidate the effectiveness of our approach as the MIA AUC decreases from 0.95\nto 0.55, and instance exploitation approaches zero. This suggests that the\nindividual risk does not increase after training, underscoring the significance\nof proactive defenses in protecting copyrighted data.\n","authors":["Ruixuan Liu","Toan Tran","Tianhao Wang","Hongsheng Hu","Shuo Wang","Li Xiong"],"pdf_url":"https://arxiv.org/pdf/2412.21123v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2412.21084v1","updated":"2024-12-30T17:02:37Z","published":"2024-12-30T17:02:37Z","title":"On the Generalizability of Machine Learning-based Ransomware Detection\n  in Block Storage","summary":"  Ransomware represents a pervasive threat, traditionally countered at the\noperating system, file-system, or network levels. However, these approaches\noften introduce significant overhead and remain susceptible to circumvention by\nattackers. Recent research activity started looking into the detection of\nransomware by observing block IO operations. However, this approach exhibits\nsignificant detection challenges. Recognizing these limitations, our research\npivots towards enabling robust ransomware detection in storage systems keeping\nin mind their limited computational resources available. To perform our\nstudies, we propose a kernel-based framework capable of efficiently extracting\nand analyzing IO operations to identify ransomware activity. The framework can\nbe adopted to storage systems using computational storage devices to improve\nsecurity and fully hide detection overheads. Our method employs a refined set\nof computationally light features optimized for ML models to accurately discern\nmalicious from benign activities.\n  Using this lightweight approach, we study a wide range of generalizability\naspects and analyze the performance of these models across a large space of\nsetups and configurations covering a wide range of realistic real-world\nscenarios. We reveal various trade-offs and provide strong arguments for the\ngeneralizability of storage-based detection of ransomware and show that our\napproach outperforms currently available ML-based ransomware detection in\nstorage. Empirical validation reveals that our decision tree-based models\nachieve remarkable effectiveness, evidenced by higher median F1 scores of up to\n12.8%, lower false negative rates of up to 10.9% and particularly decreased\nfalse positive rates of up to 17.1% compared to existing storage-based\ndetection approaches.\n","authors":["Nicolas Reategui","Roman Pletka","Dionysios Diamantopoulos"],"pdf_url":"https://arxiv.org/pdf/2412.21084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21051v1","updated":"2024-12-30T16:09:28Z","published":"2024-12-30T16:09:28Z","title":"Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense","summary":"  The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided a large number of benefits in daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks. Recent advancements in generative\nfoundation models (GFMs), particularly in the large language models (LLMs),\noffer promising solutions for security intelligence. By exploiting the powerful\nabilities in language understanding, data analysis, task inference, action\nplanning, and code generation, we present LLM-PD, a novel proactive defense\narchitecture that defeats various threats in a proactive manner. LLM-PD can\nefficiently make a decision through comprehensive data analysis and sequential\nreasoning, as well as dynamically creating and deploying actionable defense\nmechanisms on the target cloud. Furthermore, it can flexibly self-evolve based\non experience learned from previous interactions and adapt to new attack\nscenarios without additional training. The experimental results demonstrate its\nremarkable ability in terms of defense effectiveness and efficiency,\nparticularly highlighting an outstanding success rate when compared with other\nexisting methods.\n","authors":["Yuyang Zhou","Guang Cheng","Kang Du","Zihan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.21051v1.pdf","comment":"7 pages; In submission"},{"id":"http://arxiv.org/abs/2412.21030v1","updated":"2024-12-30T15:56:34Z","published":"2024-12-30T15:56:34Z","title":"Improving Location-based Thermal Emission Side-Channel Analysis Using\n  Iterative Transfer Learning","summary":"  This paper proposes the use of iterative transfer learning applied to deep\nlearning models for side-channel attacks. Currently, most of the side-channel\nattack methods train a model for each individual byte, without considering the\ncorrelation between bytes. However, since the models' parameters for attacking\ndifferent bytes may be similar, we can leverage transfer learning, meaning that\nwe first train the model for one of the key bytes, then use the trained model\nas a pretrained model for the remaining bytes. This technique can be applied\niteratively, a process known as iterative transfer learning. Experimental\nresults show that when using thermal or power consumption map images as input,\nand multilayer perceptron or convolutional neural network as the model, our\nmethod improves average performance, especially when the amount of data is\ninsufficient.\n","authors":["Tun-Chieh Lou","Chung-Che Wang","Jyh-Shing Roger Jang","Henian Li","Lang Lin","Norman Chang"],"pdf_url":"https://arxiv.org/pdf/2412.21030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20953v1","updated":"2024-12-30T13:49:28Z","published":"2024-12-30T13:49:28Z","title":"GASLITEing the Retrieval: Exploring Vulnerabilities in Dense\n  Embedding-based Search","summary":"  Dense embedding-based text retrieval$\\unicode{x2013}$retrieval of relevant\npassages from corpora via deep learning encodings$\\unicode{x2013}$has emerged\nas a powerful method attaining state-of-the-art search results and popularizing\nthe use of Retrieval Augmented Generation (RAG). Still, like other search\nmethods, embedding-based retrieval may be susceptible to search-engine\noptimization (SEO) attacks, where adversaries promote malicious content by\nintroducing adversarial passages to corpora. To faithfully assess and gain\ninsights into the susceptibility of such systems to SEO, this work proposes the\nGASLITE attack, a mathematically principled gradient-based search method for\ngenerating adversarial passages without relying on the corpus content or\nmodifying the model. Notably, GASLITE's passages (1) carry adversary-chosen\ninformation while (2) achieving high retrieval ranking for a selected query\ndistribution when inserted to corpora. We use GASLITE to extensively evaluate\nretrievers' robustness, testing nine advanced models under varied threat\nmodels, while focusing on realistic adversaries targeting queries on a specific\nconcept (e.g., a public figure). We found GASLITE consistently outperformed\nbaselines by $\\geq$140% success rate, in all settings. Particularly,\nadversaries using GASLITE require minimal effort to manipulate search\nresults$\\unicode{x2013}$by injecting a negligible amount of adversarial\npassages ($\\leq$0.0001% of the corpus), they could make them visible in the\ntop-10 results for 61-100% of unseen concept-specific queries against most\nevaluated models. Inspecting variance in retrievers' robustness, we identify\nkey factors that may contribute to models' susceptibility to SEO, including\nspecific properties in the embedding space's geometry.\n","authors":["Matan Ben-Tov","Mahmood Sharif"],"pdf_url":"https://arxiv.org/pdf/2412.20953v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07500v2","updated":"2024-12-30T08:43:32Z","published":"2024-09-10T15:24:13Z","title":"DV-FSR: A Dual-View Target Attack Framework for Federated Sequential\n  Recommendation","summary":"  Federated recommendation (FedRec) preserves user privacy by enabling\ndecentralized training of personalized models, but this architecture is\ninherently vulnerable to adversarial attacks. Significant research has been\nconducted on targeted attacks in FedRec systems, motivated by commercial and\nsocial influence considerations. However, much of this work has largely\noverlooked the differential robustness of recommendation models. Moreover, our\nempirical findings indicate that existing targeted attack methods achieve only\nlimited effectiveness in Federated Sequential Recommendation (FSR) tasks.\nDriven by these observations, we focus on investigating targeted attacks in FSR\nand propose a novel dualview attack framework, named DV-FSR. This attack method\nuniquely combines a sampling-based explicit strategy with a contrastive\nlearning-based implicit gradient strategy to orchestrate a coordinated attack.\nAdditionally, we introduce a specific defense mechanism tailored for targeted\nattacks in FSR, aiming to evaluate the mitigation effects of the attack method\nwe proposed. Extensive experiments validate the effectiveness of our proposed\napproach on representative sequential models.\n","authors":["Qitao Qin","Yucong Luo","Mingyue Cheng","Qingyang Mao","Chenyi Lei"],"pdf_url":"https://arxiv.org/pdf/2409.07500v2.pdf","comment":"I am requesting the withdrawal of my paper due to identified errors\n  that require significant revision"},{"id":"http://arxiv.org/abs/2412.20798v1","updated":"2024-12-30T08:43:28Z","published":"2024-12-30T08:43:28Z","title":"A Tale of Two Imperatives: Privacy and Explainability","summary":"  Deep learning's preponderance across scientific domains has reshaped\nhigh-stakes decision-making, making it essential to follow rigorous operational\nframeworks that include both Right-to-Privacy (RTP) and Right-to-Explanation\n(RTE). This paper examines the complexities of combining these two\nrequirements. For RTP, we focus on 'Differentially privacy' (DP), which is\nconsidered the current gold standard for privacy-preserving machine learning\ndue to its strong quantitative guarantee of privacy. For RTE, we focus on\npost-hoc explainers: they are the go-to option for model auditing as they\noperate independently of model training. We formally investigate (DP) models\nand various commonly-used post-hoc explainers: how to evaluate these explainers\nsubject to RTP, and analyze the intrinsic interactions between DP models and\nthese explainers. Furthermore, our work throws light on how RTP and RTE can be\neffectively combined in high-stakes applications. Our study concludes by\noutlining an industrial software pipeline, with the example of a wildly used\nuse-case, that respects both RTP and RTE requirements.\n","authors":["Supriya Manna","Niladri Sett"],"pdf_url":"https://arxiv.org/pdf/2412.20798v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2412.07687v2","updated":"2024-12-30T08:29:09Z","published":"2024-12-10T17:20:47Z","title":"Privacy-Preserving Customer Support: A Framework for Secure and Scalable\n  Interactions","summary":"  The growing reliance on artificial intelligence (AI) in customer support has\nsignificantly improved operational efficiency and user experience. However,\ntraditional machine learning (ML) approaches, which require extensive local\ntraining on sensitive datasets, pose substantial privacy risks and compliance\nchallenges with regulations like the General Data Protection Regulation (GDPR)\nand California Consumer Privacy Act (CCPA). Existing privacy-preserving\ntechniques, such as anonymization, differential privacy, and federated\nlearning, address some concerns but face limitations in utility, scalability,\nand complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning\n(PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in\na zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates\nthe need for local training on sensitive data by utilizing pre-trained LLMs to\ngenerate responses directly. The framework incorporates real-time data\nanonymization to redact or mask sensitive information, retrieval-augmented\ngeneration (RAG) for domain-specific query resolution, and robust\npost-processing to ensure compliance with regulatory standards. This\ncombination reduces privacy risks, simplifies compliance, and enhances\nscalability and operational efficiency. Empirical analysis demonstrates that\nthe PP-ZSL framework provides accurate, privacy-compliant responses while\nsignificantly lowering the costs and complexities of deploying AI-driven\ncustomer support systems. The study highlights potential applications across\nindustries, including financial services, healthcare, e-commerce, legal\nsupport, telecommunications, and government services. By addressing the dual\nchallenges of privacy and performance, this framework establishes a foundation\nfor secure, efficient, and regulatory-compliant AI applications in customer\ninteractions.\n","authors":["Anant Prakash Awasthi","Girdhar Gopal Agarwal","Chandraketu Singh","Rakshit Varma","Sanchit Sharma"],"pdf_url":"https://arxiv.org/pdf/2412.07687v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20787v1","updated":"2024-12-30T08:11:54Z","published":"2024-12-30T08:11:54Z","title":"SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity","summary":"  Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs.Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link.\n","authors":["Pengfei Jing","Mengyun Tang","Xiaorong Shi","Xing Zheng","Sen Nie","Shi Wu","Yong Yang","Xiapu Luo"],"pdf_url":"https://arxiv.org/pdf/2412.20787v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20762v1","updated":"2024-12-30T07:15:21Z","published":"2024-12-30T07:15:21Z","title":"Enhancing Privacy in Federated Learning through Quantum Teleportation\n  Integration","summary":"  Federated learning enables collaborative model training across multiple\nclients without sharing raw data, thereby enhancing privacy. However, the\nexchange of model updates can still expose sensitive information. Quantum\nteleportation, a process that transfers quantum states between distant\nlocations without physical transmission of the particles themselves, has\nrecently been implemented in real-world networks. This position paper explores\nthe potential of integrating quantum teleportation into federated learning\nframeworks to bolster privacy. By leveraging quantum entanglement and the\nno-cloning theorem, quantum teleportation ensures that data remains secure\nduring transmission, as any eavesdropping attempt would be detectable. We\npropose a novel architecture where quantum teleportation facilitates the secure\nexchange of model parameters and gradients among clients and servers. This\nintegration aims to mitigate risks associated with data leakage and adversarial\nattacks inherent in classical federated learning setups. We also discuss the\npractical challenges of implementing such a system, including the current\nlimitations of quantum network infrastructure and the need for hybrid\nquantum-classical protocols. Our analysis suggests that, despite these\nchallenges, the convergence of quantum communication technologies and federated\nlearning presents a promising avenue for achieving unprecedented levels of\nprivacy in distributed machine learning.\n","authors":["Koffka Khan"],"pdf_url":"https://arxiv.org/pdf/2412.20762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20740v1","updated":"2024-12-30T06:32:10Z","published":"2024-12-30T06:32:10Z","title":"Similar but Patched Code Considered Harmful -- The Impact of Similar but\n  Patched Code on Recurring Vulnerability Detection and How to Remove Them","summary":"  Identifying recurring vulnerabilities is crucial for ensuring software\nsecurity. Clone-based techniques, while widely used, often generate many false\nalarms due to the existence of similar but patched (SBP) code, which is similar\nto vulnerable code but is not vulnerable due to having been patched. Although\nthe SBP code poses a great challenge to the effectiveness of existing\napproaches, it has not yet been well explored.\n  In this paper, we propose a programming language agnostic framework, Fixed\nVulnerability Filter (FVF), to identify and filter such SBP instances in\nvulnerability detection. Different from existing studies that leverage function\nsignatures, our approach analyzes code change histories to precisely pinpoint\nSBPs and consequently reduce false alarms. Evaluation under practical scenarios\nconfirms the effectiveness and precision of our approach. Remarkably, FVF\nidentifies and filters 65.1% of false alarms from four vulnerability detection\ntools (i.e., ReDeBug, VUDDY, MVP, and an elementary hash-based approach)\nwithout yielding false positives.\n  We further apply FVF to 1,081 real-world software projects and construct a\nreal-world SBP dataset containing 6,827 SBP functions. Due to the SBP nature,\nthe dataset can act as a strict benchmark to test the sensitivity of the\nvulnerability detection approach in distinguishing real vulnerabilities and\nSBPs. Using this dataset, we demonstrate the ineffectiveness of four\nstate-of-the-art deep learning-based vulnerability detection approaches. Our\ndataset can help developers make a more realistic evaluation of vulnerability\ndetection approaches and also paves the way for further exploration of\nreal-world SBP scenarios.\n","authors":["Zixuan Tan","Jiayuan Zhou","Xing Hu","Shengyi Pan","Kui Liu","Xin Xia"],"pdf_url":"https://arxiv.org/pdf/2412.20740v1.pdf","comment":"Accepted by 47th IEEE/ACM International Conference on Software\n  Engineering (ICSE 2025)"},{"id":"http://arxiv.org/abs/2303.16307v3","updated":"2024-12-30T04:02:48Z","published":"2023-03-28T21:01:17Z","title":"Quantitative Measurement of Cyber Resilience: Modeling and\n  Experimentation","summary":"  Cyber resilience is the ability of a system to resist and recover from a\ncyber attack, thereby restoring the system's functionality. Effective design\nand development of a cyber resilient system requires experimental methods and\ntools for quantitative measuring of cyber resilience. This paper describes an\nexperimental method and test bed for obtaining resilience-relevant data as a\nsystem (in our case -- a truck) traverses its route, in repeatable, systematic\nexperiments. We model a truck equipped with an autonomous cyber-defense system\nand which also includes inherent physical resilience features. When attacked by\nmalware, this ensemble of cyber-physical features (i.e., \"bonware\") strives to\nresist and recover from the performance degradation caused by the malware's\nattack. We propose parsimonious mathematical models to aid in quantifying\nsystems' resilience to cyber attacks. Using the models, we identify\nquantitative characteristics obtainable from experimental data, and show that\nthese characteristics can serve as useful quantitative measures of cyber\nresilience.\n","authors":["Michael J. Weisman","Alexander Kott","Jason E. Ellis","Brian J. Murphy","Travis W. Parker","Sidney Smith","Joachim Vandekerckhove"],"pdf_url":"https://arxiv.org/pdf/2303.16307v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2302.04413,\n  arXiv:2302.07941"},{"id":"http://arxiv.org/abs/2412.20674v1","updated":"2024-12-30T02:58:18Z","published":"2024-12-30T02:58:18Z","title":"Blockchain-Empowered Cyber-Secure Federated Learning for Trustworthy\n  Edge Computing","summary":"  Federated Learning (FL) is a privacy-preserving distributed machine learning\nscheme, where each participant data remains on the participating devices and\nonly the local model generated utilizing the local computational power is\ntransmitted throughout the database. However, the distributed computational\nnature of FL creates the necessity to develop a mechanism that can remotely\ntrigger any network agents, track their activities, and prevent threats to the\noverall process posed by malicious participants. Particularly, the FL paradigm\nmay become vulnerable due to an active attack from the network participants,\ncalled a poisonous attack. In such an attack, the malicious participant acts as\na benign agent capable of affecting the global model quality by uploading an\nobfuscated poisoned local model update to the server. This paper presents a\ncross-device FL model that ensures trustworthiness, fairness, and authenticity\nin the underlying FL training process. We leverage trustworthiness by\nconstructing a reputation-based trust model based on contributions of agents\ntoward model convergence. We ensure fairness by identifying and removing\nmalicious agents from the training process through an outlier detection\ntechnique. Further, we establish authenticity by generating a token for each\nparticipating device through a distributed sensing mechanism and storing that\nunique token in a blockchain smart contract. Further, we insert the trust\nscores of all agents into a blockchain and validate their reputations using\nvarious consensus mechanisms that consider the computational task.\n","authors":["Ervin Moore","Ahmed Imteaj","Md Zarif Hossain","Shabnam Rezapour","M. Hadi Amini"],"pdf_url":"https://arxiv.org/pdf/2412.20674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02059v3","updated":"2024-12-30T01:42:45Z","published":"2023-10-03T14:01:28Z","title":"Security Weaknesses of Copilot-Generated Code in GitHub Projects: An\n  Empirical Study","summary":"  Modern code generation tools utilizing AI models like Large Language Models\n(LLMs) have gained increased popularity due to their ability to produce\nfunctional code. However, their usage presents security challenges, often\nresulting in insecure code merging into the code base. Thus, evaluating the\nquality of generated code, especially its security, is crucial. While prior\nresearch explored various aspects of code generation, the focus on security has\nbeen limited, mostly examining code produced in controlled environments rather\nthan open source development scenarios. To address this gap, we conducted an\nempirical study, analyzing code snippets generated by GitHub Copilot and two\nother AI code generation tools (i.e., CodeWhisperer and Codeium) from GitHub\nprojects. Our analysis identified 733 snippets, revealing a high likelihood of\nsecurity weaknesses, with 29.5% of Python and 24.2% of JavaScript snippets\naffected. These issues span 43 Common Weakness Enumeration (CWE) categories,\nincluding significant ones like CWE-330: Use of Insufficiently Random Values,\nCWE-94: Improper Control of Generation of Code, and CWE-79: Cross-site\nScripting. Notably, eight of those CWEs are among the 2023 CWE Top-25,\nhighlighting their severity. We further examined using Copilot Chat to fix\nsecurity issues in Copilot-generated code by providing Copilot Chat with\nwarning messages from the static analysis tools, and up to 55.5% of the\nsecurity issues can be fixed. We finally provide the suggestions for mitigating\nsecurity issues in generated code.\n","authors":["Yujia Fu","Peng Liang","Amjed Tahir","Zengyang Li","Mojtaba Shahin","Jiaxin Yu","Jinfu Chen"],"pdf_url":"https://arxiv.org/pdf/2310.02059v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20641v1","updated":"2024-12-30T01:10:10Z","published":"2024-12-30T01:10:10Z","title":"SafeSynthDP: Leveraging Large Language Models for Privacy-Preserving\n  Synthetic Data Generation Using Differential Privacy","summary":"  Machine learning (ML) models frequently rely on training data that may\ninclude sensitive or personal information, raising substantial privacy\nconcerns. Legislative frameworks such as the General Data Protection Regulation\n(GDPR) and the California Consumer Privacy Act (CCPA) have necessitated the\ndevelopment of strategies that preserve privacy while maintaining the utility\nof data. In this paper, we investigate the capability of Large Language Models\n(LLMs) to generate synthetic datasets integrated with Differential Privacy (DP)\nmechanisms, thereby enabling data-driven research and model training without\ndirect exposure of sensitive information. Our approach incorporates DP-based\nnoise injection methods, including Laplace and Gaussian distributions, into the\ndata generation process. We then evaluate the utility of these DP-enhanced\nsynthetic datasets by comparing the performance of ML models trained on them\nagainst models trained on the original data. To substantiate privacy\nguarantees, we assess the resilience of the generated synthetic data to\nmembership inference attacks and related threats. The experimental results\ndemonstrate that integrating DP within LLM-driven synthetic data generation\noffers a viable balance between privacy protection and data utility. This study\nprovides a foundational methodology and insight into the privacy-preserving\ncapabilities of LLMs, paving the way for compliant and effective ML research\nand applications.\n","authors":["Md Mahadi Hasan Nahid","Sadid Bin Hasan"],"pdf_url":"https://arxiv.org/pdf/2412.20641v1.pdf","comment":"15 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2406.17548v2","updated":"2024-12-30T22:39:49Z","published":"2024-06-25T13:36:53Z","title":"Laminator: Verifiable ML Property Cards using Hardware-assisted\n  Attestations","summary":"  Regulations increasingly call for various assurances from machine learning\n(ML) model providers about their training data, training process, and model\nbehavior. For better transparency, industry (e.g., Huggingface and Google) has\nadopted model cards and datasheets to describe various properties of training\ndatasets and models. In the same vein, we introduce the notion of inference\ncards to describe the properties of a given inference (e.g., binding of the\noutput to the model and its corresponding input). We coin the term ML property\ncards to collectively refer to these various types of cards.\n  To prevent a malicious model provider from including false information in ML\nproperty cards, they need to be verifiable. We show how to construct verifiable\nML property cards using property attestation, technical mechanisms by which a\nprover (e.g., a model provider) can attest to various ML properties to a\nverifier (e.g., an auditor). Since prior attestation mechanisms based purely on\ncryptography are often narrowly focused (lacking versatility) and inefficient,\nwe need an efficient mechanism to attest different types of properties across\nthe entire ML model pipeline.\n  Emerging widespread support for confidential computing has made it possible\nto run and even train models inside hardware-assisted trusted execution\nenvironments (TEEs), which provide highly efficient attestation mechanisms. We\npropose Laminator, which uses TEEs to provide the first framework for\nverifiable ML property cards via hardware-assisted ML property attestations.\nLaminator is efficient in terms of overhead, scalable to large numbers of\nverifiers, and versatile with respect to the properties it can prove during\ntraining or inference.\n","authors":["Vasisht Duddu","Oskari J√§rvinen","Lachlan J Gunn","N Asokan"],"pdf_url":"https://arxiv.org/pdf/2406.17548v2.pdf","comment":null}],"Operating Systems":[{"id":"http://arxiv.org/abs/2408.08440v2","updated":"2024-12-30T06:00:31Z","published":"2024-08-15T22:13:11Z","title":"Timing Analysis and Priority-driven Enhancements of ROS 2 Multi-threaded\n  Executors","summary":"  The second generation of Robotic Operating System, ROS 2, has gained much\nattention for its potential to be used for safety-critical robotic\napplications. The need to provide a solid foundation for timing correctness and\nscheduling mechanisms is therefore growing rapidly. Although there are some\npioneering studies conducted on formally analyzing the response time of\nprocessing chains in ROS 2, the focus has been limited to single-threaded\nexecutors, and multi-threaded executors, despite their advantages, have not\nbeen studied well. To fill this knowledge gap, in this paper, we propose a\ncomprehensive response-time analysis framework for chains running on ROS 2\nmulti-threaded executors. We first analyze the timing behavior of the default\nscheduling scheme in ROS 2 multi-threaded executors, and then present\npriority-driven scheduling enhancements to address the limitations of the\ndefault scheme. Our framework can analyze chains with both arbitrary and\nconstrained deadlines and also the effect of mutually-exclusive callback\ngroups. Evaluation is conducted by a case study on NVIDIA Jetson AGX Xavier and\nschedulability experiments using randomly-generated chains. The results\ndemonstrate that our analysis framework can safely upper-bound response times\nunder various conditions and the priority-driven scheduling enhancements not\nonly reduce the response time of critical chains but also improve analytical\nbounds.\n","authors":["Hoora Sobhani","Hyunjong Choi","Hyoseung Kim"],"pdf_url":"https://arxiv.org/pdf/2408.08440v2.pdf","comment":null}],"Information Theory":[{"id":"http://arxiv.org/abs/2412.19792v2","updated":"2024-12-30T09:37:33Z","published":"2024-12-27T18:45:36Z","title":"InfAlign: Inference-aware language model alignment","summary":"  Language model alignment has become a critical step in training modern\ngenerative language models. The goal of alignment is to finetune a reference\nmodel such that the win rate of a sample from the aligned model over a sample\nfrom the reference model is high, subject to a KL divergence constraint. Today,\nwe are increasingly using inference-time algorithms (e.g., Best-of-N,\ncontrolled decoding, tree search) to decode from language models rather than\nstandard sampling. However, the alignment objective does not capture such\ninference-time decoding procedures. We show that the existing alignment\nframework is sub-optimal in view of such inference-time methods. We then modify\nthe alignment objective and propose a framework for inference-aware alignment\n(IAPO). We prove that for any inference-time decoding algorithm, the optimal\nsolution that optimizes the inference-time win rate of the aligned policy\nagainst the reference policy is the solution to the typical RLHF problem with a\ntransformation of the reward. This motivates us to provide the KL-regularized\ncalibrate-and-transform RL (CTRL) algorithm to solve this problem, which\ninvolves a reward calibration step and a KL-regularized reward maximization\nstep with a transformation of the calibrated reward. We particularize our study\nto two important inference-time strategies: best-of-N sampling and best-of-N\njailbreaking, where N responses are sampled from the model and the one with the\nhighest or lowest reward is selected. We propose specific transformations for\nthese strategies and demonstrate that our framework offers significant\nimprovements over existing state-of-the-art methods for language model\nalignment. Empirically, we outperform baselines that are designed without\ntaking inference-time decoding into consideration by 8-12% and 4-9% on\ninference-time win rates over the Anthropic helpfulness and harmlessness dialog\nbenchmark datasets.\n","authors":["Ananth Balashankar","Ziteng Sun","Jonathan Berant","Jacob Eisenstein","Michael Collins","Adrian Hutter","Jong Lee","Chirag Nagpal","Flavien Prost","Aradhana Sinha","Ananda Theertha Suresh","Ahmad Beirami"],"pdf_url":"https://arxiv.org/pdf/2412.19792v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21200v1","updated":"2024-12-30T18:59:06Z","published":"2024-12-30T18:59:06Z","title":"Distributed Mixture-of-Agents for Edge Inference with Large Language\n  Models","summary":"  Mixture-of-Agents (MoA) has recently been proposed as a method to enhance\nperformance of large language models (LLMs), enabling multiple individual LLMs\nto work together for collaborative inference. This collaborative approach\nresults in improved responses to user prompts compared to relying on a single\nLLM. In this paper, we consider such an MoA architecture in a distributed\nsetting, where LLMs operate on individual edge devices, each uniquely\nassociated with a user and equipped with its own distributed computing power.\nThese devices exchange information using decentralized gossip algorithms,\nallowing different device nodes to talk without the supervision of a\ncentralized server. In the considered setup, different users have their own LLM\nmodels to address user prompts. Additionally, the devices gossip either their\nown user-specific prompts or augmented prompts to generate more refined answers\nto certain queries. User prompts are temporarily stored in the device queues\nwhen their corresponding LLMs are busy. Given the memory limitations of edge\ndevices, it is crucial to ensure that the average queue sizes in the system\nremain bounded. In this paper, we address this by theoretically calculating the\nqueuing stability conditions for the device queues under reasonable\nassumptions, which we validate experimentally as well. Further, we demonstrate\nthrough experiments, leveraging open-source LLMs for the implementation of\ndistributed MoA, that certain MoA configurations produce higher-quality\nresponses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The\nimplementation is available at:\nhttps://github.com/purbeshmitra/distributed_moa.\n","authors":["Purbesh Mitra","Priyanka Kaswan","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2412.21200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21181v1","updated":"2024-12-30T18:52:48Z","published":"2024-12-30T18:52:48Z","title":"Causal Hangover Effects","summary":"  It's not unreasonable to think that in-game sporting performance can be\naffected partly by what takes place off the court. We can't observe what\nhappens between games directly. Instead, we proxy for the possibility of\nathletes partying by looking at play following games in party cities. We are\ninterested to see if teams exhibit a decline in performance the day following a\ngame in a city with active nightlife; we call this a \"hangover effect\". Part of\nthe question is determining a reasonable way to measure levels of nightlife,\nand correspondingly which cities are notorious for it; we colloquially refer to\nsuch cities as \"party cities\". To carry out this study, we exploit data on\nbookmaker spreads: the expected score differential between two teams after\nconditioning on observable performance in past games and expectations about the\nupcoming game. We expect a team to meet the spread half the time, since this is\none of the easiest ways for bookmakers to guarantee a profit. We construct a\nmodel which attempts to estimate the causal effect of visiting a \"party city\"\non subsequent day performance as measured by the odds of beating the spread. In\nparticular, we only consider the hangover effect on games played back-to-back\nwithin 24 hours of each other. To the extent that odds of beating the spread\nagainst next day opponent is uncorrelated with playing in a party city the day\nbefore, which should be the case under an efficient betting market, we have\nidentification in our variable of interest. We find that visiting a city with\nactive nightlife the day prior to a game does have a statistically significant\nnegative effect on a team's likelihood of meeting bookmakers' expectations for\nboth NBA and MLB.\n","authors":["Andreas Santucci","Eric Lax"],"pdf_url":"https://arxiv.org/pdf/2412.21181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21171v1","updated":"2024-12-30T18:48:54Z","published":"2024-12-30T18:48:54Z","title":"Quantum Error Correction near the Coding Theoretical Bound","summary":"  Recent advancements in quantum computing have led to the realization of\nsystems comprising tens of reliable logical qubits, constructed from thousands\nof noisy physical qubits. However, many of the critical applications that\nquantum computers aim to solve require quantum computations involving millions\nor more logical qubits. This necessitates highly efficient quantum error\ncorrection capable of handling large numbers of logical qubits. Classical error\ncorrection theory is well-developed, with low-density parity-check (LDPC) codes\nachieving performance limits by encoding large classical bits. Despite more\nthan two decades of effort, no efficiently decodable quantum error-correcting\ncode that approaches the hashing bound, which is a fundamental lower bound on\nquantum capacity, had been discovered. Here, we present quantum\nerror-correcting codes constructed from classical LDPC codes that approach the\nhashing bound while maintaining linear computational complexity in the number\nof physical qubits. This result establishes a pathway toward realizing\nlarge-scale, fault-tolerant quantum computers. By integrating our quantum error\ncorrection scheme with devices capable of managing vast numbers of qubits, the\nprospect of solving critical real-world problems through quantum computation is\nbrought significantly closer.\n","authors":["Daiki Komoto","Kenta Kasai"],"pdf_url":"https://arxiv.org/pdf/2412.21171v1.pdf","comment":"This work has been submitted to a journal for possible publication"},{"id":"http://arxiv.org/abs/2412.21118v1","updated":"2024-12-30T17:45:08Z","published":"2024-12-30T17:45:08Z","title":"Efficient Approximate Degenerate Ordered Statistics Decoding for Quantum\n  Codes via Reliable Subset Reduction","summary":"  Efficient decoding of quantum codes is crucial for achieving high-performance\nquantum error correction. In this paper, we introduce the concept of\napproximate degenerate decoding and integrate it with ordered statistics\ndecoding (OSD). Previously, we proposed a reliability metric that leverages\nboth hard and soft decisions from the output of belief propagation (BP), which\nis particularly useful for identifying highly reliable subsets of variables.\nUsing the approach of reliable subset reduction, we reduce the effective\nproblem size. Additionally, we identify a degeneracy condition that allows\nhigh-order OSD to be simplified to order-0 OSD. By integrating these\ntechniques, we present an ADOSD algorithm that significantly improves OSD\nefficiency in the code capacity noise model. We demonstrate the effectiveness\nof our BP+ADOSD approach through extensive simulations on a varity of quantum\ncodes, including generalized hypergraph-product codes, topological codes,\nlift-connected surface codes, and bivariate bicycle codes. The results indicate\nthat the BP+ADOSD decoder outperforms existing methods, achieving higher error\nthresholds and enhanced performance at low error rates. Additionally, we\nvalidate the efficiency of our approach in terms of computational time,\ndemonstrating that ADOSD requires, on average, the same amount of time as two\nto three BP iterations on surface codes at a depolarizing error rate of around\n$1\\%$. All the proposed algorithms are compared using single-threaded CPU\nimplementations.\n","authors":["Ching-Feng Kung","Kao-Yueh Kuo","Ching-Yi Lai"],"pdf_url":"https://arxiv.org/pdf/2412.21118v1.pdf","comment":"16 pages, 10 figures, 6 tables"},{"id":"http://arxiv.org/abs/2412.20943v1","updated":"2024-12-30T13:36:36Z","published":"2024-12-30T13:36:36Z","title":"Cluster-Based Time-Variant Channel Characterization and Modeling for\n  5G-Railways","summary":"  With the development of high-speed railways, 5G for Railways (5G-R) is\ngradually replacing Global System for the Mobile Communications for Railway\n(GSM-R) worldwide to meet increasing demands. The large bandwidth, array\nantennas, and non-stationarity caused by high mobility has made 5G-R channel\ncharacterization more complex. Therefore, it is essential to develop an\naccurate channel model for 5G-R. However, researches on channel\ncharacterization and time-variant models specific to 5G-R frequency bands and\nscenarios is scarce. There are virtually no cluster-based time-variant channel\nmodels that capture statistical properties of 5G-R channel. In this paper, we\npropose a cluster-based time-variant channel model for 5G-R within an enhanced\n3GPP framework, which incorporates time evolution features. Extensive channel\nmeasurements are conducted on 5G-R private network test line in China. We then\nextract and analyze typical channel fading characteristics and multipath\ncluster characteristics. Furthermore, birth-death process of the clusters is\nmodeled by using a four-state Markov chain. Finally, a generalized clustered\ndelay line (CDL) model is established in accordance with 3GPP standard and\nvalidated by comparing the results of measurements and simulations. This work\nenhances the understanding of 5G-R channels and presents a flexible\ncluster-based time-variant channel model. The results can be used in the\ndesign, deployment, and optimization of 5G-R networks.\n","authors":["Xuejian Zhang","Ruisi He","Bo Ai","Mi Yang","Jianwen Ding","Shuaiqi Gao","Ziyi Qi","Zhengyu Zhang","Zhangdui Zhong"],"pdf_url":"https://arxiv.org/pdf/2412.20943v1.pdf","comment":"13 pages, 13 figures, submitted to IEEE Transactions on Wireless\n  Communications"},{"id":"http://arxiv.org/abs/2412.20934v1","updated":"2024-12-30T13:23:11Z","published":"2024-12-30T13:23:11Z","title":"Optimal Diffusion Processes","summary":"  Of stochastic differential equations, diffusion processes have been adopted\nin numerous applications, as more relevant and flexible models. This paper\nstudies diffusion processes in a different setting, where for a given\nstationary distribution and average variance, it seeks the diffusion process\nwith optimal convergence rate. It is shown that the optimal drift function is a\nlinear function and the convergence rate of the stochastic process is bounded\nby the ratio of the average variance to the variance of the stationary\ndistribution. Furthermore, the concavity of the optimal relaxation time as a\nfunction of the stationary distribution has been proven, and it is shown that\nall Pearson diffusion processes of the Hypergeometric type with polynomial\nfunctions of at most degree two as the variance functions are optimal.\n","authors":["Saber Jafarizadeh"],"pdf_url":"https://arxiv.org/pdf/2412.20934v1.pdf","comment":"15 pages, 0 figure, 1 table"},{"id":"http://arxiv.org/abs/2412.20920v1","updated":"2024-12-30T13:01:33Z","published":"2024-12-30T13:01:33Z","title":"Channel Charting-assisted Non-orthogonal Pilot Allocation for Uplink\n  XL-MIMO Transmission","summary":"  Extremely large-scale multiple-input multiple-output (XL-MIMO) is critical to\nfuture wireless networks. The substantial increase in the number of base\nstation (BS) antennas introduces near-field propagation effects in the wireless\nchannels, complicating channel parameter estimation and increasing pilot\noverhead. Channel charting (CC) has emerged as a potent unsupervised technique\nto effectively harness varying high-dimensional channel statistics to enable\nnon-orthogonal pilot assignment and reduce pilot overhead. In this paper, we\ninvestigate near-field channel estimation with reduced pilot overhead by\ndeveloping a CC-assisted pilot scheduling. To this end, we introduce a\npolar-domain codebook to capture the power distribution of near-field XL-MIMO\nchannels. The CC-assisted approach uses such features as inputs to enable an\neffective low-dimensional mapping of the inherent correlation patterns in\nnear-field user terminal (UT) channels. Building upon the mapped channel\ncorrelations, we further propose a near-field CC-assisted pilot allocation\n(NCC-PA) algorithm, which efficiently enhances channel orthogonality among\npilot-reusing UTs. Numerical results confirm that the NCC-PA algorithm\nsubstantially elevates the wireless transmission performance, offering a marked\nimprovement over the conventional far-field CC-PA approach.\n","authors":["Haohong Che","Li You","Jue Wang","Zhenzhou Jin","Chenjie Xie","Xiqi Gao"],"pdf_url":"https://arxiv.org/pdf/2412.20920v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.20885v1","updated":"2024-12-30T11:52:39Z","published":"2024-12-30T11:52:39Z","title":"CF-CGN: Channel Fingerprints Extrapolation for Multi-band Massive MIMO\n  Transmission based on Cycle-Consistent Generative Networks","summary":"  Multi-band massive multiple-input multiple-output (MIMO) communication can\npromote the cooperation of licensed and unlicensed spectra, effectively\nenhancing spectrum efficiency for Wi-Fi and other wireless systems. As an\nenabler for multi-band transmission, channel fingerprints (CF), also known as\nthe channel knowledge map or radio environment map, are used to assist channel\nstate information (CSI) acquisition and reduce computational complexity. In\nthis paper, we propose CF-CGN (Channel Fingerprints with Cycle-consistent\nGenerative Networks) to extrapolate CF for multi-band massive MIMO transmission\nwhere licensed and unlicensed spectra cooperate to provide ubiquitous\nconnectivity. Specifically, we first model CF as a multichannel image and\ntransform the extrapolation problem into an image translation task, which\nconverts CF from one frequency to another by exploring the shared\ncharacteristics of statistical CSI in the beam domain. Then, paired generative\nnetworks are designed and coupled by variable-weight cycle consistency losses\nto fit the reciprocal relationship at different bands. Matched with the coupled\nnetworks, a joint training strategy is developed accordingly, supporting\nsynchronous optimization of all trainable parameters. During the inference\nprocess, we also introduce a refining scheme to improve the extrapolation\naccuracy based on the resolution of CF. Numerical results illustrate that our\nproposed CF-CGN can achieve bidirectional extrapolation with an error of 5-17\ndB lower than the benchmarks in different communication scenarios,\ndemonstrating its excellent generalization ability. We further show that the\nsum rate performance assisted by CF-CGN-based CF is close to that with perfect\nCSI for multi-band massive MIMO transmission.\n","authors":["Chenjie Xie","Li You","Zhenzhou Jin","Jinke Tang","Xiqi Gao","Xiang-Gen Xia"],"pdf_url":"https://arxiv.org/pdf/2412.20885v1.pdf","comment":"13 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.04877v2","updated":"2024-12-30T11:12:54Z","published":"2024-12-06T09:19:50Z","title":"Fluid Antenna Index Modulation for MIMO Systems: Robust Transmission and\n  Low-Complexity Detection","summary":"  The fluid antenna (FA) index modulation (IM)-enabled multiple-input\nmultiple-output (MIMO) system, referred to as FA-IM, significantly enhances\nspectral efficiency (SE) compared to the conventional FA-assisted MIMO system.\nTo improve robustness against the high spatial correlation among multiple\nactivated ports of the fluid antenna, this paper proposes an innovative FA\ngrouping-based IM (FAG-IM) system. A block grouping scheme is employed based on\nthe spatial correlation model and the distribution structure of the ports.\nThen, a closed-form expression for the average bit error probability (ABEP)\nupper bound of the FAG-IM system is derived. To reduce the complexity of the\nreceiver, the message passing architecture is incorporated into the FAG-IM\nsystem. Building on this, an efficient approximate message passing (AMP)\ndetector, named structured AMP (S-AMP) detector, is proposed by exploiting the\nstructural characteristics of the transmitted signals. Simulation results\nconfirm that the proposed FAG-IM system significantly outperforms the existing\nFA-IM system in the presence of spatial correlation, achieving more robust\ntransmission. Furthermore, it is demonstrated that the proposed low-complexity\nS-AMP detector not only reduces time complexity to a linear scale but also\nsubstantially improves bit error rate (BER) performance compared to the minimum\nmean square error (MMSE) detector, thereby enhancing the practical feasibility\nof the FAG-IM system.\n","authors":["Xinghao Guo","Yin Xu","Dazhi He","Cixiao Zhang","Hanjiang Hong","Kai-Kit Wong","Wenjun Zhang","Yiyan Wu"],"pdf_url":"https://arxiv.org/pdf/2412.04877v2.pdf","comment":"Submitted to an IEEE journal"},{"id":"http://arxiv.org/abs/2412.20828v1","updated":"2024-12-30T09:51:40Z","published":"2024-12-30T09:51:40Z","title":"Effective Application of Normalized Min-Sum Decoding for BCH Codes","summary":"  High-throughput decoding of BCH codes necessitates efficient and\nparallelizable decoders. However, the algebraic rigidity of BCH codes poses\nsignificant challenges to applying parallel belief propagation variants. To\naddress this, we propose a systematic design scheme for constructing\nparity-check matrices using a heuristic approach. This involves a sequence of\nbinary sum operations and row cyclic shifts on the standard parity-check\nmatrix, aiming to generate a redundant, low-density, and quasi-regular matrix\nwith significantly fewer length-4 cycles. The relationships between frame error\nrate, rank deficiency of minimum-weight dual-code codewords, and row redundancy\nare empirically analyzed. For the revised normalized min-sum decoder, we\nintroduce three types of random automorphisms applied to decoder inputs. These\nare unpacked and aggregated by summing messages after each iteration, achieving\na 1-2dB improvement in bit error rate compared to parallelizable counterparts\nand two orders of magnitude faster convergence in iterations than iterative\nrivals. Additionally, undetected errors are highlighted as a non-negligible\nissue for very short BCH codes.\n","authors":["Guangwen Li","Xiao Yu"],"pdf_url":"https://arxiv.org/pdf/2412.20828v1.pdf","comment":"5 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.20806v1","updated":"2024-12-30T09:01:07Z","published":"2024-12-30T09:01:07Z","title":"LEO Satellite-Enabled Random Access with Large Differential Delay and\n  Doppler Shift","summary":"  This paper investigates joint device identification, channel estimation, and\nsymbol detection for LEO satellite-enabled grant-free random access systems,\nspecifically targeting scenarios where remote Internet-of-Things (IoT) devices\noperate without global navigation satellite system (GNSS) assistance.\nConsidering the constrained power consumption of these devices, the large\ndifferential delay and Doppler shift are handled at the satellite receiver. We\nfirstly propose a spreading-based multi-frame transmission scheme with\northogonal time-frequency space (OTFS) modulation to mitigate the doubly\ndispersive effect in time and frequency, and then analyze the input-output\nrelationship of the system. Next, we propose a receiver structure based on\nthree modules: a linear module for identifying active devices that leverages\nthe generalized approximate message passing algorithm to eliminate inter-user\nand inter-carrier interference; a non-linear module that employs the message\npassing algorithm to jointly estimate the channel and detect the transmitted\nsymbols; and a third module that aims to exploit the three dimensional block\nchannel sparsity in the delay-Doppler-angle domain. Soft information is\nexchanged among the three modules by careful message scheduling. Furthermore,\nthe expectation-maximization algorithm is integrated to adjust phase rotation\ncaused by the fractional Doppler and to learn the hyperparameters in the\npriors. Finally, the convolutional neural network is incorporated to enhance\nthe symbol detection. Simulation results demonstrate that the proposed\ntransmission scheme boosts the system performance, and the designed algorithms\noutperform the conventional methods significantly in terms of the device\nidentification, channel estimation, and symbol detection.\n","authors":["Boxiao Shen","Yongpeng Wu","Wenjun Zhang","Symeon Chatzinotas","Bj√∂rn Ottersten"],"pdf_url":"https://arxiv.org/pdf/2412.20806v1.pdf","comment":"This paper has been accepted by the IEEE Transactions on Wireless\n  Communications"},{"id":"http://arxiv.org/abs/2412.20772v1","updated":"2024-12-30T07:47:30Z","published":"2024-12-30T07:47:30Z","title":"Large Language Model Enabled Multi-Task Physical Layer Network","summary":"  The recent advance of Artificial Intelligence (AI) is continuously reshaping\nthe future 6G wireless communications. Recently, the development of Large\nLanguage Models (LLMs) offers a promising approach to effectively improve the\nperformance and generalization for different physical layer tasks. However,\nmost existing works finetune dedicated LLM networks for a single wireless\ncommunication task separately. Thus performing diverse physical layer tasks\nintroduces extremely high training resources, memory usage, and deployment\ncosts. To solve the problem, we propose a LLM-enabled multi-task physical layer\nnetwork to unify multiple tasks with a single LLM. Specifically, we first\npropose a multi-task LLM framework, which finetunes LLM to perform multi-user\nprecoding, signal detection and channel prediction simultaneously. Besides,\nmulti-task instruction module, input encoders, as well as output decoders, are\nelaborately designed to distinguish multiple tasks and adapted the features of\ndifferent formats of wireless data for the features of LLM. Numerical\nsimulations are also displayed to verify the effectiveness of the proposed\nmethod.\n","authors":["Tianyue Zheng","Linglong Dai"],"pdf_url":"https://arxiv.org/pdf/2412.20772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20771v1","updated":"2024-12-30T07:42:24Z","published":"2024-12-30T07:42:24Z","title":"Optimally Decoding Two-Dimensional Reed-Solomon Codes up to the\n  Half-Singleton Bound","summary":"  Constructing Reed-Solomon (RS) codes capable of correcting insertion and\ndeletion errors (ins-del errors) has been the focus of numerous recent studies.\nHowever, the development of efficient decoding algorithms for such RS codes has\nnot garnered significant attention and remains an important and intriguing open\nproblem. In this work, we take a first step toward addressing this problem by\ndesigning an optimal-time decoding algorithm for the special case of\ntwo-dimensional RS codes, capable of decoding up to the half-Singleton bound.\n","authors":["Shubhransh Singhvi"],"pdf_url":"https://arxiv.org/pdf/2412.20771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20726v1","updated":"2024-12-30T05:55:21Z","published":"2024-12-30T05:55:21Z","title":"Beam Codebook Refinement for mmWave Devices with Random Orientations:\n  Concept and Experimental Validation","summary":"  There is a growing interest in codebook-based beam-steering for\nmillimeter-wave (mmWave) systems due to its potential for low complexity and\nrapid beam search. A key focus of recent research has been the design of\ncodebooks that strike a trade-off between achievable gain and codebook size,\nwhich directly impacts beam search time. Statistical approaches have shown\npromise by leveraging the likelihood that certain beam directions\n(equivalently, sets of phase-shifter configurations) are more probable than\nothers. Such approaches are shown to be valid for static, non-rotating\ntransmission stations such as base stations. However, for the case of user\nterminals that are constantly changing orientation, the possible phase-shifter\nconfigurations become equally probable, rendering statistical methods less\nrelevant. On the other hand, user terminals come with a large number of\npossible steering vector configurations, which can span up to six orders of\nmagnitude. Therefore, efficient solutions to reduce the codebook size (set of\npossible steering vectors) without compromising array gain are needed. We\naddress this challenge by proposing a novel and practical codebook refinement\ntechnique, aiming to reduce the codebook size while maintaining array gain\nwithin $\\gamma$ dB of the maximum achievable gain at any random orientation of\nthe user terminal. We project that a steering vector at a given angle could\neffectively cover adjacent angles with a small gain loss compared to the\nmaximum achievable gain. We demonstrate experimentally that it is possible to\nreduce the codebook size from $1024^{16}$ to just a few configurations (e.g.,\nless than ten), covering all angles while maintaining the gain within\n$\\gamma=3$ dB of the maximum achievable gain.\n","authors":["Bora Bozkurt","Ahmet Muaz Aktas","Hasan Atalay Gunel","Mohaned Chraiti","Ali Gorcin","Ibrahim Hokelek"],"pdf_url":"https://arxiv.org/pdf/2412.20726v1.pdf","comment":"6 pages, 3 figures, conference"},{"id":"http://arxiv.org/abs/2412.20634v1","updated":"2024-12-30T00:46:48Z","published":"2024-12-30T00:46:48Z","title":"Graph Neural Networks for Next-Generation-IoT: Recent Advances and Open\n  Challenges","summary":"  Graph Neural Networks (GNNs) have emerged as a critical tool for optimizing\nand managing the complexities of the Internet of Things (IoT) in\nnext-generation networks. This survey presents a comprehensive exploration of\nhow GNNs may be harnessed in 6G IoT environments, focusing on key challenges\nand opportunities through a series of open questions. We commence with an\nexploration of GNN paradigms and the roles of node, edge, and graph-level tasks\nin solving wireless networking problems and highlight GNNs' ability to overcome\nthe limitations of traditional optimization methods. This guidance enhances\nproblem-solving efficiency across various next-generation (NG) IoT scenarios.\nNext, we provide a detailed discussion of the application of GNN in advanced NG\nenabling technologies, including massive MIMO, reconfigurable intelligent\nsurfaces, satellites, THz, mobile edge computing (MEC), and ultra-reliable low\nlatency communication (URLLC). We then delve into the challenges posed by\nadversarial attacks, offering insights into defense mechanisms to secure\nGNN-based NG-IoT networks. Next, we examine how GNNs can be integrated with\nfuture technologies like integrated sensing and communication (ISAC),\nsatellite-air-ground-sea integrated networks (SAGSIN), and quantum computing.\nOur findings highlight the transformative potential of GNNs in improving\nefficiency, scalability, and security within NG-IoT systems, paving the way for\nfuture advances. Finally, we propose a set of design guidelines to facilitate\nthe development of efficient, scalable, and secure GNN models tailored for NG\nIoT applications.\n","authors":["Nguyen Xuan Tung","Le Tung Giang","Bui Duc Son","Seon Geun Jeong","Trinh Van Chien","Won Joo Hwang","Lajos Hanzo"],"pdf_url":"https://arxiv.org/pdf/2412.20634v1.pdf","comment":"28 pages, 15 figures, and 6 tables. Submitted for publication"},{"id":"http://arxiv.org/abs/1307.2747v3","updated":"2024-12-30T04:16:21Z","published":"2013-07-10T10:47:14Z","title":"Impossibility of Local State Transformation via Hypercontractivity","summary":"  Local state transformation is the problem of transforming an arbitrary number\nof copies of a bipartite resource state to a bipartite target state under local\noperations. That is, given two bipartite states, is it possible to transform an\narbitrary number of copies of one of them to one copy of the other state under\nlocal operations only? This problem is a hard one in general since we assume\nthat the number of copies of the resource state is arbitrarily large. In this\npaper we prove some bounds on this problem using the hypercontractivity\nproperties of some super-operators corresponding to bipartite states. We\nmeasure hypercontractivity in terms of both the usual super-operator norms as\nwell as completely bounded norms.\n","authors":["Payam Delgosha","Salman Beigi"],"pdf_url":"https://arxiv.org/pdf/1307.2747v3.pdf","comment":"27 pages, fixed the statement of theorem 16"},{"id":"http://arxiv.org/abs/1504.04449v4","updated":"2024-12-30T03:46:38Z","published":"2015-04-17T06:20:32Z","title":"Decoding quantum information via the Petz recovery map","summary":"  We obtain a lower bound on the maximum number of qubits, $Q^{n,\n\\epsilon}(\\mathcal{N})$, which can be transmitted over $n$ uses of a quantum\nchannel $\\mathcal{N}$, for a given non-zero error threshold $\\epsilon$. To\nobtain our result, we first derive a bound on the one-shot entanglement\ntransmission capacity of the channel, and then compute its asymptotic expansion\nup to the second order. In our method to prove this achievability bound, the\ndecoding map, used by the receiver on the output of the channel, is chosen to\nbe the \\emph{Petz recovery map} (also known as the \\emph{transpose channel}).\nOur result, in particular, shows that this choice of the decoder can be used to\nestablish the coherent information as an achievable rate for quantum\ninformation transmission. Applying our achievability bound to the 50-50 erasure\nchannel (which has zero quantum capacity), we find that there is a sharp error\nthreshold above which $Q^{n, \\epsilon}(\\mathcal{N})$ scales as $\\sqrt{n}$.\n","authors":["Salman Beigi","Nilanjana Datta","Felix Leditzky"],"pdf_url":"https://arxiv.org/pdf/1504.04449v4.pdf","comment":"31 pages, removed section 4 of the previous version which included an\n  incorrect lemma"}],"Signal Processing":[{"id":"http://arxiv.org/abs/2412.19585v2","updated":"2024-12-30T09:51:38Z","published":"2024-12-27T11:03:26Z","title":"Ultralight Signal Classification Model for Automatic Modulation\n  Recognition","summary":"  The growing complexity of radar signals demands responsive and accurate\ndetection systems that can operate efficiently on resource-constrained edge\ndevices. Existing models, while effective, often rely on substantial\ncomputational resources and large datasets, making them impractical for edge\ndeployment. In this work, we propose an ultralight hybrid neural network\noptimized for edge applications, delivering robust performance across\nunfavorable signal-to-noise ratios (mean accuracy of 96.3% at 0 dB) using less\nthan 100 samples per class, and significantly reducing computational overhead.\n","authors":["Alessandro Daniele Genuardi Oquendo","Agust√≠n Mat√≠as Galante Cervi√±o","Nilotpal Kanti Sinha","Luc Andrea","Sam Mugel","Rom√°n Or√∫s"],"pdf_url":"https://arxiv.org/pdf/2412.19585v2.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.17934v2","updated":"2024-12-30T11:14:58Z","published":"2024-12-23T19:45:21Z","title":"UAV Communications: Impact of Obstacles on Channel Characteristics","summary":"  In recent years, Unmanned Aerial Vehicles (UAVs) have been utilized as\neffective platforms for carrying Wi-Fi Access Points (APs) and cellular Base\nStations (BSs), enabling low-cost, agile, and flexible wireless networks with\nhigh Quality of Service (QoS). The next generation of wireless communications\nwill rely on increasingly higher frequencies, which are easily obstructed by\nobstacles. One of the most critical concepts yet to be fully addressed is\npositioning the UAV at optimal coordinates while accounting for obstacles. To\nensure a line of sight (LoS) between UAVs and user equipment (UE), improve QoS,\nand establish reliable wireless links with maximum coverage, obstacles must be\nintegrated into the proposed placement algorithms. This paper introduces a\nsimulation-based measurement approach for characterizing an air-to-ground (AG)\nchannel in a simple scenario. By considering obstacles, we present a novel\nperspective on channel characterization. The results, in terms of throughput,\npacket delivery, packet loss, and delay, are compared using the proposed\npositioning approach.\n","authors":["Kamal Shayegan"],"pdf_url":"https://arxiv.org/pdf/2412.17934v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21178v1","updated":"2024-12-30T18:50:37Z","published":"2024-12-30T18:50:37Z","title":"Two-component spatiotemporal template for activation-inhibition of\n  speech in ECoG","summary":"  I compute the average trial-by-trial power of band-limited speech activity\nacross epochs of multi-channel high-density electrocorticography (ECoG)\nrecorded from multiple subjects during a consonant-vowel speaking task. I show\nthat previously seen anti-correlations of average beta frequency activity\n(12-35 Hz) to high-frequency gamma activity (70-140 Hz) during speech movement\nare observable between individual ECoG channels in the sensorimotor cortex\n(SMC). With this I fit a variance-based model using principal component\nanalysis to the band-powers of individual channels of session-averaged ECoG\ndata in the SMC and project SMC channels onto their lower-dimensional principal\ncomponents.\n  Spatiotemporal relationships between speech-related activity and principal\ncomponents are identified by correlating the principal components of both\nfrequency bands to individual ECoG channels over time using windowed\ncorrelation. Correlations of principal component areas to sensorimotor areas\nreveal a distinct two-component activation-inhibition-like representation for\nspeech that resembles distinct local sensorimotor areas recently shown to have\ncomplex interplay in whole-body motor control, inhibition, and posture. Notably\nthe third principal component shows insignificant correlations across all\nsubjects, suggesting two components of ECoG are sufficient to represent SMC\nactivity during speech movement.\n","authors":["Eric Easthope"],"pdf_url":"https://arxiv.org/pdf/2412.21178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21164v1","updated":"2024-12-30T18:43:21Z","published":"2024-12-30T18:43:21Z","title":"Adversarial Attack and Defense for LoRa Device Identification and\n  Authentication via Deep Learning","summary":"  LoRa provides long-range, energy-efficient communications in Internet of\nThings (IoT) applications that rely on Low-Power Wide-Area Network (LPWAN)\ncapabilities. Despite these merits, concerns persist regarding the security of\nLoRa networks, especially in situations where device identification and\nauthentication are imperative to secure the reliable access to the LoRa\nnetworks. This paper explores a deep learning (DL) approach to tackle these\nconcerns, focusing on two critical tasks, namely (i) identifying LoRa devices\nand (ii) classifying them to legitimate and rogue devices. Deep neural networks\n(DNNs), encompassing both convolutional and feedforward neural networks, are\ntrained for these tasks using actual LoRa signal data. In this setting, the\nadversaries may spoof rogue LoRa signals through the kernel density estimation\n(KDE) method based on legitimate device signals that are received by the\nadversaries. Two cases are considered, (i) training two separate classifiers,\none for each of the two tasks, and (ii) training a multi-task classifier for\nboth tasks. The vulnerabilities of the resulting DNNs to manipulations in input\nsamples are studied in form of untargeted and targeted adversarial attacks\nusing the Fast Gradient Sign Method (FGSM). Individual and common perturbations\nare considered against single-task and multi-task classifiers for the LoRa\nsignal analysis. To provide resilience against such attacks, a defense approach\nis presented by increasing the robustness of classifiers with adversarial\ntraining. Results quantify how vulnerable LoRa signal classification tasks are\nto adversarial attacks and emphasize the need to fortify IoT applications\nagainst these subtle yet effective threats.\n","authors":["Yalin E. Sagduyu","Tugba Erpek"],"pdf_url":"https://arxiv.org/pdf/2412.21164v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21132v1","updated":"2024-12-30T18:08:55Z","published":"2024-12-30T18:08:55Z","title":"DeepF-fNet: a physics-informed neural network for vibration isolation\n  optimization","summary":"  Structural optimization is essential for designing safe, efficient, and\ndurable components with minimal material usage. Traditional methods for\nvibration control often rely on active systems to mitigate unpredictable\nvibrations, which may lead to resonance and potential structural failure.\nHowever, these methods face significant challenges when addressing the\nnonlinear inverse eigenvalue problems required for optimizing structures\nsubjected to a wide range of frequencies. As a result, no existing approach has\neffectively addressed the need for real-time vibration suppression within this\ncontext, particularly in high-performance environments such as automotive\nnoise, vibration and harshness, where computational efficiency is crucial.\n  This study introduces DeepF-fNet, a novel neural network framework designed\nto replace traditional active systems in vibration-based structural\noptimization. Leveraging DeepONets within the context of physics-informed\nneural networks, DeepF-fNet integrates both data and the governing physical\nlaws. This enables rapid identification of optimal parameters to suppress\ncritical vibrations at specific frequencies, offering a more efficient and\nreal-time alternative to conventional methods.\n  The proposed framework is validated through a case study involving a locally\nresonant metamaterial used to isolate structures from user-defined frequency\nranges. The results demonstrate that DeepF-fNet outperforms traditional genetic\nalgorithms in terms of computational speed while achieving comparable results,\nmaking it a promising tool for vibration-sensitive applications. By replacing\nactive systems with machine learning techniques, DeepF-fNet paves the way for\nmore efficient and cost-effective structural optimization in real-world\nscenarios.\n","authors":["A. Tollardo","F. Cadini","M. Giglio","L. Lomazzi"],"pdf_url":"https://arxiv.org/pdf/2412.21132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21002v1","updated":"2024-12-30T15:11:39Z","published":"2024-12-30T15:11:39Z","title":"Sparse Array Sensor Selection in ISAC with Identifiability Guarantees","summary":"  This paper investigates array geometry and waveform design for integrated\nsensing and communications (ISAC) employing sensor selection. We consider ISAC\nvia index modulation, where various subsets of transmit (Tx) sensors are used\nfor both communications and monostatic active sensing. The set of Tx subarrays\nmake up a codebook, whose cardinality we maximize (for communications) subject\nto guaranteeing a desired target identifiability (for sensing). To characterize\nthe size of this novel optimal codebook, we derive first upper and lower\nbounds, which are tight in case of the canonical uniform linear array (ULA) and\nany nonredundant array. We show that the ULA achieves a large codebook -\ncomparable to the size of the conventional unconstrained case - as satisfying\nthe identifiability constraint only requires including two specific sensors in\neach Tx subarray (codeword). In contrast, nonredundant arrays, which have the\nlargest identifiability for a given number of physical sensors, only have a\nsingle admissible codeword, rendering them ineffectual for communications via\nsensor selection alone. The results serve as a step towards an analytical\nunderstanding of the limits of sensor selection in ISAC and the fundamental\ntrade-offs therein.\n","authors":["Robin Rajam√§ki","Piya Pal"],"pdf_url":"https://arxiv.org/pdf/2412.21002v1.pdf","comment":"\\copyright 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2412.20975v1","updated":"2024-12-30T14:26:41Z","published":"2024-12-30T14:26:41Z","title":"Sequential Maximum-Likelihood Estimation of Wideband Polynomial-Phase\n  Signals on Sensor Array","summary":"  This paper presents a novel sequential estimator for the direction-of-arrival\nand polynomial coefficients of wideband polynomial-phase signals impinging on a\nsensor array. Addressing the computational challenges of Maximum-likelihood\nestimation for this problem, we propose a method leveraging random sampling\nconsensus (RANSAC) applied to the time-frequency spatial signatures of sources.\nOur approach supports multiple sources and higher-order polynomials by\nemploying coherent array processing and sequential approximations of the\nMaximum-likelihood cost function. We also propose a low-complexity variant that\nestimates source directions via angular domain random sampling. Numerical\nevaluations demonstrate that the proposed methods achieve Cram\\'er-Rao bounds\nin challenging multi-source scenarios, including closely spaced time-frequency\nspatial signatures, highlighting their suitability for advanced radar signal\nprocessing applications.\n","authors":["Kaleb Debre","Tai Fei","Marius Pesavento"],"pdf_url":"https://arxiv.org/pdf/2412.20975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20937v1","updated":"2024-12-30T13:27:54Z","published":"2024-12-30T13:27:54Z","title":"Generative AI Empowered Semantic Feature Multiple Access (SFMA) Over\n  Wireless Networks","summary":"  This paper investigates a novel generative artificial intelligence (GAI)\nempowered multi-user semantic communication system called semantic feature\nmultiple access (SFMA) for video transmission, which comprises a base station\n(BS) and paired users. The BS generates and combines semantic information of\nseveral frames simultaneously requested by paired users into a single signal.\nUsers recover their frames from this combined signal and input the recovered\nframes into a GAI-based video frame interpolation model to generate the\nintermediate frame. To optimize transmission rates and temporal gaps between\nsimultaneously transmitted frames, we formulate an optimization problem to\nmaximize the system sum rate while minimizing temporal gaps. Since the standard\nsignal-to-interference-plus-noise ratio (SINR) equation does not accurately\ncapture the performance of our semantic communication system, we introduce a\nweight parameter into the SINR equation to better represent the system's\nperformance. Due to its dependence on transmit power, we propose a three-step\nsolution. First, we develop a user pairing algorithm that pairs two users with\nthe highest preference value, a weighted combination of semantic transmission\nrate and temporal gap. Second, we optimize inter-group power allocation by\nformulating an optimization problem that allocates proper transmit power across\nall user groups to maximize system sum rates while satisfying each user's\nminimum rate requirement. Third, we address intra-group power allocation to\nenhance each user's performance. Simulation results demonstrate that our method\nimproves transmission rates by up to 24.8%, 45.8%, and 66.1% compared to\nfixed-power non-orthogonal multiple access (F-NOMA), orthogonal joint\nsource-channel coding (O-JSCC), and orthogonal frequency division multiple\naccess (OFDMA), respectively.\n","authors":["Jiaxiang Wang","Yinchao Yang","Zhaohui Yang","Chongwen Huang","Mingzhe Chen","Zhaoyang Zhang","Mohammad Shikh-Bahaei"],"pdf_url":"https://arxiv.org/pdf/2412.20937v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.20920v1","updated":"2024-12-30T13:01:33Z","published":"2024-12-30T13:01:33Z","title":"Channel Charting-assisted Non-orthogonal Pilot Allocation for Uplink\n  XL-MIMO Transmission","summary":"  Extremely large-scale multiple-input multiple-output (XL-MIMO) is critical to\nfuture wireless networks. The substantial increase in the number of base\nstation (BS) antennas introduces near-field propagation effects in the wireless\nchannels, complicating channel parameter estimation and increasing pilot\noverhead. Channel charting (CC) has emerged as a potent unsupervised technique\nto effectively harness varying high-dimensional channel statistics to enable\nnon-orthogonal pilot assignment and reduce pilot overhead. In this paper, we\ninvestigate near-field channel estimation with reduced pilot overhead by\ndeveloping a CC-assisted pilot scheduling. To this end, we introduce a\npolar-domain codebook to capture the power distribution of near-field XL-MIMO\nchannels. The CC-assisted approach uses such features as inputs to enable an\neffective low-dimensional mapping of the inherent correlation patterns in\nnear-field user terminal (UT) channels. Building upon the mapped channel\ncorrelations, we further propose a near-field CC-assisted pilot allocation\n(NCC-PA) algorithm, which efficiently enhances channel orthogonality among\npilot-reusing UTs. Numerical results confirm that the NCC-PA algorithm\nsubstantially elevates the wireless transmission performance, offering a marked\nimprovement over the conventional far-field CC-PA approach.\n","authors":["Haohong Che","Li You","Jue Wang","Zhenzhou Jin","Chenjie Xie","Xiqi Gao"],"pdf_url":"https://arxiv.org/pdf/2412.20920v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.20885v1","updated":"2024-12-30T11:52:39Z","published":"2024-12-30T11:52:39Z","title":"CF-CGN: Channel Fingerprints Extrapolation for Multi-band Massive MIMO\n  Transmission based on Cycle-Consistent Generative Networks","summary":"  Multi-band massive multiple-input multiple-output (MIMO) communication can\npromote the cooperation of licensed and unlicensed spectra, effectively\nenhancing spectrum efficiency for Wi-Fi and other wireless systems. As an\nenabler for multi-band transmission, channel fingerprints (CF), also known as\nthe channel knowledge map or radio environment map, are used to assist channel\nstate information (CSI) acquisition and reduce computational complexity. In\nthis paper, we propose CF-CGN (Channel Fingerprints with Cycle-consistent\nGenerative Networks) to extrapolate CF for multi-band massive MIMO transmission\nwhere licensed and unlicensed spectra cooperate to provide ubiquitous\nconnectivity. Specifically, we first model CF as a multichannel image and\ntransform the extrapolation problem into an image translation task, which\nconverts CF from one frequency to another by exploring the shared\ncharacteristics of statistical CSI in the beam domain. Then, paired generative\nnetworks are designed and coupled by variable-weight cycle consistency losses\nto fit the reciprocal relationship at different bands. Matched with the coupled\nnetworks, a joint training strategy is developed accordingly, supporting\nsynchronous optimization of all trainable parameters. During the inference\nprocess, we also introduce a refining scheme to improve the extrapolation\naccuracy based on the resolution of CF. Numerical results illustrate that our\nproposed CF-CGN can achieve bidirectional extrapolation with an error of 5-17\ndB lower than the benchmarks in different communication scenarios,\ndemonstrating its excellent generalization ability. We further show that the\nsum rate performance assisted by CF-CGN-based CF is close to that with perfect\nCSI for multi-band massive MIMO transmission.\n","authors":["Chenjie Xie","Li You","Zhenzhou Jin","Jinke Tang","Xiqi Gao","Xiang-Gen Xia"],"pdf_url":"https://arxiv.org/pdf/2412.20885v1.pdf","comment":"13 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.20883v1","updated":"2024-12-30T11:48:09Z","published":"2024-12-30T11:48:09Z","title":"Generative Deep Synthesis of MIMO Sensing Waveforms with Desired\n  Transmit Beampattern","summary":"  This paper develops a generative deep learning model for the synthesis of\nmultiple-input multiple-output (MIMO) active sensing waveforms with desired\nproperties, including constant modulus and a user-defined beampattern. The\nproposed approach is capable synthesizing unique phase codes of on-the-fly,\nwhich has the potential to reduce interference between co-existing active\nsensing systems and facilitate Low Probability of Intercept/Low Probability of\nDetection (LPI/LPD) radar operation. The paper extends our earlier work on\nsynthesis of approximately orthogonal MIMO phase codes by introducing flexible\ncontrol over the transmit beampatterns. The developed machine learning method\nemploys a conditional Wasserstein Generative Adversarial Network (GAN)\nstructure. The main benefits of the method are its ability to discover new\nwaveforms on-demand (post training) and generate demanding beampatterns at\nlower computational complexity compared to structured optimization approaches.\n","authors":["Vesa Saarinen","Robin Rajam√§ki","Visa Koivunen"],"pdf_url":"https://arxiv.org/pdf/2412.20883v1.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2412.04877v2","updated":"2024-12-30T11:12:54Z","published":"2024-12-06T09:19:50Z","title":"Fluid Antenna Index Modulation for MIMO Systems: Robust Transmission and\n  Low-Complexity Detection","summary":"  The fluid antenna (FA) index modulation (IM)-enabled multiple-input\nmultiple-output (MIMO) system, referred to as FA-IM, significantly enhances\nspectral efficiency (SE) compared to the conventional FA-assisted MIMO system.\nTo improve robustness against the high spatial correlation among multiple\nactivated ports of the fluid antenna, this paper proposes an innovative FA\ngrouping-based IM (FAG-IM) system. A block grouping scheme is employed based on\nthe spatial correlation model and the distribution structure of the ports.\nThen, a closed-form expression for the average bit error probability (ABEP)\nupper bound of the FAG-IM system is derived. To reduce the complexity of the\nreceiver, the message passing architecture is incorporated into the FAG-IM\nsystem. Building on this, an efficient approximate message passing (AMP)\ndetector, named structured AMP (S-AMP) detector, is proposed by exploiting the\nstructural characteristics of the transmitted signals. Simulation results\nconfirm that the proposed FAG-IM system significantly outperforms the existing\nFA-IM system in the presence of spatial correlation, achieving more robust\ntransmission. Furthermore, it is demonstrated that the proposed low-complexity\nS-AMP detector not only reduces time complexity to a linear scale but also\nsubstantially improves bit error rate (BER) performance compared to the minimum\nmean square error (MMSE) detector, thereby enhancing the practical feasibility\nof the FAG-IM system.\n","authors":["Xinghao Guo","Yin Xu","Dazhi He","Cixiao Zhang","Hanjiang Hong","Kai-Kit Wong","Wenjun Zhang","Yiyan Wu"],"pdf_url":"https://arxiv.org/pdf/2412.04877v2.pdf","comment":"Submitted to an IEEE journal"},{"id":"http://arxiv.org/abs/2412.20820v1","updated":"2024-12-30T09:30:36Z","published":"2024-12-30T09:30:36Z","title":"Retrieval-Augmented Generation for Mobile Edge Computing via Large\n  Language Model","summary":"  The rapid evolution of mobile edge computing (MEC) has introduced significant\nchallenges in optimizing resource allocation in highly dynamic wireless\ncommunication systems, in which task offloading decisions should be made in\nreal-time. However, existing resource allocation strategies cannot well adapt\nto the dynamic and heterogeneous characteristics of MEC systems, since they are\nshort of scalability, context-awareness, and interpretability. To address these\nissues, this paper proposes a novel retrieval-augmented generation (RAG) method\nto improve the performance of MEC systems. Specifically, a latency minimization\nproblem is first proposed to jointly optimize the data offloading ratio,\ntransmit power allocation, and computing resource allocation. Then, an\nLLM-enabled information-retrieval mechanism is proposed to solve the problem\nefficiently. Extensive experiments across multi-user, multi-task, and highly\ndynamic offloading scenarios show that the proposed method consistently reduces\nlatency compared to several DL-based approaches, achieving 57% improvement\nunder varying user computing ability, 86% with different servers, 30% under\ndistinct transmit powers, and 42% for varying data volumes. These results show\nthe effectiveness of LLM-driven solutions to solve the resource allocation\nproblems in MEC systems.\n","authors":["Runtao Ren","Yinyu Wu","Xuhui Zhang","Jinke Ren","Yanyan Shen","Shuqiang Wang","Kim-Fung Tsang"],"pdf_url":"https://arxiv.org/pdf/2412.20820v1.pdf","comment":"This manuscript has been submitted to IEEE"},{"id":"http://arxiv.org/abs/2412.20819v1","updated":"2024-12-30T09:27:00Z","published":"2024-12-30T09:27:00Z","title":"Movable Antennas Enabled ISAC Systems: Fundamentals, Opportunities, and\n  Future Directions","summary":"  The movable antenna (MA)-enabled integrated sensing and communication (ISAC)\nsystem attracts widespread attention as an innovative framework. The ISAC\nsystem integrates sensing and communication functions, achieving resource\nsharing across various domains, significantly enhancing communication and\nsensing performance, and promoting the intelligent interconnection of\neverything. Meanwhile, MA utilizes the spatial variations of wireless channels\nby dynamically adjusting the positions of MA elements at the transmitter and\nreceiver to improve the channel and further enhance the performance of the ISAC\nsystems. In this paper, we first outline the fundamental principles of MA and\nintroduce the application scenarios of MA-enabled ISAC systems. Then, we\nsummarize the advantages of MA-enabled ISAC systems in enhancing spectral\nefficiency, achieving flexible and precise beamforming, and making the signal\ncoverage range adjustable. Besides, a specific case is studied to show the\nperformance gains in terms of transmit power that MA brings to ISAC systems.\nFinally, we discuss the challenges of MA-enabled ISAC and future research\ndirections, aiming to provide insights for future research on MA-enabled ISAC\nsystems.\n","authors":["Zhendong Li","Jianle Ba","Zhou Su","Jinyuan Huang","Haixia Peng","Wen Chen","Linkang Du","Tom H. Luan"],"pdf_url":"https://arxiv.org/pdf/2412.20819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20806v1","updated":"2024-12-30T09:01:07Z","published":"2024-12-30T09:01:07Z","title":"LEO Satellite-Enabled Random Access with Large Differential Delay and\n  Doppler Shift","summary":"  This paper investigates joint device identification, channel estimation, and\nsymbol detection for LEO satellite-enabled grant-free random access systems,\nspecifically targeting scenarios where remote Internet-of-Things (IoT) devices\noperate without global navigation satellite system (GNSS) assistance.\nConsidering the constrained power consumption of these devices, the large\ndifferential delay and Doppler shift are handled at the satellite receiver. We\nfirstly propose a spreading-based multi-frame transmission scheme with\northogonal time-frequency space (OTFS) modulation to mitigate the doubly\ndispersive effect in time and frequency, and then analyze the input-output\nrelationship of the system. Next, we propose a receiver structure based on\nthree modules: a linear module for identifying active devices that leverages\nthe generalized approximate message passing algorithm to eliminate inter-user\nand inter-carrier interference; a non-linear module that employs the message\npassing algorithm to jointly estimate the channel and detect the transmitted\nsymbols; and a third module that aims to exploit the three dimensional block\nchannel sparsity in the delay-Doppler-angle domain. Soft information is\nexchanged among the three modules by careful message scheduling. Furthermore,\nthe expectation-maximization algorithm is integrated to adjust phase rotation\ncaused by the fractional Doppler and to learn the hyperparameters in the\npriors. Finally, the convolutional neural network is incorporated to enhance\nthe symbol detection. Simulation results demonstrate that the proposed\ntransmission scheme boosts the system performance, and the designed algorithms\noutperform the conventional methods significantly in terms of the device\nidentification, channel estimation, and symbol detection.\n","authors":["Boxiao Shen","Yongpeng Wu","Wenjun Zhang","Symeon Chatzinotas","Bj√∂rn Ottersten"],"pdf_url":"https://arxiv.org/pdf/2412.20806v1.pdf","comment":"This paper has been accepted by the IEEE Transactions on Wireless\n  Communications"},{"id":"http://arxiv.org/abs/2412.20788v1","updated":"2024-12-30T08:12:03Z","published":"2024-12-30T08:12:03Z","title":"An Experimental Study of Passive UAV Tracking with Digital Arrays and\n  Cellular Downlink Signals","summary":"  Given the prospects of the low-altitude economy (LAE) and the popularity of\nunmanned aerial vehicles (UAVs), there are increasing demands on monitoring\nflying objects at low altitude in wide urban areas. In this work, the widely\ndeployed long-term evolution (LTE) base station (BS) is exploited to illuminate\nUAVs in bistatic trajectory tracking. Specifically, a passive sensing receiver\nwith two digital antenna arrays is proposed and developed to capture both the\nline-of-sight (LoS) signal and the scattered signal off a target UAV. From\ntheir cross ambiguity function, the bistatic range, Doppler shift and\nangle-of-arrival (AoA) of the target UAV can be detected in a sequence of time\nslots. In order to address missed detections and false alarms of passive\nsensing, a multi-target tracking framework is adopted to track the trajectory\nof the target UAV. It is demonstrated by experiments that the proposed UAV\ntracking system can achieve a meter-level accuracy.\n","authors":["Yifei Sun","Chao Yu","Yan Luo","Tony Xiao Han","Haisheng Tan","Rui Wang","Francis C. M. Lau"],"pdf_url":"https://arxiv.org/pdf/2412.20788v1.pdf","comment":"13 pages, 10 figures, submitted to IEEE Journal for possible\n  publication"},{"id":"http://arxiv.org/abs/2412.20772v1","updated":"2024-12-30T07:47:30Z","published":"2024-12-30T07:47:30Z","title":"Large Language Model Enabled Multi-Task Physical Layer Network","summary":"  The recent advance of Artificial Intelligence (AI) is continuously reshaping\nthe future 6G wireless communications. Recently, the development of Large\nLanguage Models (LLMs) offers a promising approach to effectively improve the\nperformance and generalization for different physical layer tasks. However,\nmost existing works finetune dedicated LLM networks for a single wireless\ncommunication task separately. Thus performing diverse physical layer tasks\nintroduces extremely high training resources, memory usage, and deployment\ncosts. To solve the problem, we propose a LLM-enabled multi-task physical layer\nnetwork to unify multiple tasks with a single LLM. Specifically, we first\npropose a multi-task LLM framework, which finetunes LLM to perform multi-user\nprecoding, signal detection and channel prediction simultaneously. Besides,\nmulti-task instruction module, input encoders, as well as output decoders, are\nelaborately designed to distinguish multiple tasks and adapted the features of\ndifferent formats of wireless data for the features of LLM. Numerical\nsimulations are also displayed to verify the effectiveness of the proposed\nmethod.\n","authors":["Tianyue Zheng","Linglong Dai"],"pdf_url":"https://arxiv.org/pdf/2412.20772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16450v2","updated":"2024-12-30T07:35:25Z","published":"2024-09-24T20:34:47Z","title":"A Multi-Agent Multi-Environment Mixed Q-Learning for Partially\n  Decentralized Wireless Network Optimization","summary":"  Q-learning is a powerful tool for network control and policy optimization in\nwireless networks, but it struggles with large state spaces. Recent\nadvancements, like multi-environment mixed Q-learning (MEMQ), improves\nperformance and reduces complexity by integrating multiple Q-learning\nalgorithms across multiple related environments so-called digital cousins.\nHowever, MEMQ is designed for centralized single-agent networks and is not\nsuitable for decentralized or multi-agent networks. To address this challenge,\nwe propose a novel multi-agent MEMQ algorithm for partially decentralized\nwireless networks with multiple mobile transmitters (TXs) and base stations\n(BSs), where TXs do not have access to each other's states and actions. In\nuncoordinated states, TXs act independently to minimize their individual costs.\nIn coordinated states, TXs use a Bayesian approach to estimate the joint state\nbased on local observations and share limited information with leader TX to\nminimize joint cost. The cost of information sharing scales linearly with the\nnumber of TXs and is independent of the joint state-action space size. The\nproposed scheme is 50% faster than centralized MEMQ with only a 20% increase in\naverage policy error (APE) and is 25% faster than several advanced\ndecentralized Q-learning algorithms with 40% less APE. The convergence of the\nalgorithm is also demonstrated.\n","authors":["Talha Bozkus","Urbashi Mitra"],"pdf_url":"https://arxiv.org/pdf/2409.16450v2.pdf","comment":"Accepted to 2025 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP 2025)"},{"id":"http://arxiv.org/abs/2410.19763v2","updated":"2024-12-30T06:43:34Z","published":"2024-10-12T07:41:37Z","title":"Movable Antenna Enabled Integrated Sensing and Communication","summary":"  In this paper, we investigate a novel integrated sensing and communication\n(ISAC) system aided by movable antennas (MAs). A bistatic radar system, in\nwhich the base station (BS) is configured with MAs, is integrated into a\nmulti-user multiple-input-single-output (MU-MISO) system. Flexible beamforming\nis studied by jointly optimizing the antenna coefficients and the antenna\npositions. Compared to conventional fixed-position antennas (FPAs), MAs provide\na new degree of freedom (DoF) in beamforming to reconfigure the field response,\nand further improve the received signal quality for both wireless communication\nand sensing. We propose a communication rate and sensing mutual information\n(MI) maximization problem by flexible beamforming optimization. The complex\nfractional objective function with logarithms are first transformed with the\nfractional programming (FP) framework. Then, we propose an efficient algorithm\nto address the non-convex problem with coupled variables by alternatively\nsolving four sub-problems. We derive the closed-form expression to update the\nantenna coefficients by Karush-Kuhn-Tucker (KKT) conditions. To improve the\ndirect gradient ascent (DGA) scheme in updating the positions of the antennas,\na 3-stage search-based projected GA (SPGA) method is proposed. Simulation\nresults show that MAs significantly enhance the overall performance of the ISAC\nsystem, achieving 59.8\\% performance gain compared to conventional ISAC system\nenabled by FPAs. Meanwhile, the proposed SPGA-based method has remarkable\nperformance improvement compared the DGA method in antenna position\noptimization.\n","authors":["Wanting Lyu","Songjie Yang","Yue Xiu","Zhongpei Zhang","Chadi Assi","Chau Yuen"],"pdf_url":"https://arxiv.org/pdf/2410.19763v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11500v4","updated":"2024-12-30T06:39:55Z","published":"2024-06-17T13:02:40Z","title":"ESI-GAL: EEG Source Imaging-based Trajectory Estimation for Grasp and\n  Lift Task","summary":"  Electroencephalogram (EEG) signals-based motor kinematics prediction (MKP)\nhas been an active area of research to develop brain-computer interface (BCI)\nsystems such as exosuits, prostheses, and rehabilitation devices. However, EEG\nsource imaging (ESI) based kinematics prediction is sparsely explored in the\nliterature. In this study, pre-movement EEG features are utilized to predict\nthree-dimensional (3D) hand kinematics for the grasp-and-lift motor task. A\npublic dataset, WAY-EEG-GAL, is utilized for MKP analysis. In particular,\nsensor-domain (EEG data) and source-domain (ESI data) based features from the\nfrontoparietal region are explored for MKP. Deep learning-based models are\nexplored to achieve efficient kinematics decoding. Various time-lagged and\nwindow sizes are analyzed for hand kinematics prediction. Subsequently,\nintra-subject and inter-subject MKP analysis is performed to investigate the\nsubject-specific and subject-independent motor-learning capabilities of the\nneural decoders. The Pearson correlation coefficient (PCC) is used as the\nperformance metric for kinematics trajectory decoding. The rEEGNet neural\ndecoder achieved the best performance with sensor-domain and source-domain\nfeatures with a time lag and window size of 100 ms and 450 ms, respectively.\nThe highest mean PCC values of 0.790, 0.795, and 0.637 are achieved using\nsensor-domain features, while 0.769, 0.777, and 0.647 are achieved using\nsource-domain features in x, y, and z-directions, respectively. This study\nexplores the feasibility of trajectory prediction using EEG sensor-domain and\nsource-domain EEG features for the grasp-and-lift task. Furthermore,\ninter-subject trajectory estimation is performed using the proposed deep\nlearning decoder with EEG source domain features.\n","authors":["Anant Jain","Lalan Kumar"],"pdf_url":"https://arxiv.org/pdf/2406.11500v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20726v1","updated":"2024-12-30T05:55:21Z","published":"2024-12-30T05:55:21Z","title":"Beam Codebook Refinement for mmWave Devices with Random Orientations:\n  Concept and Experimental Validation","summary":"  There is a growing interest in codebook-based beam-steering for\nmillimeter-wave (mmWave) systems due to its potential for low complexity and\nrapid beam search. A key focus of recent research has been the design of\ncodebooks that strike a trade-off between achievable gain and codebook size,\nwhich directly impacts beam search time. Statistical approaches have shown\npromise by leveraging the likelihood that certain beam directions\n(equivalently, sets of phase-shifter configurations) are more probable than\nothers. Such approaches are shown to be valid for static, non-rotating\ntransmission stations such as base stations. However, for the case of user\nterminals that are constantly changing orientation, the possible phase-shifter\nconfigurations become equally probable, rendering statistical methods less\nrelevant. On the other hand, user terminals come with a large number of\npossible steering vector configurations, which can span up to six orders of\nmagnitude. Therefore, efficient solutions to reduce the codebook size (set of\npossible steering vectors) without compromising array gain are needed. We\naddress this challenge by proposing a novel and practical codebook refinement\ntechnique, aiming to reduce the codebook size while maintaining array gain\nwithin $\\gamma$ dB of the maximum achievable gain at any random orientation of\nthe user terminal. We project that a steering vector at a given angle could\neffectively cover adjacent angles with a small gain loss compared to the\nmaximum achievable gain. We demonstrate experimentally that it is possible to\nreduce the codebook size from $1024^{16}$ to just a few configurations (e.g.,\nless than ten), covering all angles while maintaining the gain within\n$\\gamma=3$ dB of the maximum achievable gain.\n","authors":["Bora Bozkurt","Ahmet Muaz Aktas","Hasan Atalay Gunel","Mohaned Chraiti","Ali Gorcin","Ibrahim Hokelek"],"pdf_url":"https://arxiv.org/pdf/2412.20726v1.pdf","comment":"6 pages, 3 figures, conference"},{"id":"http://arxiv.org/abs/2412.20675v1","updated":"2024-12-30T02:59:57Z","published":"2024-12-30T02:59:57Z","title":"Improved ICNN-LSTM Model Classification Based on Attitude Sensor Data\n  for Hazardous State Assessment of Magnetic Adhesion Climbing Wall Robots","summary":"  Magnetic adhesion tracked climbing robots are widely utilized in\nhigh-altitude inspection, welding, and cleaning tasks due to their ability to\nperform various operations against gravity on vertical or inclined walls.\nHowever, during operation, the robot may experience overturning torque caused\nby its own weight and load, which can lead to the detachment of magnetic plates\nand subsequently pose safety risks. This paper proposes an improved ICNN-LSTM\nnetwork classification method based on Micro-Electro-Mechanical Systems (MEMS)\nattitude sensor data for real-time monitoring and assessment of hazardous\nstates in magnetic adhesion tracked climbing robots. Firstly, a data\nacquisition strategy for attitude sensors capable of capturing minute\nvibrations is designed. Secondly, a feature extraction and classification model\ncombining an Improved Convolutional Neural Network (ICNN) with a Long\nShort-Term Memory (LSTM) network is proposed. Experimental validation\ndemonstrates that the proposed minute vibration sensing method achieves\nsignificant results, and the proposed classification model consistently\nexhibits high accuracy compared to other models. The research findings provide\neffective technical support for the safe operation of climbing robots\n","authors":["Zhen Ma","He Xu","Jielong Dou","Yi Qin","Xueyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.20675v1.pdf","comment":"20 pages, 8 figures, manuscript for Journal of Autonomous Robots"},{"id":"http://arxiv.org/abs/2412.20660v1","updated":"2024-12-30T02:30:40Z","published":"2024-12-30T02:30:40Z","title":"Energy Efficient LoRaWAN in LEO Satellites","summary":"  LPWAN service's inexpensive cost and long range capabilities make it a\npromising addition and countless satellite companies have started taking\nadvantage of this technology to connect IoT users across the globe. However,\nLEO satellites have the unique challenge of using rechargeable batteries and\ngreen solar energy to power their components. LPWAN technology is not optimized\nto maximize battery lifespan of network nodes. By incorporating a MAC protocol\nthat maximizes node the battery lifespan across the network, we can reduce\nbattery waste and usage of scarce Earth resources to develop satellite\nbatteries.\n","authors":["Muskan Shergill","Zach Thompson","Guanqun Song","Ting Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.20660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19356v2","updated":"2024-12-30T19:09:55Z","published":"2024-05-23T21:45:15Z","title":"An LSTM Feature Imitation Network for Hand Movement Recognition from\n  sEMG Signals","summary":"  Surface Electromyography (sEMG) is a non-invasive signal that is used in the\nrecognition of hand movement patterns, the diagnosis of diseases, and the\nrobust control of prostheses. Despite the remarkable success of recent\nend-to-end Deep Learning approaches, they are still limited by the need for\nlarge amounts of labeled data. To alleviate the requirement for big data, we\npropose utilizing a feature-imitating network (FIN) for closed-form temporal\nfeature learning over a 300ms signal window on Ninapro DB2, and applying it to\nthe task of 17 hand movement recognition. We implement a lightweight LSTM-FIN\nnetwork to imitate four standard temporal features (entropy, root mean square,\nvariance, simple square integral). We observed that the LSTM-FIN network can\nachieve up to 99\\% R2 accuracy in feature reconstruction and 80\\% accuracy in\nhand movement recognition. Our results also showed that the model can be\nrobustly applied for both within- and cross-subject movement recognition, as\nwell as simulated low-latency environments. Overall, our work demonstrates the\npotential of the FIN modeling paradigm in data-scarce scenarios for sEMG signal\nprocessing.\n","authors":["Chuheng Wu","S. Farokh Atashzar","Mohammad M. Ghassemi","Tuka Alhanai"],"pdf_url":"https://arxiv.org/pdf/2405.19356v2.pdf","comment":"\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2412.21200v1","updated":"2024-12-30T18:59:06Z","published":"2024-12-30T18:59:06Z","title":"Distributed Mixture-of-Agents for Edge Inference with Large Language\n  Models","summary":"  Mixture-of-Agents (MoA) has recently been proposed as a method to enhance\nperformance of large language models (LLMs), enabling multiple individual LLMs\nto work together for collaborative inference. This collaborative approach\nresults in improved responses to user prompts compared to relying on a single\nLLM. In this paper, we consider such an MoA architecture in a distributed\nsetting, where LLMs operate on individual edge devices, each uniquely\nassociated with a user and equipped with its own distributed computing power.\nThese devices exchange information using decentralized gossip algorithms,\nallowing different device nodes to talk without the supervision of a\ncentralized server. In the considered setup, different users have their own LLM\nmodels to address user prompts. Additionally, the devices gossip either their\nown user-specific prompts or augmented prompts to generate more refined answers\nto certain queries. User prompts are temporarily stored in the device queues\nwhen their corresponding LLMs are busy. Given the memory limitations of edge\ndevices, it is crucial to ensure that the average queue sizes in the system\nremain bounded. In this paper, we address this by theoretically calculating the\nqueuing stability conditions for the device queues under reasonable\nassumptions, which we validate experimentally as well. Further, we demonstrate\nthrough experiments, leveraging open-source LLMs for the implementation of\ndistributed MoA, that certain MoA configurations produce higher-quality\nresponses compared to others, as evaluated on AlpacaEval 2.0 benchmark. The\nimplementation is available at:\nhttps://github.com/purbeshmitra/distributed_moa.\n","authors":["Purbesh Mitra","Priyanka Kaswan","Sennur Ulukus"],"pdf_url":"https://arxiv.org/pdf/2412.21200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.21103v1","updated":"2024-12-30T17:27:34Z","published":"2024-12-30T17:27:34Z","title":"Parallel DNA Sequence Alignment on High-Performance Systems with CUDA\n  and MPI","summary":"  Sequence alignment is a cornerstone of bioinformatics, widely used to\nidentify similarities between DNA, RNA, and protein sequences and studying\nevolutionary relationships and functional properties. The Needleman-Wunsch\nalgorithm remains a robust and accurate method for global sequence alignment.\nHowever, its computational complexity, O(mn), poses significant challenges when\nprocessing large-scale datasets or performing multiple sequence alignments. To\naddress these limitations, a hybrid implementation of the Needleman-Wunsch\nalgorithm that leverages CUDA for parallel execution on GPUs and MPI for\ndistributed computation across multiple nodes on a supercomputer is proposed.\nCUDA efficiently offloads computationally intensive tasks to GPU cores, while\nMPI enables communication and workload distribution across nodes to handle\nlarge-scale alignments.\n  This work details the implementation and performance evaluation of the\nNeedleman-Wunsch algorithm in a massively parallel computing environment.\nExperimental results demonstrate significant acceleration of the alignment\nprocess compared to traditional CPU-based implementations, particularly for\nlarge input sizes and multiple sequence alignments. In summary, the combination\nof CUDA and MPI effectively overcomes the computational bottlenecks inherent to\nthe Needleman-Wunsch algorithm without requiring substantial modifications to\nthe underlying algorithm, highlighting the potential of high-performance\ncomputing in advancing sequence alignment workflows.\n","authors":["Linus Zwaka"],"pdf_url":"https://arxiv.org/pdf/2412.21103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.15681v2","updated":"2024-12-30T13:10:11Z","published":"2024-10-21T06:43:04Z","title":"Federated Learning with MMD-based Early Stopping for Adaptive GNSS\n  Interference Classification","summary":"  Federated learning (FL) enables multiple devices to collaboratively train a\nglobal model while maintaining data on local servers. Each device trains the\nmodel on its local server and shares only the model updates (i.e., gradient\nweights) during the aggregation step. A significant challenge in FL is managing\nthe feature distribution of novel and unbalanced data across devices. In this\npaper, we propose an FL approach using few-shot learning and aggregation of the\nmodel weights on a global server. We introduce a dynamic early stopping method\nto balance out-of-distribution classes based on representation learning,\nspecifically utilizing the maximum mean discrepancy of feature embeddings\nbetween local and global models. An exemplary application of FL is to\norchestrate machine learning models along highways for interference\nclassification based on snapshots from global navigation satellite system\n(GNSS) receivers. Extensive experiments on four GNSS datasets from two\nreal-world highways and controlled environments demonstrate that our FL method\nsurpasses state-of-the-art techniques in adapting to both novel interference\nclasses and multipath scenarios.\n","authors":["Nishant S. Gaikwad","Lucas Heublein","Nisha L. Raichur","Tobias Feigl","Christopher Mutschler","Felix Ott"],"pdf_url":"https://arxiv.org/pdf/2410.15681v2.pdf","comment":"Git repository:\n  https://gitlab.cc-asp.fraunhofer.de/darcy_gnss/federated_learning"},{"id":"http://arxiv.org/abs/2412.20860v1","updated":"2024-12-30T10:58:48Z","published":"2024-12-30T10:58:48Z","title":"Adaptive Heuristics for Scheduling DNN Inferencing on Edge and Cloud for\n  Personalized UAV Fleets","summary":"  Drone fleets with onboard cameras coupled with computer vision and DNN\ninferencing models can support diverse applications. One such novel domain is\nfor one or more buddy drones to assist Visually Impaired People (VIPs) lead an\nactive lifestyle. Video inferencing tasks from such drones can help both\nnavigate the drone and provide situation awareness to the VIP, and hence have\nstrict execution deadlines. We propose a deadline-driven heuristic, DEMS-A, to\nschedule diverse DNN tasks generated continuously to perform inferencing over\nvideo segments generated by multiple drones linked to an edge, with the option\nto execute on the cloud. We use strategies like task dropping, work stealing\nand migration, and dynamic adaptation to cloud variability, to guarantee a\nQuality of Service (QoS), i.e. maximize the utility and the number of tasks\ncompleted. We also introduce an additional Quality of Experience (QoE) metric\nuseful to the assistive drone domain, which values the frequency of success for\ntask types to ensure the responsiveness and reliability of the VIP application.\nWe extend our DEMS solution to GEMS to solve this. We evaluate these\nstrategies, using (i) an emulated setup of a fleet of over 80 drones supporting\nover 25 VIPs, with real DNN models executing on pre-recorded drone video\nstreams, using Jetson Nano edges and AWS Lambda cloud functions, and (ii) a\nreal-world setup of a Tello drone and a Jetson Orin Nano edge generating drone\ncommands to follow a VIP in real-time. Our strategies present a task completion\nrate of up to 88%, up to 2.7x higher QoS utility compared to the baselines, a\nfurther 16% higher QoS utility while adapting to network variability, and up to\n75% higher QoE utility. Our practical validation exhibits task completion of up\nto 87% for GEMS and 33% higher total utility of GEMS compared to edge-only.\n","authors":["Suman Raj","Radhika Mittal","Harshil Gupta","Yogesh Simmhan"],"pdf_url":"https://arxiv.org/pdf/2412.20860v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2412.20796v1","updated":"2024-12-30T08:38:09Z","published":"2024-12-30T08:38:09Z","title":"FastCHGNet: Training one Universal Interatomic Potential to 1.5 Hours\n  with 32 GPUs","summary":"  Graph neural network universal interatomic potentials (GNN-UIPs) have\ndemonstrated remarkable generalization and transfer capabilities in material\ndiscovery and property prediction. These models can accelerate molecular\ndynamics (MD) simulation by several orders of magnitude while maintaining\n\\textit{ab initio} accuracy, making them a promising new paradigm in material\nsimulations. One notable example is Crystal Hamiltonian Graph Neural Network\n(CHGNet), pretrained on the energies, forces, stresses, and magnetic moments\nfrom the MPtrj dataset, representing a state-of-the-art GNN-UIP model for\ncharge-informed MD simulations. However, training the CHGNet model is\ntime-consuming(8.3 days on one A100 GPU) for three reasons: (i) requiring\nmulti-layer propagation to reach more distant atom information, (ii) requiring\nsecond-order derivatives calculation to finish weights updating and (iii) the\nimplementation of reference CHGNet does not fully leverage the computational\ncapabilities. This paper introduces FastCHGNet, an optimized CHGNet, with three\ncontributions: Firstly, we design innovative Force/Stress Readout modules to\ndecompose Force/Stress prediction. Secondly, we adopt massive optimizations\nsuch as kernel fusion, redundancy bypass, etc, to exploit GPU computation power\nsufficiently. Finally, we extend CHGNet to support multiple GPUs and propose a\nload-balancing technique to enhance GPU utilization. Numerical results show\nthat FastCHGNet reduces memory footprint by a factor of 3.59. The final\ntraining time of FastCHGNet can be decreased to \\textbf{1.53 hours} on 32 GPUs\nwithout sacrificing model accuracy.\n","authors":["Yuanchang Zhou","Siyu Hu","Chen Wang","Lin-Wang Wang","Guangming Tan","Weile Jia"],"pdf_url":"https://arxiv.org/pdf/2412.20796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12530v2","updated":"2024-12-30T06:16:19Z","published":"2024-10-16T13:10:04Z","title":"Disentangling data distribution for Federated Learning","summary":"  Federated Learning (FL) facilitates collaborative training of a global model\nwhose performance is boosted by private data owned by distributed clients,\nwithout compromising data privacy. Yet the wide applicability of FL is hindered\nby entanglement of data distributions across different clients. This paper\ndemonstrates for the first time that by disentangling data distributions FL can\nin principle achieve efficiencies comparable to those of distributed systems,\nrequiring only one round of communication. To this end, we propose a novel\nFedDistr algorithm, which employs stable diffusion models to decouple and\nrecover data distributions. Empirical results on the CIFAR100 and DomainNet\ndatasets show that FedDistr significantly enhances model utility and efficiency\nin both disentangled and near-disentangled scenarios while ensuring privacy,\noutperforming traditional federated learning methods.\n","authors":["Xinyuan Zhao","Hanlin Gu","Lixin Fan","Yuxing Han","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20674v1","updated":"2024-12-30T02:58:18Z","published":"2024-12-30T02:58:18Z","title":"Blockchain-Empowered Cyber-Secure Federated Learning for Trustworthy\n  Edge Computing","summary":"  Federated Learning (FL) is a privacy-preserving distributed machine learning\nscheme, where each participant data remains on the participating devices and\nonly the local model generated utilizing the local computational power is\ntransmitted throughout the database. However, the distributed computational\nnature of FL creates the necessity to develop a mechanism that can remotely\ntrigger any network agents, track their activities, and prevent threats to the\noverall process posed by malicious participants. Particularly, the FL paradigm\nmay become vulnerable due to an active attack from the network participants,\ncalled a poisonous attack. In such an attack, the malicious participant acts as\na benign agent capable of affecting the global model quality by uploading an\nobfuscated poisoned local model update to the server. This paper presents a\ncross-device FL model that ensures trustworthiness, fairness, and authenticity\nin the underlying FL training process. We leverage trustworthiness by\nconstructing a reputation-based trust model based on contributions of agents\ntoward model convergence. We ensure fairness by identifying and removing\nmalicious agents from the training process through an outlier detection\ntechnique. Further, we establish authenticity by generating a token for each\nparticipating device through a distributed sensing mechanism and storing that\nunique token in a blockchain smart contract. Further, we insert the trust\nscores of all agents into a blockchain and validate their reputations using\nvarious consensus mechanisms that consider the computational task.\n","authors":["Ervin Moore","Ahmed Imteaj","Md Zarif Hossain","Shabnam Rezapour","M. Hadi Amini"],"pdf_url":"https://arxiv.org/pdf/2412.20674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05302v3","updated":"2024-12-30T01:29:09Z","published":"2024-11-26T09:41:26Z","title":"A High Energy-Efficiency Multi-core Neuromorphic Architecture for Deep\n  SNN Training","summary":"  There is a growing necessity for edge training to adapt to dynamically\nchanging environment. Neuromorphic computing represents a significant pathway\nfor high-efficiency intelligent computation in energy-constrained edges, but\nexisting neuromorphic architectures lack the ability of directly training\nspiking neural networks (SNNs) based on backpropagation. We develop a\nmulti-core neuromorphic architecture with Feedforward-Propagation,\nBack-Propagation, and Weight-Gradient engines in each core, supporting high\nefficient parallel computing at both the engine and core levels. It combines\nvarious data flows and sparse computation optimization by fully leveraging the\nsparsity in SNN training, obtaining a high energy efficiency of 1.05TFLOPS/W@\nFP16 @ 28nm, 55 ~ 85% reduction of DRAM access compared to A100 GPU in SNN\ntrainings, and a 20-core deep SNN training and a 5-worker federated learning on\nFPGAs. Our study develops the first multi-core neuromorphic architecture\nsupporting the direct SNN training, facilitating the neuromorphic computing in\nedge-learnable applications.\n","authors":["Mingjing Li","Huihui Zhou","Xiaofeng Xu","Zhiwei Zhong","Puli Quan","Xueke Zhu","Yanyu Lin","Wenjie Lin","Hongyu Guo","Junchao Zhang","Yunhao Ma","Wei Wang","Qingyan Meng","Zhengyu Ma","Guoqi Li","Xiaoxin Cui","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2412.05302v3.pdf","comment":null}]},"2024-12-29T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2407.19639v3","updated":"2024-12-29T02:22:04Z","published":"2024-07-29T01:46:44Z","title":"Segmented Private Data Aggregation in the Multi-message Shuffle Model","summary":"  The shuffle model of differential privacy (DP) offers compelling\nprivacy-utility trade-offs in decentralized settings (e.g., internet of things,\nmobile edge networks). Particularly, the multi-message shuffle model, where\neach user may contribute multiple messages, has shown that accuracy can\napproach that of the central model of DP. However, existing studies typically\nassume a uniform privacy protection level for all users, which may deter\nconservative users from participating and prevent liberal users from\ncontributing more information, thereby reducing the overall data utility, such\nas the accuracy of aggregated statistics. In this work, we pioneer the study of\nsegmented private data aggregation within the multi-message shuffle model of\nDP, introducing flexible privacy protection for users and enhanced utility for\nthe aggregation server. Our framework not only protects users' data but also\nanonymizes their privacy level choices to prevent potential data leakage from\nthese choices. To optimize the privacy-utility-communication trade-offs, we\nexplore approximately optimal configurations for the number of blanket messages\nand conduct almost tight privacy amplification analyses within the shuffle\nmodel. Through extensive experiments, we demonstrate that our segmented\nmulti-message shuffle framework achieves a reduction of about 50\\% in\nestimation error compared to existing approaches, significantly enhancing both\nprivacy and utility.\n","authors":["Shaowei Wang","Hongqiao Chen","Sufen Zeng","Ruilin Yang","Hui Jiang","Peigen Ye","Kaiqi Yu","Rundong Mei","Shaozheng Huang","Wei Yang","Bangzhou Xin"],"pdf_url":"https://arxiv.org/pdf/2407.19639v3.pdf","comment":"Fix typo in an author's name"},{"id":"http://arxiv.org/abs/2412.20603v1","updated":"2024-12-29T22:15:19Z","published":"2024-12-29T22:15:19Z","title":"Privacy-Preserving Identity and Access Management in Multiple Cloud\n  Environments: Models, Issues, and Solutions","summary":"  This paper focuses the attention on privacy-preserving identity and access\nmanagement in multiple Cloud environments, which is an annoying problem in the\nmodern big data era. Within this conceptual context, the paper describes\ncontemporaneous models and issues, and put the basis for future solid\nsolutions. Finally, we provide a summary table where we embed an innovative\ntaxonomy of state-of-the-art research proposals in the reference scientific\nfield.\n","authors":["Alfredo Cuzzocrea","Islam Belmerabet"],"pdf_url":"https://arxiv.org/pdf/2412.20603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20529v1","updated":"2024-12-29T17:33:04Z","published":"2024-12-29T17:33:04Z","title":"Attacks on the neural network and defense methods","summary":"  This article will discuss the use of attacks on a neural network trained on\naudio data, as well as possible methods of protection against these attacks.\nFGSM, PGD and CW attacks, as well as data poisoning, will be considered. Within\nthe framework of protection, Art-IBM and advertorch libraries will be\nconsidered. The obtained accuracy metrics within the framework of attack\napplications are presented\n","authors":["A. Korenev","G. Belokrylov","B. Lodonova","A. Novokhrestov"],"pdf_url":"https://arxiv.org/pdf/2412.20529v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20495v1","updated":"2024-12-29T15:17:42Z","published":"2024-12-29T15:17:42Z","title":"A Multiparty Homomorphic Encryption Approach to Confidential Federated\n  Kaplan Meier Survival Analysis","summary":"  The proliferation of healthcare data has expanded opportunities for\ncollaborative research, yet stringent privacy regulations hinder pooling\nsensitive patient records. We propose a \\emph{multiparty homomorphic\nencryption-based} framework for \\emph{privacy-preserving federated\nKaplan--Meier survival analysis}, offering native floating-point support, a\ntheoretical model, and explicit reconstruction-attack mitigation. Compared to\nprior work, our framework ensures encrypted federated survival estimates\nclosely match centralized outcomes, supported by formal utility-loss bounds\nthat demonstrate convergence as aggregation and decryption noise diminish.\nExtensive experiments on the NCCTG Lung Cancer and synthetic Breast Cancer\ndatasets confirm low \\emph{mean absolute error (MAE)} and \\emph{root mean\nsquared error (RMSE)}, indicating negligible deviations between encrypted and\nnon-encrypted survival curves. Log-rank and numerical accuracy tests reveal\n\\emph{no significant difference} between federated encrypted and non-encrypted\nanalyses, preserving statistical validity. A reconstruction-attack evaluation\nshows smaller federations (2--3 providers) with overlapping data between the\ninstitutions are vulnerable, a challenge mitigated by multiparty encryption.\nLarger federations (5--50 sites) degrade reconstruction accuracy further, with\nencryption improving confidentiality. Despite an 8--19$\\times$ computational\noverhead, threshold-based homomorphic encryption is \\emph{feasible for\nmoderate-scale deployments}, balancing security and runtime. By providing\nrobust privacy guarantees alongside high-fidelity survival estimates, our\nframework advances the state-of-the art in secure multi-institutional survival\nanalysis.\n","authors":["Narasimha Raghavan Veeraragavan","Svetlana Boudko","Jan Franz Nyg√•rd"],"pdf_url":"https://arxiv.org/pdf/2412.20495v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2412.20476v1","updated":"2024-12-29T14:29:34Z","published":"2024-12-29T14:29:34Z","title":"Cut the Deadwood Out: Post-Training Model Purification with Selective\n  Module Substitution","summary":"  The success of DNNs often depends on training with large-scale datasets, but\nbuilding such datasets is both expensive and challenging. Consequently, public\ndatasets from open-source platforms like HuggingFace have become popular,\nposing significant risks of data poisoning attacks. Existing backdoor defenses\nin NLP primarily focus on identifying and removing poisoned samples; however,\npurifying a backdoored model with these sample-cleaning approaches typically\nrequires expensive retraining. Therefore, we propose Greedy Module Substitution\n(GMS), which identifies and substitutes ''deadwood'' modules (i.e., components\ncritical to backdoor pathways) in a backdoored model to purify it. Our method\nrelaxes the common dependency of prior model purification methods on clean\ndatasets or clean auxiliary models. When applied to RoBERTa-large under\nbackdoor attacks, GMS demonstrates strong effectiveness across various\nsettings, particularly against widely recognized challenging attacks like LWS,\nachieving a post-purification attack success rate (ASR) of 9.7% on SST-2\ncompared to 58.8% for the best baseline approach.\n","authors":["Yao Tong","Weijun Li","Xuanli He","Haolan Zhan","Qiongkai Xu"],"pdf_url":"https://arxiv.org/pdf/2412.20476v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2412.20456v1","updated":"2024-12-29T12:51:34Z","published":"2024-12-29T12:51:34Z","title":"Sub-optimal Learning in Meta-Classifier Attacks: A Study of Membership\n  Inference on Differentially Private Location Aggregates","summary":"  The widespread collection and sharing of location data, even in aggregated\nform, raises major privacy concerns. Previous studies used\nmeta-classifier-based membership inference attacks~(MIAs) with multi-layer\nperceptrons~(MLPs) to estimate privacy risks in location data, including when\nprotected by differential privacy (DP). In this work, however, we show that a\nsignificant gap exists between the expected attack accuracy given by DP and the\nempirical attack accuracy even with informed attackers (also known as DP\nattackers), indicating a potential underestimation of the privacy risk. To\nexplore the potential causes for the observed gap, we first propose two new\nmetric-based MIAs: the one-threshold attack and the two-threshold attack. We\nevaluate their performances on real-world location data and find that different\ndata distributions require different attack strategies for optimal performance:\nthe one-threshold attack is more effective with Gaussian DP noise, while the\ntwo-threshold attack performs better with Laplace DP noise. Comparing their\nperformance with one of the MLP-based attack models in previous works shows\nthat the MLP only learns the one-threshold rule, leading to a suboptimal\nperformance under the Laplace DP noise and an underestimation of the privacy\nrisk. Second, we theoretically prove that MLPs can encode complex rules~(\\eg,\nthe two-threshold attack rule), which can be learned when given a substantial\namount of training data. We conclude by discussing the implications of our\nfindings in practice, including broader applications extending beyond location\naggregates to any differentially private datasets containing multiple\nobservations per individual and how techniques such as synthetic data\ngeneration and pre-training might enable MLP to learn more complex optimal\nrules.\n","authors":["Yuhan Liu","Florent Guepin","Igor Shilov","Yves-Alexandre De Montjoye"],"pdf_url":"https://arxiv.org/pdf/2412.20456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20447v1","updated":"2024-12-29T12:09:19Z","published":"2024-12-29T12:09:19Z","title":"Cool, But What About Oracles? An Oracle-Based Perspective on Blockchain\n  Integration in the Accounting Field","summary":"  The Bitcoin Network is a sophisticated accounting system that allows its\nunderlying cryptocurrency to be trusted even in the absence of a reliable\nfinancial authority. Given its undeniable success, the technology, generally\nreferred to as blockchain, has also been proposed as a means to improve legacy\naccounting systems. Accounting for real-world data, however, requires the\nintervention of a third party known as an Oracle, which, having not the same\ncharacteristics as a blockchain, could potentially reduce the expected\nintegration benefit. Through a systematic review of the literature, this study\naims to investigate whether the papers concerning blockchain integration in\naccounting consider and address the limitations posed by oracles. A broad\noverview of the limitations that emerged in the literature is provided and\ndistinguished according to the specific accounting integration. Results support\nthe view that although research on the subject counts numerous articles, actual\nstudies considering oracle limitations are lacking. Interestingly, despite the\nscarce production of papers addressing oracles in various accounting sectors,\nreporting for ESG already shows interesting workarounds for oracle limitations,\nwith permissioned chains envisioned as a valid support for the safe storage of\nsustainability data.\n","authors":["Giulio Caldarelli"],"pdf_url":"https://arxiv.org/pdf/2412.20447v1.pdf","comment":"This manuscript is not Proofread. Some tables and figures, as well as\n  paragraph content may be subject to change in the journal version"},{"id":"http://arxiv.org/abs/2412.20406v1","updated":"2024-12-29T09:10:52Z","published":"2024-12-29T09:10:52Z","title":"A Multidisciplinary Approach to Telegram Data Analysis","summary":"  This paper presents a multidisciplinary approach to analyzing data from\nTelegram for early warning information regarding cyber threats. With the\nproliferation of hacktivist groups utilizing Telegram to disseminate\ninformation regarding future cyberattacks or to boast about successful ones,\nthe need for effective data analysis methods is paramount. The primary\nchallenge lies in the vast number of channels and the overwhelming volume of\ndata, necessitating advanced techniques for discerning pertinent risks amidst\nthe noise. To address this challenge, we employ a combination of neural network\narchitectures and traditional machine learning algorithms. These methods are\nutilized to classify and identify potential cyber threats within the Telegram\ndata. Additionally, sentiment analysis and entity recognition techniques are\nincorporated to provide deeper insights into the nature and context of the\ncommunicated information. The study evaluates the effectiveness of each method\nin detecting and categorizing cyber threats, comparing their performance and\nidentifying areas for improvement. By leveraging these diverse analytical\ntools, we aim to enhance early warning systems for cyber threats, enabling more\nproactive responses to potential security breaches. This research contributes\nto the ongoing efforts to bolster cybersecurity measures in an increasingly\ninterconnected digital landscape.\n","authors":["Velizar Varbanov","Kalin Kopanov","Tatiana Atanasova"],"pdf_url":"https://arxiv.org/pdf/2412.20406v1.pdf","comment":"7 pages, 1 table, 2 figures, 24th International Multidisciplinary\n  Scientific GeoConference SGEM 2024"},{"id":"http://arxiv.org/abs/1702.03587v2","updated":"2024-12-29T12:42:36Z","published":"2017-02-12T22:50:28Z","title":"Post-Quantum Cryptography(PQC): Generalized ElGamal Cipher over\n  GL(8,F251)","summary":"  Post-quantum cryptography (PQC) attempts to find cryptographic protocols\nresistant to attacks using for instance Shor's polynomial time algorithm for\nnumerical field problems like integer factorization (IFP) or the discrete\nlogarithm (DLP). Other aspects are the backdoors discovered in deterministic\nrandom generators or recent advances in solving some instances of DLP. Using\nalternative algebraic structures like non-commutative or non-associative\npartial groupoids, magmas, monoids, semigroups, quasigroups or groups, are\nvalid choices for these new protocols. This paper focuses on an asymmetric\ncipher based on a generalized ElGamal non-arbitrated protocol using a\nnon-commutative general linear group. The developed protocol forces a hard\nsubgroup membership search problem into a non-commutative structure. The\nprotocol involves at first a generalized Diffie-Hellman key interchange and\nfurther on the private and public parameters are recursively updated each time\na new cipher session is launched. Security is based on a hard variation of the\nGeneralized Symmetric Decomposition Problem (GSDP). Working with GL(8, F251)\n64-bit security is achieved, and if GL(16, F251) is chosen, the security rises\nto 127-bit. An appealing feature is that there is no need for big number\nlibraries as all arithmetic is performed in Z_251. Therefore the new protocol\nis particularly useful for computational platforms with very limited\ncapabilities like smartphones or smartcards.\n","authors":["Pedro Hecht"],"pdf_url":"https://arxiv.org/pdf/1702.03587v2.pdf","comment":"6 pages, 6 Tables, 14 Figures"}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2412.20501v1","updated":"2024-12-29T15:37:37Z","published":"2024-12-29T15:37:37Z","title":"TokenRing: An Efficient Parallelism Framework for Infinite-Context LLMs\n  via Bidirectional Communication","summary":"  Efficient parallelization of Large Language Models (LLMs) with long sequences\nis essential but challenging due to their significant computational and memory\ndemands, particularly stemming from communication bottlenecks in attention\nmechanisms. While sequence parallelism (SP) has been introduced as a potential\nsolution, existing methods often suffer from limited scalability or\ninefficiency, rendering their effectiveness.\n  Ring-Attention demonstrates the potential for scaling sequence processing but\nfaces significant limitations due to its reliance on peer-to-peer (P2P)\ncommunication and inefficient utilization of network resources. As the degree\nof SP increases, the quadratic decrease in computation time per step contrasts\nsharply with the linear reduction in communication volume, exacerbating\ncommunication bottlenecks. To address these challenges, we propose TokenRing, a\nfine-grained parallel framework that leverages bidirectional P2P communication\nto effectively overlap computation and data transmission. By partitioning the\nattention block and concurrently transmitting Query and block outputs (i.e.,\n$block\\_out$ and $block\\_lse$) within a fully connected mesh topology,\nTokenRing achieves significant reductions in communication overhead and better\nload balancing. These innovations improve the scalability and efficiency of\ndistributed Transformer models, particularly for long-context sequences.\nExperimental results demonstrate that TokenRing enhances throughput and reduces\ncommunication latency. Moreover, its design adapts seamlessly to various\nmulti-GPU interconnect solutions, such as Huawei Ascend, ensuring broad\ncompatibility and cost-effectiveness for distributed LLM inference and\ntraining. The code is available at:\n\\url{https://github.com/ACA-Lab-SJTU/token-ring}.\n","authors":["Zongwu Wang","Fangxin Liu","Mingshuai Li","Li Jiang"],"pdf_url":"https://arxiv.org/pdf/2412.20501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20379v1","updated":"2024-12-29T06:49:16Z","published":"2024-12-29T06:49:16Z","title":"NeutronTP: Load-Balanced Distributed Full-Graph GNN Training with Tensor\n  Parallelism","summary":"  Graph neural networks (GNNs) have emerged as a promising direction. Training\nlarge-scale graphs that relies on distributed computing power poses new\nchallenges. Existing distributed GNN systems leverage data parallelism by\npartitioning the input graph and distributing it to multiple workers. However,\ndue to the irregular nature of the graph structure, existing distributed\napproaches suffer from unbalanced workloads and high overhead in managing\ncross-worker vertex dependencies. In this paper, we leverage tensor parallelism\nfor distributed GNN training. GNN tensor parallelism eliminates cross-worker\nvertex dependencies by partitioning features instead of graph structures.\nDifferent workers are assigned training tasks on different feature slices with\nthe same dimensional size, leading to a complete load balance. We achieve\nefficient GNN tensor parallelism through two critical functions. Firstly, we\nemploy a generalized decoupled training framework to decouple NN operations\nfrom graph aggregation operations, significantly reducing the communication\noverhead caused by NN operations which must be computed using complete\nfeatures. Secondly, we employ a memory-efficient task scheduling strategy to\nsupport the training of large graphs exceeding single GPU memory, while further\nimproving performance by overlapping communication and computation. By\nintegrating the above techniques, we propose a distributed GNN training system\nNeutronTP. Our experimental results on a 16-node Aliyun cluster demonstrate\nthat NeutronTP achieves 1.29X-8.72X speedup over state-of-the-art GNN systems\nincluding DistDGL, NeutronStar, and Sancus.\n","authors":["Xin Ai","Hao Yuan","Zeyu Ling","Qiange Wang","Yanfeng Zhang","Zhenbo Fu","Chaoyi Chen","Yu Gu","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2412.20379v1.pdf","comment":"14 pages 16 figures, VLDB2025"},{"id":"http://arxiv.org/abs/2205.13648v4","updated":"2024-12-29T05:00:26Z","published":"2022-05-26T21:56:31Z","title":"A Unified Analysis of Federated Learning with Arbitrary Client\n  Participation","summary":"  Federated learning (FL) faces challenges of intermittent client availability\nand computation/communication efficiency. As a result, only a small subset of\nclients can participate in FL at a given time. It is important to understand\nhow partial client participation affects convergence, but most existing works\nhave either considered idealized participation patterns or obtained results\nwith non-zero optimality error for generic patterns. In this paper, we provide\na unified convergence analysis for FL with arbitrary client participation. We\nfirst introduce a generalized version of federated averaging (FedAvg) that\namplifies parameter updates at an interval of multiple FL rounds. Then, we\npresent a novel analysis that captures the effect of client participation in a\nsingle term. By analyzing this term, we obtain convergence upper bounds for a\nwide range of participation patterns, including both non-stochastic and\nstochastic cases, which match either the lower bound of stochastic gradient\ndescent (SGD) or the state-of-the-art results in specific settings. We also\ndiscuss various insights, recommendations, and experimental results.\n","authors":["Shiqiang Wang","Mingyue Ji"],"pdf_url":"https://arxiv.org/pdf/2205.13648v4.pdf","comment":"Presented at NeurIPS 2022. This latest version includes a minor fix\n  of Step (a) in Equation (C.15) in the proof, which only affects a numerical\n  constant in the learning rate choice for the theory. The convergence bounds\n  expressed in $\\mathcal{O}(\\cdot)$ as well as all the main findings and\n  conclusions remain the same"},{"id":"http://arxiv.org/abs/2412.20301v1","updated":"2024-12-29T00:11:22Z","published":"2024-12-29T00:11:22Z","title":"Distributed Hybrid Sketching for $\\ell_2$-Embeddings","summary":"  Linear algebraic operations are ubiquitous in engineering applications, and\narise often in a variety of fields including statistical signal processing and\nmachine learning. With contemporary large datasets, to perform linear algebraic\nmethods and regression tasks, it is necessary to resort to both distributed\ncomputations as well as data compression. In this paper, we study\n\\textit{distributed} $\\ell_2$-subspace embeddings, a common technique used to\nefficiently perform linear regression. In our setting, data is distributed\nacross multiple computing nodes and a goal is to minimize communication between\nthe nodes and the coordinator in the distributed centralized network, while\nmaintaining the geometry of the dataset. Furthermore, there is also the concern\nof keeping the data private and secure from potential adversaries. In this\nwork, we address these issues through randomized sketching, where the key idea\nis to apply distinct sketching matrices on the local datasets. A novelty of\nthis work is that we also consider \\textit{hybrid sketching}, \\textit{i.e.} a\nsecond sketch is applied on the aggregated locally sketched datasets, for\nenhanced embedding results. One of the main takeaways of this work is that by\nhybrid sketching, we can interpolate between the trade-offs that arise in\noff-the-shelf sketching matrices. That is, we can obtain gains in terms of\nembedding dimension or multiplication time. Our embedding arguments are also\njustified numerically.\n","authors":["Neophytos Charalambides","Arya Mazumdar"],"pdf_url":"https://arxiv.org/pdf/2412.20301v1.pdf","comment":"21 pages, 10 figures, 1 table"},{"id":"http://arxiv.org/abs/2412.20341v1","updated":"2024-12-29T03:56:32Z","published":"2024-12-29T03:56:32Z","title":"Asynchronous Federated Clustering with Unknown Number of Clusters","summary":"  Federated Clustering (FC) is crucial to mining knowledge from unlabeled\nnon-Independent Identically Distributed (non-IID) data provided by multiple\nclients while preserving their privacy. Most existing attempts learn cluster\ndistributions at local clients, and then securely pass the desensitized\ninformation to the server for aggregation. However, some tricky but common FC\nproblems are still relatively unexplored, including the heterogeneity in terms\nof clients' communication capacity and the unknown number of proper clusters\n$k^*$. To further bridge the gap between FC and real application scenarios,\nthis paper first shows that the clients' communication asynchrony and unknown\n$k^*$ are complex coupling problems, and then proposes an Asynchronous\nFederated Cluster Learning (AFCL) method accordingly. It spreads the excessive\nnumber of seed points to the clients as a learning medium and coordinates them\nacross the clients to form a consensus. To alleviate the distribution imbalance\ncumulated due to the unforeseen asynchronous uploading from the heterogeneous\nclients, we also design a balancing mechanism for seeds updating. As a result,\nthe seeds gradually adapt to each other to reveal a proper number of clusters.\nExtensive experiments demonstrate the efficacy of AFCL.\n","authors":["Yunfan Zhang","Yiqun Zhang","Yang Lu","Mengke Li","Xi Chen","Yiu-ming Cheung"],"pdf_url":"https://arxiv.org/pdf/2412.20341v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.20322v1","updated":"2024-12-29T02:20:33Z","published":"2024-12-29T02:20:33Z","title":"GreenLLM: Disaggregating Large Language Model Serving on Heterogeneous\n  GPUs for Lower Carbon Emissions","summary":"  LLMs have been widely adopted across many real-world applications. However,\ntheir widespread use comes with significant environmental costs due to their\nhigh computational intensity and resource demands. Specifically, this has\ndriven the development of new generations of high-performing GPUs, exacerbating\nthe problem of electronic waste and accelerating the premature disposal of\ndevices. To address this problem, this paper focuses on reducing the carbon\nemissions of LLM serving by reusing older, low-performing GPUs. We present\nGreenLLM, an SLO-aware LLM serving framework designed to minimize carbon\nemissions by reusing older GPUs. GreenLLM builds on two identified use cases\nthat disaggregate specific computations onto older GPUs, reducing carbon\nemissions while meeting performance goals. To deepen our understanding of the\npotential carbon savings from disaggregation, we also provide a theoretical\nanalysis of its relationship with carbon intensity and GPU lifetime. Our\nevaluations show that GreenLLM reduces carbon emissions by up to 40.6% compared\nto running standard LLM serving on new GPU only, meeting latency SLOs for over\n90% of requests across various applications, latency requirements, carbon\nintensities, and GPU lifetimes.\n","authors":["Tianyao Shi","Yanran Wu","Sihang Liu","Yi Ding"],"pdf_url":"https://arxiv.org/pdf/2412.20322v1.pdf","comment":"15 pages, 15 figures"}],"Information Theory":[{"id":"http://arxiv.org/abs/2408.10001v4","updated":"2024-12-29T19:15:20Z","published":"2024-08-19T13:55:50Z","title":"Coprime Bivariate Bicycle Codes","summary":"  This work (1) proposes a novel numerical algorithm to accelerate the search\nprocess for good Bivariate Bicycle (BB) codes and (2) defines a new subclass of\nBB codes suitable for quantum error correction. The proposed acceleration\nsearch algorithm reduces the search space by excluding some equivalent codes\nfrom the search space, as well as setting thresholds to drop bad codes at an\nearly stage. A number of new BB codes found by this algorithm are reported. The\nproposed subclass of BB codes employs coprimes to construct groups via\npolynomials as the basis for the BB code, rather than using the standard BB\ncodes with unconstrained constructors. In contrast to vanilla BB codes, where\nparameters remain unknown prior to code discovery, the rate of the proposed\ncode can be determined beforehand by specifying a factor polynomial as an input\nto the numerical search algorithm. Using this coprime BB construction, we found\na number of surprisingly short to medium-length codes that were previously\nunknown.\n","authors":["Ming Wang","Frank Mueller"],"pdf_url":"https://arxiv.org/pdf/2408.10001v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08887v2","updated":"2024-12-29T16:54:13Z","published":"2024-06-13T07:42:25Z","title":"Low-Overhead Channel Estimation via 3D Extrapolation for TDD mmWave\n  Massive MIMO Systems Under High-Mobility Scenarios","summary":"  In time division duplexing (TDD) millimeter wave (mmWave) massive\nmultiple-input multiple-output (MIMO) systems, downlink channel state\ninformation (CSI) can be obtained from uplink channel estimation thanks to\nchannel reciprocity. However, under high-mobility scenarios, frequent uplink\nchannel estimation is needed due to channel aging. Additionally, large amounts\nof antennas and subcarriers result in high-dimensional CSI matrices,\naggravating pilot training overhead. To address this, we propose a three-domain\n(3D) channel extrapolation framework across spatial, frequency, and temporal\ndomains. First, considering the effectiveness of traditional knowledge-driven\nchannel estimation methods and the marginal effects of pilots in the spatial\nand frequency domains, a knowledge-and-data driven spatial-frequency channel\nextrapolation network (KDD-SFCEN) is proposed for uplink channel estimation via\njoint spatial-frequency channel extrapolation to reduce spatial-frequency\ndomain pilot overhead. Then, leveraging channel reciprocity and temporal\ndependencies, we propose a temporal uplink-downlink channel extrapolation\nnetwork (TUDCEN) powered by generative artificial intelligence for slot-level\nchannel extrapolation, aiming to reduce the tremendous temporal domain pilot\noverhead caused by high mobility. Numerical results demonstrate the superiority\nof the proposed framework in significantly reducing the pilot training overhead\nby 16 times and improving the system's spectral efficiency under high-mobility\nscenarios compared with state-of-the-art channel estimation/extrapolation\nmethods.\n","authors":["Binggui Zhou","Xi Yang","Shaodan Ma","Feifei Gao","Guanghua Yang"],"pdf_url":"https://arxiv.org/pdf/2406.08887v2.pdf","comment":"17 pages, 11 figures, 3 tables. Accepted by IEEE Transactions on\n  Wireless Communications"},{"id":"http://arxiv.org/abs/2412.20487v1","updated":"2024-12-29T15:02:50Z","published":"2024-12-29T15:02:50Z","title":"Multimodal Variational Autoencoder: a Barycentric View","summary":"  Multiple signal modalities, such as vision and sounds, are naturally present\nin real-world phenomena. Recently, there has been growing interest in learning\ngenerative models, in particular variational autoencoder (VAE), to for\nmultimodal representation learning especially in the case of missing\nmodalities. The primary goal of these models is to learn a modality-invariant\nand modality-specific representation that characterizes information across\nmultiple modalities. Previous attempts at multimodal VAEs approach this mainly\nthrough the lens of experts, aggregating unimodal inference distributions with\na product of experts (PoE), a mixture of experts (MoE), or a combination of\nboth. In this paper, we provide an alternative generic and theoretical\nformulation of multimodal VAE through the lens of barycenter. We first show\nthat PoE and MoE are specific instances of barycenters, derived by minimizing\nthe asymmetric weighted KL divergence to unimodal inference distributions. Our\nnovel formulation extends these two barycenters to a more flexible choice by\nconsidering different types of divergences. In particular, we explore the\nWasserstein barycenter defined by the 2-Wasserstein distance, which better\npreserves the geometry of unimodal distributions by capturing both\nmodality-specific and modality-invariant representations compared to KL\ndivergence. Empirical studies on three multimodal benchmarks demonstrated the\neffectiveness of the proposed method.\n","authors":["Peijie Qiu","Wenhui Zhu","Sayantan Kumar","Xiwen Chen","Xiaotong Sun","Jin Yang","Abolfazl Razi","Yalin Wang","Aristeidis Sotiras"],"pdf_url":"https://arxiv.org/pdf/2412.20487v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2409.20509v2","updated":"2024-12-29T14:29:40Z","published":"2024-09-30T17:11:12Z","title":"A physics-compliant diagonal representation for wireless channels\n  parametrized by beyond-diagonal reconfigurable intelligent surfaces","summary":"  The parametrization of wireless channels by so-called \"beyond-diagonal\nreconfigurable intelligent surfaces\" (BD-RIS) is mathematically characterized\nby a matrix whose off-diagonal entries are partially or fully populated.\nPhysically, this corresponds to tunable coupling mechanisms between the RIS\nelements that originate from the RIS control circuit. Here, we derive a\nphysics-compliant diagonal representation for BD-RIS-parametrized channels. We\nrecognize that any RIS control circuit can always be separated into its static\nparts (SLC) and a set of tunable individual loads (IL). Therefore, a\nBD-RIS-parametrized channel results from the chain cascade of three systems: i)\nradio environment (RE), ii) SLC, and iii) IL. RE and SLC are static\nnon-diagonal systems whose cascade K is terminated by the tunable diagonal\nsystem IL. This physics-compliant representation in terms of K and IL is\ndirectly analogous to that for conventional (\"diagonal\") RIS (D-RIS).\nTherefore, scenarios with BD-RIS can also readily be captured by the\nphysics-compliant coupled-dipole model PhysFad, as we show. In addition,\nphysics-compliant algorithms for system-level optimization with D-RIS can be\ndirectly applied to scenarios with BD-RIS. We demonstrate this important\nimplication of our conceptual finding in a case study on end-to-end channel\nestimation and optimization in a BD-RIS-parametrized rich-scattering\nenvironment. Our case study is the first experimentally grounded system-level\noptimization for BD-RIS: We obtain the characteristics of RE and IL from\nexperimental measurements and a commercial PIN diode, respectively. Altogether,\nour physics-compliant diagonal representation for BD-RIS enables a paradigm\nshift in how practitioners in wireless communications and signal processing\nimplement system-level optimizations for BD-RIS because it enables them to\ndirectly apply existing physics-compliant D-RIS algorithms.\n","authors":["Philipp del Hougne"],"pdf_url":"https://arxiv.org/pdf/2409.20509v2.pdf","comment":"14 pages, 6 figures, submitted to an IEEE Journal"},{"id":"http://arxiv.org/abs/2412.20417v1","updated":"2024-12-29T09:51:19Z","published":"2024-12-29T09:51:19Z","title":"Movable Antenna Array Aided Ultra Reliable Covert Communications","summary":"  In this paper, we construct a framework of the movable antenna (MA) aided\ncovert communication shielded by the general noise uncertainty for the first\ntime. According to the analysis performance on the derived closed-form\nexpressions of the sum of the probabilities of the detection errors and the\ncommunication outage probability, the perfect covertness and the ultra\nreliability can be achieved by adjusting the antenna position in the MA array.\nThen, we formulate the communication covertness maximization problem with the\nconstraints of the ultra reliability and the independent discrete movable\nposition to optimize the transmitter's parameter. With the maximal ratio\ntransmitting (MRT) design for the beamforming, we solve the closed-form optimal\ninformation transmit power and design a lightweight discrete projected gradient\ndescent (DPGD) algorithm to determine the optimal antenna position. The\nnumerical results show that the optimal achievable covertness and the feasible\nregion of the steering angle with the MA array is significant larger than the\none with the fixed-position antenna (FPA) array.\n","authors":["Yida Wang","Guojie Hu","Xiaoling Hu","Xingbo Lu","Yuzhen Huang"],"pdf_url":"https://arxiv.org/pdf/2412.20417v1.pdf","comment":"has been presented in IEEE GLOBECOM 2024"},{"id":"http://arxiv.org/abs/2205.13648v4","updated":"2024-12-29T05:00:26Z","published":"2022-05-26T21:56:31Z","title":"A Unified Analysis of Federated Learning with Arbitrary Client\n  Participation","summary":"  Federated learning (FL) faces challenges of intermittent client availability\nand computation/communication efficiency. As a result, only a small subset of\nclients can participate in FL at a given time. It is important to understand\nhow partial client participation affects convergence, but most existing works\nhave either considered idealized participation patterns or obtained results\nwith non-zero optimality error for generic patterns. In this paper, we provide\na unified convergence analysis for FL with arbitrary client participation. We\nfirst introduce a generalized version of federated averaging (FedAvg) that\namplifies parameter updates at an interval of multiple FL rounds. Then, we\npresent a novel analysis that captures the effect of client participation in a\nsingle term. By analyzing this term, we obtain convergence upper bounds for a\nwide range of participation patterns, including both non-stochastic and\nstochastic cases, which match either the lower bound of stochastic gradient\ndescent (SGD) or the state-of-the-art results in specific settings. We also\ndiscuss various insights, recommendations, and experimental results.\n","authors":["Shiqiang Wang","Mingyue Ji"],"pdf_url":"https://arxiv.org/pdf/2205.13648v4.pdf","comment":"Presented at NeurIPS 2022. This latest version includes a minor fix\n  of Step (a) in Equation (C.15) in the proof, which only affects a numerical\n  constant in the learning rate choice for the theory. The convergence bounds\n  expressed in $\\mathcal{O}(\\cdot)$ as well as all the main findings and\n  conclusions remain the same"},{"id":"http://arxiv.org/abs/2407.09398v3","updated":"2024-12-29T00:44:30Z","published":"2024-07-12T16:19:38Z","title":"6G: The Intelligent Network of Everything","summary":"  The global 6G vision has taken its shape after years of international\nresearch and development efforts. This work culminated in ITU-R Recommendation\non \"IMT-2030 Framework\". While the definition phase of technological\nrequirements is currently ongoing, 3GPP standardization process on 6G networks\nis expected to start in 2025 and worldwide commercialization around 2029-2030.\nThis article serves as a comprehensive guide to 6G by providing an overall\nvision, a contemporary survey of the main literature, and an informative\ntutorial-type presentation style. In our vision, 6G will be based on three\nfundamental elements: wireless, artificial intelligence, and Internet of\nEverything. Consequently, 6G can ultimately become the Intelligent Network of\nEverything while serving as an enabling platform for the next major disruption\nin mobile communication, called mobile intelligence. The potential of mobile\nintelligence is that anything can be made connected, intelligent, and aware of\nits environment. This will revolutionize the way how devices, systems, and\napplications are designed; how they operate and interact with humans and each\nother; and how they can be used for the benefit of people, society, and the\nworld in general. After high-level visioning, the main details of 6G are\ndiscussed, including fundamental elements, disruptive applications, key use\ncases, main performance requirements, potential technologies, and defining\nfeatures. A special focus is given to a comprehensive set of potential 6G\ntechnologies, each of which is introduced in a tutorial manner. Finally, we\nspeculate on what comes after 6G and sketch the first high-level vision of 7G.\nAll in all, the objective of this article is to provide a thorough guide to 6G\nin order to serve as a source of knowledge and inspiration for further research\nand development work in academia, industry, and standardization bodies.\n","authors":["Harri Pennanen","Tuomo H√§nninen","Oskari Tervo","Antti T√∂lli","Matti Latva-aho"],"pdf_url":"https://arxiv.org/pdf/2407.09398v3.pdf","comment":"Accepted for publication in IEEE Access, 101 pages, 50 figures, 16\n  tables"},{"id":"http://arxiv.org/abs/2412.20301v1","updated":"2024-12-29T00:11:22Z","published":"2024-12-29T00:11:22Z","title":"Distributed Hybrid Sketching for $\\ell_2$-Embeddings","summary":"  Linear algebraic operations are ubiquitous in engineering applications, and\narise often in a variety of fields including statistical signal processing and\nmachine learning. With contemporary large datasets, to perform linear algebraic\nmethods and regression tasks, it is necessary to resort to both distributed\ncomputations as well as data compression. In this paper, we study\n\\textit{distributed} $\\ell_2$-subspace embeddings, a common technique used to\nefficiently perform linear regression. In our setting, data is distributed\nacross multiple computing nodes and a goal is to minimize communication between\nthe nodes and the coordinator in the distributed centralized network, while\nmaintaining the geometry of the dataset. Furthermore, there is also the concern\nof keeping the data private and secure from potential adversaries. In this\nwork, we address these issues through randomized sketching, where the key idea\nis to apply distinct sketching matrices on the local datasets. A novelty of\nthis work is that we also consider \\textit{hybrid sketching}, \\textit{i.e.} a\nsecond sketch is applied on the aggregated locally sketched datasets, for\nenhanced embedding results. One of the main takeaways of this work is that by\nhybrid sketching, we can interpolate between the trade-offs that arise in\noff-the-shelf sketching matrices. That is, we can obtain gains in terms of\nembedding dimension or multiplication time. Our embedding arguments are also\njustified numerically.\n","authors":["Neophytos Charalambides","Arya Mazumdar"],"pdf_url":"https://arxiv.org/pdf/2412.20301v1.pdf","comment":"21 pages, 10 figures, 1 table"}],"Signal Processing":[{"id":"http://arxiv.org/abs/2409.13067v2","updated":"2024-12-29T22:53:05Z","published":"2024-09-19T20:01:07Z","title":"E-Sort: Empowering End-to-end Neural Network for Multi-channel Spike\n  Sorting with Transfer Learning and Fast Post-processing","summary":"  Decoding extracellular recordings is a crucial task in electrophysiology and\nbrain-computer interfaces. Spike sorting, which distinguishes spikes and their\nputative neurons from extracellular recordings, becomes computationally\ndemanding with the increasing number of channels in modern neural probes. To\naddress the intensive workload and complex neuron interactions, we propose\nE-Sort, an end-to-end neural network-based spike sorter with transfer learning\nand parallelizable post-processing. Our framework reduces the required number\nof annotated spikes for training by 44% compared to training from scratch,\nachieving up to 25.68% higher accuracy. Additionally, our novel post-processing\nalgorithm is compatible with deep learning frameworks, making E-Sort\nsignificantly faster than state-of-the-art spike sorters. On synthesized\nNeuropixels recordings, E-Sort achieves comparable accuracy with Kilosort4\nwhile sorting 50 seconds of data in only 1.32 seconds. Our method demonstrates\nrobustness across various probe geometries, noise levels, and drift conditions,\noffering a substantial improvement in both accuracy and runtime efficiency\ncompared to existing spike sorters.\n","authors":["Yuntao Han","Shiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2409.13067v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20549v1","updated":"2024-12-29T18:39:28Z","published":"2024-12-29T18:39:28Z","title":"Secure Wireless Communications via Frequency Diverse Arrays","summary":"  A novel frequency diverse array (FDA)-assisted secure transmission framework\nis proposed, which leverages additional frequency offsets to enhance physical\nlayer security. Specifically, an FDA-assisted wiretap channel is considered,\nwhere the transmit beamforming and frequency offsets at each antenna are\njointly optimized. A novel alternating optimization-based method is introduced\nto address the non-convex problem of secure transmission, focusing on\nminimizing transmit power and maximizing the secrecy rate. Numerical results\nare provided to demonstrate the superiority of the FDA-based framework compared\nto systems employing traditional phased array antennas in secure transmission.\n","authors":["Zhenqiao Cheng","Chongjun Ouyang","Xingqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.20549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08887v2","updated":"2024-12-29T16:54:13Z","published":"2024-06-13T07:42:25Z","title":"Low-Overhead Channel Estimation via 3D Extrapolation for TDD mmWave\n  Massive MIMO Systems Under High-Mobility Scenarios","summary":"  In time division duplexing (TDD) millimeter wave (mmWave) massive\nmultiple-input multiple-output (MIMO) systems, downlink channel state\ninformation (CSI) can be obtained from uplink channel estimation thanks to\nchannel reciprocity. However, under high-mobility scenarios, frequent uplink\nchannel estimation is needed due to channel aging. Additionally, large amounts\nof antennas and subcarriers result in high-dimensional CSI matrices,\naggravating pilot training overhead. To address this, we propose a three-domain\n(3D) channel extrapolation framework across spatial, frequency, and temporal\ndomains. First, considering the effectiveness of traditional knowledge-driven\nchannel estimation methods and the marginal effects of pilots in the spatial\nand frequency domains, a knowledge-and-data driven spatial-frequency channel\nextrapolation network (KDD-SFCEN) is proposed for uplink channel estimation via\njoint spatial-frequency channel extrapolation to reduce spatial-frequency\ndomain pilot overhead. Then, leveraging channel reciprocity and temporal\ndependencies, we propose a temporal uplink-downlink channel extrapolation\nnetwork (TUDCEN) powered by generative artificial intelligence for slot-level\nchannel extrapolation, aiming to reduce the tremendous temporal domain pilot\noverhead caused by high mobility. Numerical results demonstrate the superiority\nof the proposed framework in significantly reducing the pilot training overhead\nby 16 times and improving the system's spectral efficiency under high-mobility\nscenarios compared with state-of-the-art channel estimation/extrapolation\nmethods.\n","authors":["Binggui Zhou","Xi Yang","Shaodan Ma","Feifei Gao","Guanghua Yang"],"pdf_url":"https://arxiv.org/pdf/2406.08887v2.pdf","comment":"17 pages, 11 figures, 3 tables. Accepted by IEEE Transactions on\n  Wireless Communications"},{"id":"http://arxiv.org/abs/2412.20484v1","updated":"2024-12-29T14:52:13Z","published":"2024-12-29T14:52:13Z","title":"Exploiting NOMA Transmissions in Multi-UAV-assisted Wireless Networks:\n  From Aerial-RIS to Mode-switching UAVs","summary":"  In this paper, we consider an aerial reconfigurable intelligent surface\n(ARIS)-assisted wireless network, where multiple unmanned aerial vehicles\n(UAVs) collect data from ground users (GUs) by using the non-orthogonal\nmultiple access (NOMA) method. The ARIS provides enhanced channel\ncontrollability to improve the NOMA transmissions and reduce the co-channel\ninterference among UAVs. We also propose a novel dual-mode switching scheme,\nwhere each UAV equipped with both an ARIS and a radio frequency (RF)\ntransceiver can adaptively perform passive reflection or active transmission.\nWe aim to maximize the overall network throughput by jointly optimizing the\nUAVs' trajectory planning and operating modes, the ARIS's passive beamforming,\nand the GUs' transmission control strategies. We propose an optimization-driven\nhierarchical deep reinforcement learning (O-HDRL) method to decompose it into a\nseries of subproblems. Specifically, the multi-agent deep deterministic policy\ngradient (MADDPG) adjusts the UAVs' trajectory planning and mode switching\nstrategies, while the passive beamforming and transmission control strategies\nare tackled by the optimization methods. Numerical results reveal that the\nO-HDRL efficiently improves the learning stability and reward performance\ncompared to the benchmark methods. Meanwhile, the dual-mode switching scheme is\nverified to achieve a higher throughput performance compared to the fixed ARIS\nscheme.\n","authors":["Songhan Zhao","Shimin Gong","Bo Gu","Lanhua Li","Bin Lyu","Dinh Thai Hoang","Changyan Yi"],"pdf_url":"https://arxiv.org/pdf/2412.20484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20509v2","updated":"2024-12-29T14:29:40Z","published":"2024-09-30T17:11:12Z","title":"A physics-compliant diagonal representation for wireless channels\n  parametrized by beyond-diagonal reconfigurable intelligent surfaces","summary":"  The parametrization of wireless channels by so-called \"beyond-diagonal\nreconfigurable intelligent surfaces\" (BD-RIS) is mathematically characterized\nby a matrix whose off-diagonal entries are partially or fully populated.\nPhysically, this corresponds to tunable coupling mechanisms between the RIS\nelements that originate from the RIS control circuit. Here, we derive a\nphysics-compliant diagonal representation for BD-RIS-parametrized channels. We\nrecognize that any RIS control circuit can always be separated into its static\nparts (SLC) and a set of tunable individual loads (IL). Therefore, a\nBD-RIS-parametrized channel results from the chain cascade of three systems: i)\nradio environment (RE), ii) SLC, and iii) IL. RE and SLC are static\nnon-diagonal systems whose cascade K is terminated by the tunable diagonal\nsystem IL. This physics-compliant representation in terms of K and IL is\ndirectly analogous to that for conventional (\"diagonal\") RIS (D-RIS).\nTherefore, scenarios with BD-RIS can also readily be captured by the\nphysics-compliant coupled-dipole model PhysFad, as we show. In addition,\nphysics-compliant algorithms for system-level optimization with D-RIS can be\ndirectly applied to scenarios with BD-RIS. We demonstrate this important\nimplication of our conceptual finding in a case study on end-to-end channel\nestimation and optimization in a BD-RIS-parametrized rich-scattering\nenvironment. Our case study is the first experimentally grounded system-level\noptimization for BD-RIS: We obtain the characteristics of RE and IL from\nexperimental measurements and a commercial PIN diode, respectively. Altogether,\nour physics-compliant diagonal representation for BD-RIS enables a paradigm\nshift in how practitioners in wireless communications and signal processing\nimplement system-level optimizations for BD-RIS because it enables them to\ndirectly apply existing physics-compliant D-RIS algorithms.\n","authors":["Philipp del Hougne"],"pdf_url":"https://arxiv.org/pdf/2409.20509v2.pdf","comment":"14 pages, 6 figures, submitted to an IEEE Journal"},{"id":"http://arxiv.org/abs/2412.20417v1","updated":"2024-12-29T09:51:19Z","published":"2024-12-29T09:51:19Z","title":"Movable Antenna Array Aided Ultra Reliable Covert Communications","summary":"  In this paper, we construct a framework of the movable antenna (MA) aided\ncovert communication shielded by the general noise uncertainty for the first\ntime. According to the analysis performance on the derived closed-form\nexpressions of the sum of the probabilities of the detection errors and the\ncommunication outage probability, the perfect covertness and the ultra\nreliability can be achieved by adjusting the antenna position in the MA array.\nThen, we formulate the communication covertness maximization problem with the\nconstraints of the ultra reliability and the independent discrete movable\nposition to optimize the transmitter's parameter. With the maximal ratio\ntransmitting (MRT) design for the beamforming, we solve the closed-form optimal\ninformation transmit power and design a lightweight discrete projected gradient\ndescent (DPGD) algorithm to determine the optimal antenna position. The\nnumerical results show that the optimal achievable covertness and the feasible\nregion of the steering angle with the MA array is significant larger than the\none with the fixed-position antenna (FPA) array.\n","authors":["Yida Wang","Guojie Hu","Xiaoling Hu","Xingbo Lu","Yuzhen Huang"],"pdf_url":"https://arxiv.org/pdf/2412.20417v1.pdf","comment":"has been presented in IEEE GLOBECOM 2024"},{"id":"http://arxiv.org/abs/2412.20391v1","updated":"2024-12-29T08:04:54Z","published":"2024-12-29T08:04:54Z","title":"Open-Source Heterogeneous SoCs for AI: The PULP Platform Experience","summary":"  Since 2013, the PULP (Parallel Ultra-Low Power) Platform project has been one\nof the most active and successful initiatives in designing research IPs and\nreleasing them as open-source. Its portfolio now ranges from processor cores to\nnetwork-on-chips, peripherals, SoC templates, and full hardware accelerators.\nIn this article, we focus on the PULP experience designing heterogeneous AI\nacceleration SoCs - an endeavour encompassing SoC architecture definition;\ndevelopment, verification, and integration of acceleration IPs; front- and\nback-end VLSI design; testing; development of AI deployment software.\n","authors":["Francesco Conti","Angelo Garofalo","Davide Rossi","Giuseppe Tagliavini","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2412.20391v1.pdf","comment":"Preprinted submitted to IEEE Solid-State Circuits Magazine"},{"id":"http://arxiv.org/abs/2412.20371v1","updated":"2024-12-29T06:30:20Z","published":"2024-12-29T06:30:20Z","title":"Cooperative ISAC-empowered Low-Altitude Economy","summary":"  This paper proposes a cooperative integrated sensing and communication (ISAC)\nscheme for the low-altitude sensing scenario, aiming at estimating the\nparameters of the unmanned aerial vehicles (UAVs) and enhancing the sensing\nperformance via cooperation. The proposed scheme consists of two stages. In\nStage I, we formulate the monostatic parameter estimation problem via using a\ntensor decomposition model. By leveraging the Vandermonde structure of the\nfactor matrix, a spatial smoothing tensor decomposition scheme is introduced to\nestimate the UAVs' parameters. To further reduce the computational complexity,\nwe design a reduced-dimensional (RD) angle of arrival (AoA) estimation\nalgorithm based on generalized Rayleigh quotient (GRQ). In Stage II, the\npositions and true velocities of the UAVs are determined through the data\nfusion across multiple base stations (BSs). Specifically, we first develop a\nfalse removing minimum spanning tree (MST)-based data association method to\naccurately match the BSs' parameter estimations to the same UAV. Then, a Pareto\noptimality method and a residual weighting scheme are developed to facilitate\nthe position and velocity estimation, respectively. We further extend our\napproach to the dual-polarized system. Simulation results validate the\neffectiveness of the proposed schemes in comparison to the conventional\ntechniques.\n","authors":["Jun Tang","Yiming Yu","Cunhua Pan","Hong Ren","Dongming Wang","Jiangzhou Wang","Xiaohu You"],"pdf_url":"https://arxiv.org/pdf/2412.20371v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02085v5","updated":"2024-12-29T04:41:32Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between the latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v5.pdf","comment":"Accepted to TMLR with Survey Certificate, review, survey, 37 pages, 5\n  figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.20349v1","updated":"2024-12-29T04:39:05Z","published":"2024-12-29T04:39:05Z","title":"Two-Timescale Design for AP Mode Selection of Cooperative ISAC Networks","summary":"  As an emerging technology, cooperative bi-static integrated sensing and\ncommunication (ISAC) is promising to achieve high-precision sensing, high-rate\ncommunication as well as self-interference (SI) avoidance. This paper\ninvestigates the two-timescale design for access point (AP) mode selection to\nrealize the full potential of the cooperative bi-static ISAC network with low\nsystem overhead, where the beamforming at the APs is adapted to the\nrapidly-changing instantaneous channel state information (CSI), while the AP\nmode is adapted to the slowly-changing statistical CSI. We first apply the\nminimum mean square error (MMSE) estimator to estimate the channel between the\nAPs and the channels from the APs to the user equipments (UEs). Then we adopt\nthe low-complexity maximum ratio transmission (MRT) beamforming and the maximum\nratio combining (MRC) detector, and derive the closed-form expressions of the\ncommunication rate and the sensing signal-to-interference-plus-noise-ratio\n(SINR). We formulate a non-convex integer optimization problem to maximize the\nminimum sensing SINR under the communication quality of service (QoS)\nconstraints. McCormick envelope relaxation and successive convex approximation\n(SCA) techniques are applied to solve the challenging non-convex integer\noptimization problem. Simulation results validate the closed-form expressions\nand prove the convergence and effectiveness of the proposed AP mode selection\nscheme.\n","authors":["Zhichu Ren","Cunhua Pan","Hong Ren","Dongming Wang","Lexi Xu","Jiangzhou Wang"],"pdf_url":"https://arxiv.org/pdf/2412.20349v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.20314v1","updated":"2024-12-29T01:29:33Z","published":"2024-12-29T01:29:33Z","title":"Resource Allocation for ISAC Networks with Application to Target\n  Tracking","summary":"  Future 6G networks are expected to empower communication systems by\nintegrating sensing capabilities, resulting in integrated sensing and\ncommunication (ISAC) systems. However, this integration may exacerbate the data\ntraffic congestion in existing communication systems due to limited resources.\nTherefore, the resources of ISAC systems must be carefully allocated to ensure\nhigh performance. Given the increasing demands for both sensing and\ncommunication services, current methods are inadequate for tracking targets\nfrequently in every frame while simultaneously communicating with users. To\naddress this gap, this work formulates an optimization problem that jointly\nallocates resources in the time, frequency, power, and spatial domains for\ntargets and users, accounting for the movement of targets and time-varying\ncommunication channels. Specifically, we minimize the trace of posterior\nCram\\'er-Rao bound for target tracking subject to communication throughput and\nresource allocation constraints. To solve this non-convex problem, we develop a\nblock coordinate descent (BCD) algorithm based on the penalty method,\nsuccessive convex approximation (SCA), and one-dimensional search. Simulation\nresults demonstrate the validity of the proposed algorithm and the performance\ntrade-off between sensing and communication.\n","authors":["Lu Wang","Luis F. Abanto-Leon"],"pdf_url":"https://arxiv.org/pdf/2412.20314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20301v1","updated":"2024-12-29T00:11:22Z","published":"2024-12-29T00:11:22Z","title":"Distributed Hybrid Sketching for $\\ell_2$-Embeddings","summary":"  Linear algebraic operations are ubiquitous in engineering applications, and\narise often in a variety of fields including statistical signal processing and\nmachine learning. With contemporary large datasets, to perform linear algebraic\nmethods and regression tasks, it is necessary to resort to both distributed\ncomputations as well as data compression. In this paper, we study\n\\textit{distributed} $\\ell_2$-subspace embeddings, a common technique used to\nefficiently perform linear regression. In our setting, data is distributed\nacross multiple computing nodes and a goal is to minimize communication between\nthe nodes and the coordinator in the distributed centralized network, while\nmaintaining the geometry of the dataset. Furthermore, there is also the concern\nof keeping the data private and secure from potential adversaries. In this\nwork, we address these issues through randomized sketching, where the key idea\nis to apply distinct sketching matrices on the local datasets. A novelty of\nthis work is that we also consider \\textit{hybrid sketching}, \\textit{i.e.} a\nsecond sketch is applied on the aggregated locally sketched datasets, for\nenhanced embedding results. One of the main takeaways of this work is that by\nhybrid sketching, we can interpolate between the trade-offs that arise in\noff-the-shelf sketching matrices. That is, we can obtain gains in terms of\nembedding dimension or multiplication time. Our embedding arguments are also\njustified numerically.\n","authors":["Neophytos Charalambides","Arya Mazumdar"],"pdf_url":"https://arxiv.org/pdf/2412.20301v1.pdf","comment":"21 pages, 10 figures, 1 table"},{"id":"http://arxiv.org/abs/2412.20400v1","updated":"2024-12-29T08:29:50Z","published":"2024-12-29T08:29:50Z","title":"Design of an improved microstrip antenna operating at a frequency band\n  of 28GHz","summary":"  The design of an improved microstrip antenna operating in the 28 GHz\nfrequency spectrum is the main goal of this work. The design used a Roger RT\n5880 LZ substrate with a thickness and permittivity of 0.762mm and 1.96,\nrespectively. The antenna was simulated in CST Microwave Studio. As the antenna\nfeed, a quarter-wave transformer was used to provide an impedance match of 50\nohms. To improve the antenna's performance, a Ushaped element was added to the\nground plane. The antenna resonated at 28 GHz frequency, according to\nsimulation data, with a return loss of -21.4 dB, VSWR of 1.18, bandwidth of\n2.026 GHz, and gain of 8.19 dB. The proposed antenna exhibits a performance\nimprovement in terms of gain and bandwidth due to the addition of U-shaped\nelement when benchmarked with existing designs in the literature work\n","authors":["S. O. Zakariyya","B. O. Sadiq","R. A. Alao","J. A. Adesina","E. Obi"],"pdf_url":"https://arxiv.org/pdf/2412.20400v1.pdf","comment":null}]},"2024-12-28T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2412.20255v1","updated":"2024-12-28T19:59:33Z","published":"2024-12-28T19:59:33Z","title":"An Anomaly Detection System Based on Generative Classifiers for\n  Controller Area Network","summary":"  As electronic systems become increasingly complex and prevalent in modern\nvehicles, securing onboard networks is crucial, particularly as many of these\nsystems are safety-critical. Researchers have demonstrated that modern vehicles\nare susceptible to various types of attacks, enabling attackers to gain control\nand compromise safety-critical electronic systems. Consequently, several\nIntrusion Detection Systems (IDSs) have been proposed in the literature to\ndetect such cyber-attacks on vehicles. This paper introduces a novel generative\nclassifier-based Intrusion Detection System (IDS) designed for anomaly\ndetection in automotive networks, specifically focusing on the Controller Area\nNetwork (CAN). Leveraging variational Bayes, our proposed IDS utilizes a deep\nlatent variable model to construct a causal graph for conditional\nprobabilities. An auto-encoder architecture is utilized to build the classifier\nto estimate conditional probabilities, which contribute to the final prediction\nprobabilities through Bayesian inference. Comparative evaluations against\nstate-of-the-art IDSs on a public Car-hacking dataset highlight our proposed\nclassifier's superior performance in improving detection accuracy and F1-score.\nThe proposed IDS demonstrates its efficacy by outperforming existing models\nwith limited training data, providing enhanced security assurance for\nautomotive systems.\n","authors":["Chunheng Zhao","Stefano Longari","Michele Carminati","Pierluigi Pisu"],"pdf_url":"https://arxiv.org/pdf/2412.20255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20231v1","updated":"2024-12-28T17:59:21Z","published":"2024-12-28T17:59:21Z","title":"How To Think About End-To-End Encryption and AI: Training, Processing,\n  Disclosure, and Consent","summary":"  End-to-end encryption (E2EE) has become the gold standard for securing\ncommunications, bringing strong confidentiality and privacy guarantees to\nbillions of users worldwide. However, the current push towards widespread\nintegration of artificial intelligence (AI) models, including in E2EE systems,\nraises some serious security concerns. This work performs a critical\nexamination of the (in)compatibility of AI models and E2EE applications. We\nexplore this on two fronts: (1) the integration of AI \"assistants\" within E2EE\napplications, and (2) the use of E2EE data for training AI models. We analyze\nthe potential security implications of each, and identify conflicts with the\nsecurity guarantees of E2EE. Then, we analyze legal implications of integrating\nAI models in E2EE applications, given how AI integration can undermine the\nconfidentiality that E2EE promises. Finally, we offer a list of detailed\nrecommendations based on our technical and legal analyses, including: technical\ndesign choices that must be prioritized to uphold E2EE security; how service\nproviders must accurately represent E2EE security; and best practices for the\ndefault behavior of AI features and for requesting user consent. We hope this\npaper catalyzes an informed conversation on the tensions that arise between the\nbrisk deployment of AI and the security offered by E2EE, and guides the\nresponsible development of new AI features.\n","authors":["Mallory Knodel","Andr√©s F√°brega","Daniella Ferrari","Jacob Leiken","Betty Li Hou","Derek Yen","Sam de Alfaro","Kyunghyun Cho","Sunoo Park"],"pdf_url":"https://arxiv.org/pdf/2412.20231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03168v2","updated":"2024-12-28T13:39:59Z","published":"2024-10-04T06:01:27Z","title":"Can Watermarked LLMs be Identified by Users via Crafted Prompts?","summary":"  Text watermarking for Large Language Models (LLMs) has made significant\nprogress in detecting LLM outputs and preventing misuse. Current watermarking\ntechniques offer high detectability, minimal impact on text quality, and\nrobustness to text editing. However, current researches lack investigation into\nthe imperceptibility of watermarking techniques in LLM services. This is\ncrucial as LLM providers may not want to disclose the presence of watermarks in\nreal-world scenarios, as it could reduce user willingness to use the service\nand make watermarks more vulnerable to attacks. This work is the first to\ninvestigate the imperceptibility of watermarked LLMs. We design an\nidentification algorithm called Water-Probe that detects watermarks through\nwell-designed prompts to the LLM. Our key motivation is that current\nwatermarked LLMs expose consistent biases under the same watermark key,\nresulting in similar differences across prompts under different watermark keys.\nExperiments show that almost all mainstream watermarking algorithms are easily\nidentified with our well-designed prompts, while Water-Probe demonstrates a\nminimal false positive rate for non-watermarked LLMs. Finally, we propose that\nthe key to enhancing the imperceptibility of watermarked LLMs is to increase\nthe randomness of watermark key selection. Based on this, we introduce the\nWater-Bag strategy, which significantly improves watermark imperceptibility by\nmerging multiple watermark keys.\n","authors":["Aiwei Liu","Sheng Guan","Yiming Liu","Leyi Pan","Yifei Zhang","Liancheng Fang","Lijie Wen","Philip S. Yu","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2410.03168v2.pdf","comment":"30 pages, 5 figures, 11 tables"},{"id":"http://arxiv.org/abs/2412.20087v1","updated":"2024-12-28T09:08:37Z","published":"2024-12-28T09:08:37Z","title":"On the Validity of Traditional Vulnerability Scoring Systems for\n  Adversarial Attacks against LLMs","summary":"  This research investigates the effectiveness of established vulnerability\nmetrics, such as the Common Vulnerability Scoring System (CVSS), in evaluating\nattacks against Large Language Models (LLMs), with a focus on Adversarial\nAttacks (AAs). The study explores the influence of both general and specific\nmetric factors in determining vulnerability scores, providing new perspectives\non potential enhancements to these metrics.\n  This study adopts a quantitative approach, calculating and comparing the\ncoefficient of variation of vulnerability scores across 56 adversarial attacks\non LLMs. The attacks, sourced from various research papers, and obtained\nthrough online databases, were evaluated using multiple vulnerability metrics.\nScores were determined by averaging the values assessed by three distinct LLMs.\nThe results indicate that existing scoring-systems yield vulnerability scores\nwith minimal variation across different attacks, suggesting that many of the\nmetric factors are inadequate for assessing adversarial attacks on LLMs. This\nis particularly true for context-specific factors or those with predefined\nvalue sets, such as those in CVSS. These findings support the hypothesis that\ncurrent vulnerability metrics, especially those with rigid values, are limited\nin evaluating AAs on LLMs, highlighting the need for the development of more\nflexible, generalized metrics tailored to such attacks.\n  This research offers a fresh analysis of the effectiveness and applicability\nof established vulnerability metrics, particularly in the context of\nAdversarial Attacks on Large Language Models, both of which have gained\nsignificant attention in recent years. Through extensive testing and\ncalculations, the study underscores the limitations of these metrics and opens\nup new avenues for improving and refining vulnerability assessment frameworks\nspecifically tailored for LLMs.\n","authors":["Atmane Ayoub Mansour Bahar","Ahmad Samer Wazan"],"pdf_url":"https://arxiv.org/pdf/2412.20087v1.pdf","comment":"101 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.19980v1","updated":"2024-12-28T02:47:14Z","published":"2024-12-28T02:47:14Z","title":"Hades: Homomorphic Augmented Decryption for Efficient Symbol-comparison\n  -- A Database's Perspective","summary":"  Outsourced databases powered by fully homomorphic encryption (FHE) offer the\npromise of secure data processing on untrusted cloud servers. A crucial aspect\nof database functionality, and one that has remained challenging to integrate\nefficiently within FHE schemes, is the ability to perform comparisons on\nencrypted data. Such comparisons are fundamental for various database\noperations, including building indexes for efficient data retrieval and\nexecuting range queries to select data within specific intervals. While\ntraditional approaches like Order-Preserving Encryption (OPE) could enable\ncomparisons, they are fundamentally incompatible with FHE without significantly\nincreasing ciphertext size, thereby exacerbating the inherent performance\noverhead of FHE and further hindering its practical deployment. This paper\nintroduces HADES, a novel cryptographic framework that enables efficient and\nsecure comparisons directly on FHE ciphertexts without any ciphertext\nexpansion. Based on the Ring Learning with Errors (RLWE) problem, HADES\nprovides CPA-security and incorporates perturbation-aware encryption to\nmitigate frequency-analysis attacks. Implemented using OpenFHE, HADES supports\nboth integer and floating-point operations, demonstrating practical performance\non real-world datasets and outperforming state-of-the-art baselines.\n","authors":["Dongfang Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.19980v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19979v1","updated":"2024-12-28T02:45:15Z","published":"2024-12-28T02:45:15Z","title":"Explainable Semantic Federated Learning Enabled Industrial Edge Network\n  for Fire Surveillance","summary":"  In fire surveillance, Industrial Internet of Things (IIoT) devices require\ntransmitting large monitoring data frequently, which leads to huge consumption\nof spectrum resources. Hence, we propose an Industrial Edge Semantic Network\n(IESN) to allow IIoT devices to send warnings through Semantic communication\n(SC). Thus, we should consider (1) Data privacy and security. (2) SC model\nadaptation for heterogeneous devices. (3) Explainability of semantics.\nTherefore, first, we present an eXplainable Semantic Federated Learning (XSFL)\nto train the SC model, thus ensuring data privacy and security. Then, we\npresent an Adaptive Client Training (ACT) strategy to provide a specific SC\nmodel for each device according to its Fisher information matrix, thus\novercoming the heterogeneity. Next, an Explainable SC (ESC) mechanism is\ndesigned, which introduces a leakyReLU-based activation mapping to explain the\nrelationship between the extracted semantics and monitoring data. Finally,\nsimulation results demonstrate the effectiveness of XSFL.\n","authors":["Li Dong","Yubo Peng","Feibo Jiang","Kezhi Wang","Kun Yang"],"pdf_url":"https://arxiv.org/pdf/2412.19979v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2412.20200v1","updated":"2024-12-28T16:23:10Z","published":"2024-12-28T16:23:10Z","title":"Federated Unlearning with Gradient Descent and Conflict Mitigation","summary":"  Federated Learning (FL) has received much attention in recent years. However,\nalthough clients are not required to share their data in FL, the global model\nitself can implicitly remember clients' local data. Therefore, it's necessary\nto effectively remove the target client's data from the FL global model to ease\nthe risk of privacy leakage and implement ``the right to be forgotten\".\nFederated Unlearning (FU) has been considered a promising way to remove data\nwithout full retraining. But the model utility easily suffers significant\nreduction during unlearning due to the gradient conflicts. Furthermore, when\nconducting the post-training to recover the model utility, the model is prone\nto move back and revert what has already been unlearned. To address these\nissues, we propose Federated Unlearning with Orthogonal Steepest Descent\n(FedOSD). We first design an unlearning Cross-Entropy loss to overcome the\nconvergence issue of the gradient ascent. A steepest descent direction for\nunlearning is then calculated in the condition of being non-conflicting with\nother clients' gradients and closest to the target client's gradient. This\nbenefits to efficiently unlearn and mitigate the model utility reduction. After\nunlearning, we recover the model utility by maintaining the achievement of\nunlearning. Finally, extensive experiments in several FL scenarios verify that\nFedOSD outperforms the SOTA FU algorithms in terms of unlearning and model\nutility.\n","authors":["Zibin Pan","Zhichao Wang","Chi Li","Kaiyan Zheng","Boqi Wang","Xiaoying Tang","Junhua Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.20200v1.pdf","comment":"To be published in the Proceedings of the 39th AAAI Conference on\n  Artificial Intelligence (AAAI-25)"}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2412.20249v1","updated":"2024-12-28T19:34:31Z","published":"2024-12-28T19:34:31Z","title":"Next-Gen Interconnection Systems with Compute Express Link: a\n  Comprehensive Survey","summary":"  Interconnection is crucial for computing systems. However, the current\ninterconnection performance between processors and devices, such as memory\ndevices and accelerators, significantly lags behind their computing\nperformance, severely limiting the overall performance. To address this\nchallenge, Intel proposes Compute Express Link (CXL), an open industry-standard\ninterconnection. With memory semantics, CXL offers low-latency, scalable, and\ncoherent interconnection between processors and devices. This paper introduces\nrecent advances in CXL-based interconnection systems with memory semantics. We\nclassify the existing research into three categories: Pooling Memory,\nDistributed Shared Memory, and Unified Memory. Pooling Memory interconnects\nprocessors and memory, aims to address memory wall challenge. Distributed\nshared memory interconnects processors across nodes, aims to synchronize the\ncluster. Unified memory interconnects processors and accelerators, aims to\nenhance collaboration in heterogeneous computing systems. Finally, we discuss\nthe future research and envision memory-centric computing with CXL.\n","authors":["Chen Chen","Xinkui Zhao","Guanjie Cheng","Yuesheng Xu","Shuiguang Deng","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2412.20249v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2412.20221v1","updated":"2024-12-28T17:17:03Z","published":"2024-12-28T17:17:03Z","title":"Revisiting Cache Freshness for Emerging Real-Time Applications","summary":"  Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.\n","authors":["Ziming Mao","Rishabh Iyer","Scott Shenker","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2412.20221v1.pdf","comment":"HotNets '24"},{"id":"http://arxiv.org/abs/2412.20200v1","updated":"2024-12-28T16:23:10Z","published":"2024-12-28T16:23:10Z","title":"Federated Unlearning with Gradient Descent and Conflict Mitigation","summary":"  Federated Learning (FL) has received much attention in recent years. However,\nalthough clients are not required to share their data in FL, the global model\nitself can implicitly remember clients' local data. Therefore, it's necessary\nto effectively remove the target client's data from the FL global model to ease\nthe risk of privacy leakage and implement ``the right to be forgotten\".\nFederated Unlearning (FU) has been considered a promising way to remove data\nwithout full retraining. But the model utility easily suffers significant\nreduction during unlearning due to the gradient conflicts. Furthermore, when\nconducting the post-training to recover the model utility, the model is prone\nto move back and revert what has already been unlearned. To address these\nissues, we propose Federated Unlearning with Orthogonal Steepest Descent\n(FedOSD). We first design an unlearning Cross-Entropy loss to overcome the\nconvergence issue of the gradient ascent. A steepest descent direction for\nunlearning is then calculated in the condition of being non-conflicting with\nother clients' gradients and closest to the target client's gradient. This\nbenefits to efficiently unlearn and mitigate the model utility reduction. After\nunlearning, we recover the model utility by maintaining the achievement of\nunlearning. Finally, extensive experiments in several FL scenarios verify that\nFedOSD outperforms the SOTA FU algorithms in terms of unlearning and model\nutility.\n","authors":["Zibin Pan","Zhichao Wang","Chi Li","Kaiyan Zheng","Boqi Wang","Xiaoying Tang","Junhua Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.20200v1.pdf","comment":"To be published in the Proceedings of the 39th AAAI Conference on\n  Artificial Intelligence (AAAI-25)"},{"id":"http://arxiv.org/abs/2412.20151v1","updated":"2024-12-28T13:32:36Z","published":"2024-12-28T13:32:36Z","title":"Contention-Aware Microservice Deployment in Collaborative Mobile Edge\n  Networks","summary":"  As an emerging computing paradigm, mobile edge computing (MEC) provides\nprocessing capabilities at the network edge, aiming to reduce latency and\nimprove user experience. Meanwhile, the advancement of containerization\ntechnology facilitates the deployment of microservice-based applications via\nedge node collaboration, ensuring highly efficient service delivery. However,\nexisting research overlooks the resource contention among microservices in MEC.\nThis neglect potentially results in inadequate resources for microservices\nconstituting latency-sensitive applications, leading to increased response time\nand ultimately compromising quality of service (QoS). To solve this problem, we\npropose the Contention-Aware Multi-Application Microservice Deployment (CAMD)\nalgorithm for collaborative MEC, balancing rapid response for applications with\nlow-latency requirements and overall processing efficiency. The CAMD algorithm\ndecomposes the overall deployment problem into manageable sub-problems, each\nfocusing on a single microservice, then employs a heuristic approach to\noptimize these sub-problems, and ultimately arrives at an optimized deployment\nscheme through an iterative process. Finally, the superiority of the proposed\nalgorithm is evidenced through intensive experiments and comparison with\nbaseline algorithms.\n","authors":["Xinlei Ge","Yang Li","Xing Zhang","Yukun Sun","Yunji Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.20151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20020v1","updated":"2024-12-28T04:43:39Z","published":"2024-12-28T04:43:39Z","title":"Calibre: Towards Fair and Accurate Personalized Federated Learning with\n  Self-Supervised Learning","summary":"  In the context of personalized federated learning, existing approaches train\na global model to extract transferable representations, based on which any\nclient could train personalized models with a limited number of data samples.\nSelf-supervised learning is considered a promising direction as the global\nmodel it produces is generic and facilitates personalization for all clients\nfairly. However, when data is heterogeneous across clients, the global model\ntrained using SSL is unable to learn high-quality personalized models. In this\npaper, we show that when the global model is trained with SSL without\nmodifications, its produced representations have fuzzy class boundaries. As a\nresult, personalized learning within each client produces models with low\naccuracy. In order to improve SSL towards better accuracy without sacrificing\nits advantage in fairness, we propose Calibre, a new personalized federated\nlearning framework designed to calibrate SSL representations by maintaining a\nsuitable balance between more generic and more client-specific representations.\nCalibre is designed based on theoretically-sound properties, and introduces (1)\na client-specific prototype loss as an auxiliary training objective; and (2) an\naggregation algorithm guided by such prototypes across clients. Our\nexperimental results in an extensive array of non-i.i.d.~settings show that\nCalibre achieves state-of-the-art performance in terms of both mean accuracy\nand fairness across clients. Code repo:\nhttps://github.com/TL-System/plato/tree/main/examples/ssl/calibre.\n","authors":["Sijia Chen","Ningxin Su","Baochun Li"],"pdf_url":"https://arxiv.org/pdf/2412.20020v1.pdf","comment":"ICDCS camera-ready paper, Code repo:\n  https://github.com/TL-System/plato/tree/main/examples/ssl/calibre"},{"id":"http://arxiv.org/abs/2412.20004v1","updated":"2024-12-28T04:00:42Z","published":"2024-12-28T04:00:42Z","title":"Adaptive Parameter-Efficient Federated Fine-Tuning on Heterogeneous\n  Devices","summary":"  Federated fine-tuning (FedFT) has been proposed to fine-tune the pre-trained\nlanguage models in a distributed manner. However, there are two critical\nchallenges for efficient FedFT in practical applications, i.e., resource\nconstraints and system heterogeneity. Existing works rely on\nparameter-efficient fine-tuning methods, e.g., low-rank adaptation (LoRA), but\nwith major limitations. Herein, based on the inherent characteristics of FedFT,\nwe observe that LoRA layers with higher ranks added close to the output help to\nsave resource consumption while achieving comparable fine-tuning performance.\nThen we propose a novel LoRA-based FedFT framework, termed LEGEND, which faces\nthe difficulty of determining the number of LoRA layers (called, LoRA depth)\nand the rank of each LoRA layer (called, rank distribution). We analyze the\ncoupled relationship between LoRA depth and rank distribution, and design an\nefficient LoRA configuration algorithm for heterogeneous devices, thereby\npromoting fine-tuning efficiency. Extensive experiments are conducted on a\nphysical platform with 80 commercial devices. The results show that LEGEND can\nachieve a speedup of 1.5-2.8$\\times$ and save communication costs by about\n42.3% when achieving the target accuracy, compared to the advanced solutions.\n","authors":["Jun Liu","Yunming Liao","Hongli Xu","Yang Xu","Jianchun Liu","Chen Qian"],"pdf_url":"https://arxiv.org/pdf/2412.20004v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19991v1","updated":"2024-12-28T03:28:52Z","published":"2024-12-28T03:28:52Z","title":"A Robust Federated Learning Framework for Undependable Devices at Scale","summary":"  In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.\n","authors":["Shilong Wang","Jianchun Liu","Hongli Xu","Chunming Qiao","Huarong Deng","Qiuye Zheng","Jiantao Gong"],"pdf_url":"https://arxiv.org/pdf/2412.19991v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19989v1","updated":"2024-12-28T03:20:36Z","published":"2024-12-28T03:20:36Z","title":"Caesar: A Low-deviation Compression Approach for Efficient Federated\n  Learning","summary":"  Compression is an efficient way to relieve the tremendous communication\noverhead of federated learning (FL) systems. However, for the existing works,\nthe information loss under compression will lead to unexpected model/gradient\ndeviation for the FL training, significantly degrading the training\nperformance, especially under the challenges of data heterogeneity and model\nobsolescence. To strike a delicate trade-off between model accuracy and traffic\ncost, we propose Caesar, a novel FL framework with a low-deviation compression\napproach. For the global model download, we design a greedy method to optimize\nthe compression ratio for each device based on the staleness of the local\nmodel, ensuring a precise initial model for local training. Regarding the local\ngradient upload, we utilize the device's local data properties (\\ie, sample\nvolume and label distribution) to quantify its local gradient's importance,\nwhich then guides the determination of the gradient compression ratio. Besides,\nwith the fine-grained batch size optimization, Caesar can significantly\ndiminish the devices' idle waiting time under the synchronized barrier. We have\nimplemented Caesar on two physical platforms with 40 smartphones and 80 NVIDIA\nJetson devices. Extensive results show that Caesar can reduce the traffic costs\nby about 25.54%$\\thicksim$37.88% compared to the compression-based baselines\nwith the same target accuracy, while incurring only a 0.68% degradation in\nfinal test accuracy relative to the full-precision communication.\n","authors":["Jiaming Yan","Jianchun Liu","Hongli Xu","Liusheng Huang","Jiantao Gong","Xudong Liu","Kun Hou"],"pdf_url":"https://arxiv.org/pdf/2412.19989v1.pdf","comment":"12 pages, 27 figures"},{"id":"http://arxiv.org/abs/2412.19987v1","updated":"2024-12-28T03:14:27Z","published":"2024-12-28T03:14:27Z","title":"Delayed Random Partial Gradient Averaging for Federated Learning","summary":"  Federated learning (FL) is a distributed machine learning paradigm that\nenables multiple clients to train a shared model collaboratively while\npreserving privacy. However, the scaling of real-world FL systems is often\nlimited by two communication bottlenecks:(a) while the increasing computing\npower of edge devices enables the deployment of large-scale Deep Neural\nNetworks (DNNs), the limited bandwidth constraints frequent transmissions over\nlarge DNNs; and (b) high latency cost greatly degrades the performance of FL.\nIn light of these bottlenecks, we propose a Delayed Random Partial Gradient\nAveraging (DPGA) to enhance FL. Under DPGA, clients only share partial local\nmodel gradients with the server. The size of the shared part in a local model\nis determined by the update rate, which is coarsely initialized and\nsubsequently refined over the temporal dimension. Moreover, DPGA largely\nreduces the system run time by enabling computation in parallel with\ncommunication. We conduct experiments on non-IID CIFAR-10/100 to demonstrate\nthe efficacy of our method.\n","authors":["Xinyi Hu"],"pdf_url":"https://arxiv.org/pdf/2412.19987v1.pdf","comment":null}],"Operating Systems":[{"id":"http://arxiv.org/abs/2412.20221v1","updated":"2024-12-28T17:17:03Z","published":"2024-12-28T17:17:03Z","title":"Revisiting Cache Freshness for Emerging Real-Time Applications","summary":"  Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.\n","authors":["Ziming Mao","Rishabh Iyer","Scott Shenker","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2412.20221v1.pdf","comment":"HotNets '24"}],"Information Theory":[{"id":"http://arxiv.org/abs/2412.20241v1","updated":"2024-12-28T18:39:05Z","published":"2024-12-28T18:39:05Z","title":"A Hybrid Quantum-Classical Autoencoder Framework for End-to-End\n  Communication Systems","summary":"  This paper investigates the application of quantum machine learning to\nEnd-to-End (E2E) communication systems in wireless fading scenarios. We\nintroduce a novel hybrid quantum-classical autoencoder architecture that\ncombines parameterized quantum circuits with classical deep neural networks\n(DNNs). Specifically, we propose a hybrid quantum-classical autoencoder (QAE)\nframework to optimize the E2E communication system. Our results demonstrate the\nfeasibility of the proposed hybrid system, and reveal that it is the first work\nthat can achieve comparable block error rate (BLER) performance to classical\nDNN-based and conventional channel coding schemes, while significantly reducing\nthe number of trainable parameters. Additionally, the proposed QAE exhibits\nsteady and superior BLER convergence over the classical autoencoder baseline.\n","authors":["Bolun Zhang","Gan Zheng","Nguyen Van Huynh"],"pdf_url":"https://arxiv.org/pdf/2412.20241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09379v2","updated":"2024-12-28T15:47:34Z","published":"2024-08-18T06:36:52Z","title":"Zak-OTFS with Interleaved Pilots to Extend the Region of Predictable\n  Operation","summary":"  When the delay period of the Zak-OTFS carrier is greater than the delay\nspread of the channel, and the Doppler period of the carrier is greater than\nthe Doppler spread of the channel, the effective channel filter taps can simply\nbe read off from the response to a single pilot carrier waveform. The\ninput-output (I/O) relation can then be reconstructed for a sampled system that\noperates under finite duration and bandwidth constraints. We introduce a\nframework for pilot design in the delay-Doppler (DD) domain which makes it\npossible to support users with very different delay-Doppler characteristics\nwhen it is not possible to choose a single delay and Doppler period to support\nall users. The method is to interleave single pilots in the DD domain, and to\nchoose the pilot spacing so that the I/O relation can be reconstructed by\nsolving a small linear system of equations.\n","authors":["Jinu Jayachandran","Imran Ali Khan","Saif Khan Mohammed","Ronny Hadani","Ananthanarayanan Chockalingam","Robert Calderbank"],"pdf_url":"https://arxiv.org/pdf/2408.09379v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2407.05845v2","updated":"2024-12-28T14:44:16Z","published":"2024-07-08T11:44:46Z","title":"Linear Complementary dual codes and Linear Complementary pairs of AG\n  codes in function fields","summary":"  In recent years, linear complementary pairs (LCP) of codes and linear\ncomplementary dual (LCD) codes have gained significant attention due to their\napplications in coding theory and cryptography. In this work, we construct\nexplicit LCPs of codes and LCD codes from function fields of genus $g \\geq 1$.\nTo accomplish this, we present pairs of suitable divisors giving rise to\nnon-special divisors of degree $g-1$ in the function field. The results are\napplied in constructing LCPs of algebraic geometry codes and LCD algebraic\ngeometry (AG) codes in Kummer extensions, hyperelliptic function fields, and\nelliptic curves.\n","authors":["Alonso S. Castellanos","Adler V. Marques","Luciane Quoos"],"pdf_url":"https://arxiv.org/pdf/2407.05845v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20083v1","updated":"2024-12-28T08:43:36Z","published":"2024-12-28T08:43:36Z","title":"Achieving Full-Bandwidth Sensing Performance with Partial Bandwidth\n  Allocation for ISAC","summary":"  This letter studies an uplink integrated sensing and communication (ISAC)\nsystem using discrete Fourier transform spread orthogonal frequency division\nmultiplexing (DFT-s-OFDM) transmission. We try to answer the following\nfundamental question: With only a fractional bandwidth allocated to the user\nwith sensing task, can the same delay resolution and unambiguous range be\nachieved as if all bandwidth were allocated to it? We affirmatively answer the\nquestion by proposing a novel two-stage delay estimation (TSDE) method that\nexploits the following facts: without increasing the allocated bandwidth,\nhigher delay resolution can be achieved via distributed subcarrier allocation\ncompared to its collocated counterpart, while there is a trade-off between\ndelay resolution and unambiguous range by varying the decimation factor of\nsubcarriers. Therefore, the key idea of the proposed TSDE method is to first\nperform coarse delay estimation with collocated subcarriers to achieve a large\nunambiguous range, and then use distributed subcarriers with optimized\ndecimation factor to enhance delay resolution while avoiding delay ambiguity.\nOur analysis shows that the proposed TSDE method can achieve the full-bandwidth\ndelay resolution and unambiguous range, by using only at most half of the full\nbandwidth, provided that the channel delay spread is less than half of the\nunambiguous range. Numerical results show the superiority of the proposed\nmethod over the conventional method with collocated subcarriers.\n","authors":["Zhiqiang Xiao","Zhiwen Zhou","Qianglong Dai","Yong Zeng","Fei Yang","Yan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.20083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19979v1","updated":"2024-12-28T02:45:15Z","published":"2024-12-28T02:45:15Z","title":"Explainable Semantic Federated Learning Enabled Industrial Edge Network\n  for Fire Surveillance","summary":"  In fire surveillance, Industrial Internet of Things (IIoT) devices require\ntransmitting large monitoring data frequently, which leads to huge consumption\nof spectrum resources. Hence, we propose an Industrial Edge Semantic Network\n(IESN) to allow IIoT devices to send warnings through Semantic communication\n(SC). Thus, we should consider (1) Data privacy and security. (2) SC model\nadaptation for heterogeneous devices. (3) Explainability of semantics.\nTherefore, first, we present an eXplainable Semantic Federated Learning (XSFL)\nto train the SC model, thus ensuring data privacy and security. Then, we\npresent an Adaptive Client Training (ACT) strategy to provide a specific SC\nmodel for each device according to its Fisher information matrix, thus\novercoming the heterogeneity. Next, an Explainable SC (ESC) mechanism is\ndesigned, which introduces a leakyReLU-based activation mapping to explain the\nrelationship between the extracted semantics and monitoring data. Finally,\nsimulation results demonstrate the effectiveness of XSFL.\n","authors":["Li Dong","Yubo Peng","Feibo Jiang","Kezhi Wang","Kun Yang"],"pdf_url":"https://arxiv.org/pdf/2412.19979v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2412.19973v1","updated":"2024-12-28T02:05:13Z","published":"2024-12-28T02:05:13Z","title":"An Overview of Cellular ISAC for Low-Altitude UAV: New Opportunities and\n  Challenges","summary":"  Low-altitude unmanned aerial vehicles (UAVs) are expected to play an\nimportant role in future wireless networks, either as aerial base stations\n(BSs) or aerial users connected to the cellular network. In addition,\nintegrated sensing and communication (ISAC) has been identified as one of the\nsix usage scenarios for the forthcoming sixth-generation (6G) mobile networks,\naimed at improving network functionalities and realizing situational awareness\nof the physical world. While most existing research efforts focus on\nterrestrial two-dimensional (2D) communication and sensing, UAV as an aerial\nplatform offers a new degree of freedom for designing three-dimensional (3D)\nair-ground (AG) ISAC networks. In this article, we provide an overview of\ncellular-connected UAV ISAC, by elaborating the UAV's roles as a target to be\nsensed and as an aerial anchor to provide sensing functionality, respectively.\nIn particular, we pay attention to the network coverage issue and topics\nspecific to UAV networking, emphasizing the new opportunities as well as unique\nchallenges to be addressed.\n","authors":["Yuxuan Song","Yong Zeng","Yuhang Yang","Zixiang Ren","Gaoyuan Cheng","Xiaoli Xu","Jie Xu","Shi Jin","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.19973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01952v4","updated":"2024-12-28T00:05:04Z","published":"2023-03-03T14:25:18Z","title":"Quantum state testing beyond the polarizing regime and quantum\n  triangular discrimination","summary":"  The complexity class Quantum Statistical Zero-Knowledge ($\\mathsf{QSZK}$)\ncaptures computational difficulties of the time-bounded quantum state testing\nproblem with respect to the trace distance, known as the Quantum State\nDistinguishability Problem (QSDP) introduced by Watrous (FOCS 2002). However,\nQSDP is in $\\mathsf{QSZK}$ merely within the constant polarizing regime,\nsimilar to its classical counterpart shown by Sahai and Vadhan (JACM 2003) due\nto the polarization lemma (error reduction for SDP).\n  Recently, Berman, Degwekar, Rothblum, and Vasudevan (TCC 2019) extended the\n$\\mathsf{SZK}$ containment for SDP beyond the polarizing regime via the\ntime-bounded distribution testing problems with respect to the triangular\ndiscrimination and the Jensen-Shannon divergence. Our work introduces proper\nquantum analogs for these problems by defining quantum counterparts for\ntriangular discrimination. We investigate whether the quantum analogs behave\nsimilarly to their classical counterparts and examine the limitations of\nexisting approaches to polarization regarding quantum distances. These new\n$\\mathsf{QSZK}$-complete problems improve $\\mathsf{QSZK}$ containments for QSDP\nbeyond the polarizing regime and establish a simple $\\mathsf{QSZK}$-hardness\nfor the quantum entropy difference problem (QEDP) defined by Ben-Aroya,\nSchwartz, and Ta-Shma (ToC 2010). Furthermore, we prove that QSDP with some\nexponentially small errors is in $\\mathsf{PP}$, while the same problem without\nerror is in $\\mathsf{NQP}$.\n","authors":["Yupan Liu"],"pdf_url":"https://arxiv.org/pdf/2303.01952v4.pdf","comment":"32 pages. v4: added a polarization lemma for QTD, and minor changes.\n  v3: added a simple QSZK-hardness proof for QEDP, updated a correct version of\n  Theorem 5.1(2), and improved presentation. v2: minor changes"}],"Signal Processing":[{"id":"http://arxiv.org/abs/2412.20254v1","updated":"2024-12-28T19:58:39Z","published":"2024-12-28T19:58:39Z","title":"An Optimization Driven Link SINR Assurance in RIS-assisted Indoor\n  Networks","summary":"  Future smart factories are expected to deploy applications over\nhigh-performance indoor wireless channels in the millimeter-wave (mmWave)\nbands, which on the other hand are susceptible to high path losses and Line-of\nSight (LoS) blockages. Low-cost Reconfigurable Intelligent Surfaces (RISs) can\nprovide great opportunities in such scenarios, due to its ability to alleviate\nLoS link blockages. In this paper, we formulate a combinatorial optimization\nproblem, solved with Integer Linear Programming (ILP) to optimally maintain\nconnectivity by solving the problem of allocating RIS to robots in a wireless\nindoor network. Our model exploits the characteristic of nulling interference\nfrom RISs by tuning RIS reflection coefficients. We further consider\nQuality-of-Service (QoS) at receivers in terms of\nSignal-to-Interference-plus-Noise Ratio (SINR) and connection outages due to\ninsufficient transmission quality service. Numerical results for optimal\nsolutions and heuristics show the benefits of optimally deploying RISs by\nproviding continuous connectivity through SINR, which significantly reduces\noutages due to link quality.\n","authors":["Cao Vien Phung","Max Franke","Ehsan Tohidi","June Heinemann","Andre Drummond","Stefan Schmid","Slawomir Stanczak","Admela Jukan"],"pdf_url":"https://arxiv.org/pdf/2412.20254v1.pdf","comment":"This paper is uploaded here for research community, thus it is for\n  non-commercial purposes"},{"id":"http://arxiv.org/abs/2412.20245v1","updated":"2024-12-28T19:14:37Z","published":"2024-12-28T19:14:37Z","title":"Machine-Learning Enabled Multidimensional Data Utilization in\n  Multi-resonance Biosensors: A Pathway to Enhanced Accuracy","summary":"  A novel framework is proposed that combines multi-resonance biosensors with\nmachine learning (ML) to significantly enhance the accuracy of parameter\nprediction in biosensing. Unlike traditional single-resonance systems, which\nare limited to one-dimensional datasets, this approach leverages\nmulti-dimensional data generated by a custom-designed nanostructure, a periodic\narray of silicon nanorods with a triangular cross-section over an aluminum\nreflector. High bulk sensitivity values are achieved for this multi-resonant\nstructure, with certain resonant peaks reaching up to 1706 nm/RIU. The\npredictive power of multiple resonant peaks from transverse magnetic (TM) and\ntransverse electric (TE) polarizations is evaluated using Ridge Regression\nmodeling. Systematic analysis reveals that incorporating multiple resonances\nyields up to three orders of magnitude improvement in refractive index\ndetection precision compared to single-peak analyses. This precision\nenhancement is achieved without modifications to the biosensor hardware,\nhighlighting the potential of data-centric strategies in biosensing. The\nfindings establish a new paradigm in biosensing, demonstrating that the synergy\nbetween multi-resonance data acquisition and ML-based analysis can\nsignificantly enhance detection accuracy. This study provides a scalable\npathway for advancing high-precision biosensing technologies.\n","authors":["Majid Aalizadeh","Morteza Azmoudeh Afshar","Xudong Fan"],"pdf_url":"https://arxiv.org/pdf/2412.20245v1.pdf","comment":"31 pages total. References are before supplementary information at\n  page 19. Supplementary information are placed after references at the end of\n  the manuscript"},{"id":"http://arxiv.org/abs/2412.20241v1","updated":"2024-12-28T18:39:05Z","published":"2024-12-28T18:39:05Z","title":"A Hybrid Quantum-Classical Autoencoder Framework for End-to-End\n  Communication Systems","summary":"  This paper investigates the application of quantum machine learning to\nEnd-to-End (E2E) communication systems in wireless fading scenarios. We\nintroduce a novel hybrid quantum-classical autoencoder architecture that\ncombines parameterized quantum circuits with classical deep neural networks\n(DNNs). Specifically, we propose a hybrid quantum-classical autoencoder (QAE)\nframework to optimize the E2E communication system. Our results demonstrate the\nfeasibility of the proposed hybrid system, and reveal that it is the first work\nthat can achieve comparable block error rate (BLER) performance to classical\nDNN-based and conventional channel coding schemes, while significantly reducing\nthe number of trainable parameters. Additionally, the proposed QAE exhibits\nsteady and superior BLER convergence over the classical autoencoder baseline.\n","authors":["Bolun Zhang","Gan Zheng","Nguyen Van Huynh"],"pdf_url":"https://arxiv.org/pdf/2412.20241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.09379v2","updated":"2024-12-28T15:47:34Z","published":"2024-08-18T06:36:52Z","title":"Zak-OTFS with Interleaved Pilots to Extend the Region of Predictable\n  Operation","summary":"  When the delay period of the Zak-OTFS carrier is greater than the delay\nspread of the channel, and the Doppler period of the carrier is greater than\nthe Doppler spread of the channel, the effective channel filter taps can simply\nbe read off from the response to a single pilot carrier waveform. The\ninput-output (I/O) relation can then be reconstructed for a sampled system that\noperates under finite duration and bandwidth constraints. We introduce a\nframework for pilot design in the delay-Doppler (DD) domain which makes it\npossible to support users with very different delay-Doppler characteristics\nwhen it is not possible to choose a single delay and Doppler period to support\nall users. The method is to interleave single pilots in the DD domain, and to\nchoose the pilot spacing so that the I/O relation can be reconstructed by\nsolving a small linear system of equations.\n","authors":["Jinu Jayachandran","Imran Ali Khan","Saif Khan Mohammed","Ronny Hadani","Ananthanarayanan Chockalingam","Robert Calderbank"],"pdf_url":"https://arxiv.org/pdf/2408.09379v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2412.20184v1","updated":"2024-12-28T15:47:04Z","published":"2024-12-28T15:47:04Z","title":"Frames and vertex-frequency representations in graph fractional Fourier\n  domain","summary":"  Vertex-frequency analysis, particularly the windowed graph Fourier transform\n(WGFT), is a significant challenge in graph signal processing. Tight frame\ntheories is known for its low computational complexity in signal\nreconstruction, while fractional order methods shine at unveil more detailed\nstructural characteristics of graph signals. In the graph fractional Fourier\ndomain, we introduce multi-windowed graph fractional Fourier frames (MWGFRFF)\nto facilitate the construction of tight frames. This leads to developing the\nmulti-windowed graph fractional Fourier transform (MWGFRFT), enabling novel\nvertex-frequency analysis methods. A reconstruction formula is derived, along\nwith results concerning dual and tight frames. To enhance computational\nefficiency, a fast MWGFRFT (FMWGFRFT) algorithm is proposed. Furthermore, we\ndefine shift multi-windowed graph fractional Fourier frames (SMWGFRFF) and\ntheir associated transform (SMWGFRFT), exploring their dual and tight frames.\nExperimental results indicate that FMWGFRFT and SMWGFRFT excel in extracting\nvertex-frequency features in the graph fractional Fourier domain, with their\ncombined use optimizing analytical performance. Applications in signal anomaly\ndetection demonstrate the advantages of FMWGFRFT.\n","authors":["Linbo Shang","Zhichao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.20184v1.pdf","comment":"13 pages, 11 figures"},{"id":"http://arxiv.org/abs/2412.20146v1","updated":"2024-12-28T13:14:30Z","published":"2024-12-28T13:14:30Z","title":"Bird Vocalization Embedding Extraction Using Self-Supervised\n  Disentangled Representation Learning","summary":"  This paper addresses the extraction of the bird vocalization embedding from\nthe whole song level using disentangled representation learning (DRL). Bird\nvocalization embeddings are necessary for large-scale bioacoustic tasks, and\nself-supervised methods such as Variational Autoencoder (VAE) have shown their\nperformance in extracting such low-dimensional embeddings from vocalization\nsegments on the note or syllable level. To extend the processing level to the\nentire song instead of cutting into segments, this paper regards each\nvocalization as the generalized and discriminative part and uses two encoders\nto learn these two parts. The proposed method is evaluated on the Great Tits\ndataset according to the clustering performance, and the results outperform the\ncompared pre-trained models and vanilla VAE. Finally, this paper analyzes the\ninformative part of the embedding, further compresses its dimension, and\nexplains the disentangled performance of bird vocalizations.\n","authors":["Runwu Shi","Katsutoshi Itoyama","Kazuhiro Nakadai"],"pdf_url":"https://arxiv.org/pdf/2412.20146v1.pdf","comment":"Presented on Vocal Interactivity in-and-between Humans, Animals and\n  Robots (VIHAR 2024),\n  https://vihar-2024.vihar.org/assets/VIHAR_2024_proceedings.pdf"},{"id":"http://arxiv.org/abs/2412.20142v1","updated":"2024-12-28T13:10:50Z","published":"2024-12-28T13:10:50Z","title":"ASE: Practical Acoustic Speed Estimation Beyond Doppler via Sound\n  Diffusion Field","summary":"  Passive human speed estimation plays a critical role in acoustic sensing.\nDespite extensive study, existing systems, however, suffer from various\nlimitations: First, previous acoustic speed estimation exploits Doppler\nFrequency Shifts (DFS) created by moving targets and relies on microphone\narrays, making them only capable of sensing the radial speed within a\nconstrained distance. Second, the channel measurement rate proves inadequate to\nestimate high moving speeds. To overcome these issues, we present ASE, an\naccurate and robust Acoustic Speed Estimation system on a single commodity\nmicrophone. We model the sound propagation from a unique perspective of the\nacoustic diffusion field, and infer the speed from the acoustic spatial\ndistribution, a completely different way of thinking about speed estimation\nbeyond prior DFS-based approaches. We then propose a novel Orthogonal\nTime-Delayed Multiplexing (OTDM) scheme for acoustic channel estimation at a\nhigh rate that was previously infeasible, making it possible to estimate high\nspeeds. We further develop novel techniques for motion detection and signal\nenhancement to deliver a robust and practical system. We implement and evaluate\nASE through extensive real-world experiments. Our results show that ASE\nreliably tracks walking speed, independently of target location and direction,\nwith a mean error of 0.13 m/s, a reduction of 2.5x from DFS, and a detection\nrate of 97.4% for large coverage, e.g., free walking in a 4m $\\times$ 4m room.\nWe believe ASE pushes acoustic speed estimation beyond the conventional\nDFS-based paradigm and will inspire exciting research in acoustic sensing.\n","authors":["Sheng Lyu","Chenshu Wu"],"pdf_url":"https://arxiv.org/pdf/2412.20142v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11446v3","updated":"2024-12-28T12:25:12Z","published":"2024-06-17T12:00:49Z","title":"An Approximate Wave-Number Domain Expression for Near-Field XL-array\n  Channel","summary":"  As Extremely large-scale array (XL-array) technology advances and carrier\nfrequency rises, the near-field effects in communication are intensifying. In\nnear-field conditions, channels exhibit a diffusion phenomenon in the angular\ndomain, existing research indicates that this phenomenon can be leveraged for\nefficient parameter estimation and beam training. However, the channel model in\nangular domain lacks closed-form analysis, making the time complexity of the\ncorresponding algorithm high. To address this issue, this paper analyzes the\nnear-field diffusion effect in the wave-number domain, where the wave-number\ndomain can be viewed as the continuous form of the angular domain. A\nclosed-form approximate wave-number domain expression is proposed, based on the\nPrinciple of Stationary Phase. Subsequently, we derive a simplified expression\nfor the case where the user distance is much larger than the array aperture,\nwhich is more concise. Subsequently, we verify the accuracy of the proposed\napproximate expression through simulations and demonstrate its effectiveness\nusing a beam training example. Results indicate that the beam training scheme,\nimproved by the wave-number domain approximation model, can effectively\nestimate near-field user parameters and perform beam training using far-field\nDFT codebooks. Moreover, its performance surpasses that of existing DFT\ncodebook-based beam training methods.\n","authors":["Hongbo Xing","Yuxiang Zhang","Jianhua Zhang","Huixin Xu","Guangyi Liu","Qixing Wang"],"pdf_url":"https://arxiv.org/pdf/2406.11446v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.15407v2","updated":"2024-12-28T11:30:38Z","published":"2023-10-23T23:39:47Z","title":"Finite-Time Adaptive Fuzzy Tracking Control for Nonlinear State\n  Constrained Pure-Feedback Systems","summary":"  This paper investigates the finite-time adaptive fuzzy tracking control\nproblem for a class of pure-feedback system with full-state constraints. With\nthe help of Mean-Value Theorem, the pure-feedback nonlinear system is\ntransformed into strict-feedback case. By employing finite-time-stable like\nfunction and state transformation for output tracking error, the output\ntracking error converges to a predefined set in a fixed finite interval. To\ntackle the problem of state constraints, integral Barrier Lyapunov functions\nare utilized to guarantee that the state variables remain within the prescribed\nconstraints with feasibility check. Fuzzy logic systems are utilized to\napproximate the unknown nonlinear functions. In addition, all the signals in\nthe closed-loop system are guaranteed to be semi-global ultimately uniformly\nbounded. Finally, two simulation examples are given to show the effectiveness\nof the proposed control strategy.\n","authors":["Ju Wu","Tong Wang","Min Ma"],"pdf_url":"https://arxiv.org/pdf/2310.15407v2.pdf","comment":"typos checked and corrected in 'Introduction'"},{"id":"http://arxiv.org/abs/2402.10220v2","updated":"2024-12-28T08:59:38Z","published":"2024-01-19T22:45:55Z","title":"Towards Semi-Autonomous Robotic Arm Manipulation Operator Intention\n  Detection from Force Data","summary":"  In hazardous environments like nuclear facilities, robotic systems are\nessential for executing tasks that would otherwise expose humans to dangerous\nradiation levels, which pose severe health risks and can be fatal. However,\nmany operations in the nuclear environment require teleoperating robots,\nresulting in a significant cognitive load on operators as well as physical\nstrain over extended periods of time. To address this challenge, we propose\nenhancing the teleoperation system with an assistive model capable of\npredicting operator intentions and dynamically adapting to their needs. The\nmachine learning model processes robotic arm force data, analyzing\nspatiotemporal patterns to accurately detect the ongoing task before its\ncompletion. To support this approach, we collected a diverse dataset from\nteleoperation experiments involving glovebox tasks in nuclear applications.\nThis dataset encompasses heterogeneous spatiotemporal data captured from the\nteleoperation system. We employ a hybrid Convolutional Neural Network (CNN) and\nLong Short-Term Memory (LSTM) model to learn and forecast operator intentions\nbased on the spatiotemporal data. By accurately predicting these intentions,\nthe robot can execute tasks more efficiently and effectively, requiring minimal\ninput from the operator. Our experiments validated the model using the dataset,\nfocusing on tasks such as radiation surveys and object grasping. The proposed\napproach demonstrated an F1-score of 89% for task classification and an\nF1-score of 86% classification forecasted operator intentions over a 5-second\nwindow. These results highlight the potential of our method to improve the\nsafety, precision, and efficiency of robotic operations in hazardous\nenvironments, thereby significantly reducing human radiation exposure.\n","authors":["Abdullah Alharthi","Ozan Tokatli","Erwin Lopez","Guido Herrmann"],"pdf_url":"https://arxiv.org/pdf/2402.10220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20083v1","updated":"2024-12-28T08:43:36Z","published":"2024-12-28T08:43:36Z","title":"Achieving Full-Bandwidth Sensing Performance with Partial Bandwidth\n  Allocation for ISAC","summary":"  This letter studies an uplink integrated sensing and communication (ISAC)\nsystem using discrete Fourier transform spread orthogonal frequency division\nmultiplexing (DFT-s-OFDM) transmission. We try to answer the following\nfundamental question: With only a fractional bandwidth allocated to the user\nwith sensing task, can the same delay resolution and unambiguous range be\nachieved as if all bandwidth were allocated to it? We affirmatively answer the\nquestion by proposing a novel two-stage delay estimation (TSDE) method that\nexploits the following facts: without increasing the allocated bandwidth,\nhigher delay resolution can be achieved via distributed subcarrier allocation\ncompared to its collocated counterpart, while there is a trade-off between\ndelay resolution and unambiguous range by varying the decimation factor of\nsubcarriers. Therefore, the key idea of the proposed TSDE method is to first\nperform coarse delay estimation with collocated subcarriers to achieve a large\nunambiguous range, and then use distributed subcarriers with optimized\ndecimation factor to enhance delay resolution while avoiding delay ambiguity.\nOur analysis shows that the proposed TSDE method can achieve the full-bandwidth\ndelay resolution and unambiguous range, by using only at most half of the full\nbandwidth, provided that the channel delay spread is less than half of the\nunambiguous range. Numerical results show the superiority of the proposed\nmethod over the conventional method with collocated subcarriers.\n","authors":["Zhiqiang Xiao","Zhiwen Zhou","Qianglong Dai","Yong Zeng","Fei Yang","Yan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.20083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20069v1","updated":"2024-12-28T07:47:56Z","published":"2024-12-28T07:47:56Z","title":"Amplitude-to-Phase Conversion in Injection-Locked CMOS Ring Oscillators","summary":"  Injection-locked ring oscillators (ILROs) are extensively employed for\nmulti-phase clock generation in wireline and optical links. However, existing\ninjection-locking theorems primarily rely on linearized phase-domain or\nnonlinear time-domain models, which fail to account for amplitude-to-phase\nconversion effects inherent in ILROs. This paper introduces an enhanced\nanalytical model based on an extension of Adler's equation, explicitly\nincorporating amplitude-to-phase conversion. Simulation results demonstrate\nstrong alignment with the proposed analytical predictions, validating the\nmodel's accuracy in capturing the locking range and phasor relationships.\n","authors":["Zhaowen Wang"],"pdf_url":"https://arxiv.org/pdf/2412.20069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16060v2","updated":"2024-12-28T07:37:31Z","published":"2023-10-23T23:31:47Z","title":"Adaptive Fuzzy Tracking Control for Nonlinear State Constrained\n  Pure-Feedback Systems With Input Delay via Dynamic Surface Technique","summary":"  This brief constructs the adaptive backstepping control scheme for a class of\npure-feedback systems with input delay and full state constraints. With the\nhelp of Mean Value Theorem, the pure-feedback system is transformed into\nstrict-feedback one. Barrier Lyapunov functions are employed to guarantee all\nof the states remain constrained within predefined sets. By introducing the\nPade approximation method and corresponding intermediate, the impact generated\nby input delay on the output tracking performance of the system can be\neliminated. Furthermore, a low-pass filter driven by a newly-defined control\ninput, is employed to generate the actual control input, which facilitates the\ndesign of backstepping control. To approximate the unknown functions with a\ndesired level of accuracy, the fuzzy logic systems (FLSs) are utilized by\nchoosing appropriate fuzzy rules, logics and so on. The minimal learning\nparameter (MLP) technique is employed to decrease the number of nodes and\nparameters in FLSs, and dynamic surface control (DSC) technique is leveraged to\navoid so-called \"explosion of complexity\". Moreover, smooth robust compensators\nare introduced to circumvent the influences of external disturbance and\napproximation errors. By stability analysis, it is proved that all of signals\nin the closed-loop system are semi-globally ultimately uniform bounded, and the\ntracking error can be within a arbitrary small neighbor of origin via selecting\nappropriate parameters of controllers. Finally, the results of numerical\nillustration are provided to demonstrate the effectiveness of the designed\nmethod.\n","authors":["Ju Wu","Tong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.16060v2.pdf","comment":"typos checked and corrected, redo literature review to update\n  introduction part"},{"id":"http://arxiv.org/abs/2412.20060v1","updated":"2024-12-28T07:27:51Z","published":"2024-12-28T07:27:51Z","title":"Self-Calibrated Dual Contrasting for Annotation-Efficient Bacteria Raman\n  Spectroscopy Clustering and Classification","summary":"  Raman scattering is based on molecular vibration spectroscopy and provides a\npowerful technology for pathogenic bacteria diagnosis using the unique\nmolecular fingerprint information of a substance. The integration of deep\nlearning technology has significantly improved the efficiency and accuracy of\nintelligent Raman spectroscopy (RS) recognition. However, the current RS\nrecognition methods based on deep neural networks still require the annotation\nof a large amount of spectral data, which is labor-intensive. This paper\npresents a novel annotation-efficient Self-Calibrated Dual Contrasting (SCDC)\nmethod for RS recognition that operates effectively with few or no annotation.\nOur core motivation is to represent the spectrum from two different\nperspectives in two distinct subspaces: embedding and category. The embedding\nperspective captures instance-level information, while the category perspective\nreflects category-level information. Accordingly, we have implemented a dual\ncontrastive learning approach from two perspectives to obtain discriminative\nrepresentations, which are applicable for Raman spectroscopy recognition under\nboth unsupervised and semi-supervised learning conditions. Furthermore, a\nself-calibration mechanism is proposed to enhance robustness. Validation of the\nidentification task on three large-scale bacterial Raman spectroscopy datasets\ndemonstrates that our SCDC method achieves robust recognition performance with\nvery few (5$\\%$ or 10$\\%$) or no annotations, highlighting the potential of the\nproposed method for biospectral identification in annotation-efficient clinical\nscenarios.\n","authors":["Haiming Yao","Wei Luo","Tao Zhou","Ang Gao","Xue Wang"],"pdf_url":"https://arxiv.org/pdf/2412.20060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20048v1","updated":"2024-12-28T06:32:49Z","published":"2024-12-28T06:32:49Z","title":"CrossSpeech++: Cross-lingual Speech Synthesis with Decoupled Language\n  and Speaker Generation","summary":"  The goal of this work is to generate natural speech in multiple languages\nwhile maintaining the same speaker identity, a task known as cross-lingual\nspeech synthesis. A key challenge of cross-lingual speech synthesis is the\nlanguage-speaker entanglement problem, which causes the quality of\ncross-lingual systems to lag behind that of intra-lingual systems. In this\npaper, we propose CrossSpeech++, which effectively disentangles language and\nspeaker information and significantly improves the quality of cross-lingual\nspeech synthesis. To this end, we break the complex speech generation pipeline\ninto two simple components: language-dependent and speaker-dependent\ngenerators. The language-dependent generator produces linguistic variations\nthat are not biased by specific speaker attributes. The speaker-dependent\ngenerator models acoustic variations that characterize speaker identity. By\nhandling each type of information in separate modules, our method can\neffectively disentangle language and speaker representation. We conduct\nextensive experiments using various metrics, and demonstrate that CrossSpeech++\nachieves significant improvements in cross-lingual speech synthesis,\noutperforming existing methods by a large margin.\n","authors":["Ji-Hoon Kim","Hong-Sun Yang","Yoon-Cheol Ju","Il-Hwan Kim","Byeong-Yeol Kim","Joon Son Chung"],"pdf_url":"https://arxiv.org/pdf/2412.20048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20041v1","updated":"2024-12-28T06:12:37Z","published":"2024-12-28T06:12:37Z","title":"On Random Sampling of Diffused Graph Signals with Sparse Inputs on\n  Vertex Domain","summary":"  The sampling of graph signals has recently drawn much attention due to the\nwide applications of graph signal processing. While a lot of efficient methods\nand interesting results have been reported to the sampling of band-limited or\nsmooth graph signals, few research has been devoted to non-smooth graph\nsignals, especially to sparse graph signals, which are also of importance in\nmany practical applications. This paper addresses the random sampling of\nnon-smooth graph signals generated by diffusion of sparse inputs. We aim to\npresent a solid theoretical analysis on the random sampling of diffused sparse\ngraph signals, which can be parallel to that of band-limited graph signals, and\nthus present a sufficient condition to the number of samples ensuring the\nunique recovery for uniform random sampling. Then, we focus on two classes of\nwidely used binary graph models, and give explicit and tighter estimations on\nthe sampling numbers ensuring unique recovery. We also propose an adaptive\nvariable-density sampling strategy to provide a better performance than uniform\nrandom sampling. Finally, simulation experiments are presented to validate the\neffectiveness of the theoretical results.\n","authors":["Yingcheng Lai","Li Chai","Jinming Xu"],"pdf_url":"https://arxiv.org/pdf/2412.20041v1.pdf","comment":"14 pages, 6 figures"},{"id":"http://arxiv.org/abs/2411.09241v2","updated":"2024-12-28T05:59:31Z","published":"2024-11-14T07:15:24Z","title":"BlueME: Robust Underwater Robot-to-Robot Communication Using Compact\n  Magnetoelectric Antennas","summary":"  We present the design, development, and experimental validation of BlueME, a\ncompact magnetoelectric (ME) antenna array system for underwater robot-to-robot\ncommunication. BlueME employs ME antennas operating at their natural mechanical\nresonance frequency to efficiently transmit and receive very-low-frequency\n(VLF) electromagnetic signals underwater. We outline the design, simulation,\nfabrication, and integration of the proposed system on low-power embedded\nplatforms focusing on portable and scalable applications. For performance\nevaluation, we deployed BlueME on an autonomous surface vehicle (ASV) and a\nremotely operated vehicle (ROV) in open-water field trials. Our tests\ndemonstrate that BlueME maintains reliable signal transmission at distances\nbeyond 200 meters while consuming only 1 watt of power. Field trials show that\nthe system operates effectively in challenging underwater conditions such as\nturbidity, obstacles, and multipath interference -- that generally affect\nacoustics and optics. Our analysis also examines the impact of complete\nsubmersion on system performance and identifies key deployment considerations.\nThis work represents the first practical underwater deployment of ME antennas\noutside the laboratory, and implements the largest VLF ME array system to date.\nBlueME demonstrates significant potential for marine robotics and automation in\nmulti-robot cooperative systems and remote sensor networks.\n","authors":["Mehron Talebi","Sultan Mahmud","Adam Khalifa","Md Jahidul Islam"],"pdf_url":"https://arxiv.org/pdf/2411.09241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19996v1","updated":"2024-12-28T03:42:05Z","published":"2024-12-28T03:42:05Z","title":"Embodied AI-empowered Low Altitude Economy: Integrated Sensing,\n  Communications, Computation, and Control (ISC3)","summary":"  Low altitude economy (LAE) holds immense potential to drive urban development\nacross various sectors. However, LAE also faces challenges in data collection\nand processing efficiency, flight control precision, and network performance.\nThe challenges could be solved by realizing an integration of sensing,\ncommunications, computation, and control (ISC3) for LAE. In this regard,\nembodied artificial intelligence (EAI), with its unique perception, planning,\nand decision-making capabilities, offers a promising solution to realize ISC3.\nSpecifically, this paper investigates an application of EAI into ISC3 to\nsupport LAE, exploring potential research focuses, solutions, and case study.\nWe begin by outlining rationales and benefits of introducing EAI into LAE,\nfollowed by reviewing research directions and solutions for EAI in ISC3. We\nthen propose a framework of an EAI-enabled ISC3 for LAE. The framework's\neffectiveness is evaluated through a case study of express delivery utilizing\nan EAI-enabled UAV. Finally, we discuss several future research directions for\nadvancing EAI-enabled LAE.\n","authors":["Yaoqi Yang","Yong Chen","Jiacheng Wang","Geng Sun","Dusit Niyato"],"pdf_url":"https://arxiv.org/pdf/2412.19996v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19974v1","updated":"2024-12-28T02:15:45Z","published":"2024-12-28T02:15:45Z","title":"Exploiting Movable-Element STARS for Wireless Communications","summary":"  A novel movable-element enabled simultaneously transmitting and reflecting\nsurface (ME-STARS) communication system is proposed, where ME-STARS elements\npositions can be adjusted to enhance the degress-of-freedom for transmission\nand reflection. For each ME-STARS operating protocols, namely energy-splitting\n(ES), mode switching (MS), and time switching (TS), a weighted sum rate (WSR)\nmaximization problem is formulated to jointly optimize the active beamforming\nat the base station (BS) as well as the elements positions and passive\nbeamforming at the ME-STARS. An alternative optimization (AO)-based iterative\nalgorithm is developed to decompose the original non-convex problem into three\nsubproblems. Specifically, the gradient descent algorithm is employed for\nsolving the ME-STARS element position optimization subproblem, and the weighted\nminimum mean square error and the successive convex approximation methods are\ninvoked for solving the active and passive beamforming subproblems,\nrespectively. It is further demonstrated that the proposed AO algorithm for ES\ncan be extended to solve the problems for MS and TS. Numerical results unveil\nthat: 1) the ME-STARS can significantly improve the WSR compared to the STARS\nwith fixed position elements and the conventional reconfigurable intelligent\nsurface with movable elements, thanks to the extra spatial-domain diversity and\nthe higher flexibility in beamforming; and 2) the performance gain of ME-STARS\nis significant in the scenarios with larger number of users or more scatterers.\n","authors":["Jingjing Zhao","Quan Zhou","Xidong Mu","Kaiquan Cai","Yanbo Zhu","Yuanwei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.19974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.00035v2","updated":"2024-12-28T02:15:27Z","published":"2024-08-18T02:10:29Z","title":"EEG Right & Left Voluntary Hand Movement-based Virtual Brain-Computer\n  Interfacing Keyboard Using Hybrid Deep Learning Approach","summary":"  Brain-machine interfaces (BMIs), particularly those based on\nelectroencephalography (EEG), offer promising solutions for assisting\nindividuals with motor disabilities. However, challenges in reliably\ninterpreting EEG signals for specific tasks, such as simulating keystrokes,\npersist due to the complexity and variability of brain activity. Current\nEEG-based BMIs face limitations in adaptability, usability, and robustness,\nespecially in applications like virtual keyboards, as traditional\nmachine-learning models struggle to handle high-dimensional EEG data\neffectively. To address these gaps, we developed an EEG-based BMI system\ncapable of accurately identifying voluntary keystrokes, specifically leveraging\nright and left voluntary hand movements. Using a publicly available EEG\ndataset, the signals were pre-processed with band-pass filtering, segmented\ninto 22-electrode arrays, and refined into event-related potential (ERP)\nwindows, resulting in a 19x200 feature array categorized into three classes:\nresting state (0), 'd' key press (1), and 'l' key press (2). Our approach\nemploys a hybrid neural network architecture with BiGRU-Attention as the\nproposed model for interpreting EEG signals, achieving superior test accuracy\nof 90% and a mean accuracy of 91% in 10-fold stratified cross-validation. This\nperformance outperforms traditional ML methods like Support Vector Machines\n(SVMs) and Naive Bayes, as well as advanced architectures such as Transformers,\nCNN-Transformer hybrids, and EEGNet. Finally, the BiGRU-Attention model is\nintegrated into a real-time graphical user interface (GUI) to simulate and\npredict keystrokes from brain activity. Our work demonstrates how deep learning\ncan advance EEG-based BMI systems by addressing the challenges of signal\ninterpretation and classification.\n","authors":["Biplov Paneru","Bipul Thapa","Bishwash Paneru","Sanjog Chhetri Sapkota"],"pdf_url":"https://arxiv.org/pdf/2409.00035v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19967v1","updated":"2024-12-28T01:37:25Z","published":"2024-12-28T01:37:25Z","title":"MobileNetV2: A lightweight classification model for home-based sleep\n  apnea screening","summary":"  This study proposes a novel lightweight neural network model leveraging\nfeatures extracted from electrocardiogram (ECG) and respiratory signals for\nearly OSA screening. ECG signals are used to generate feature spectrograms to\npredict sleep stages, while respiratory signals are employed to detect\nsleep-related breathing abnormalities. By integrating these predictions, the\nmethod calculates the apnea-hypopnea index (AHI) with enhanced accuracy,\nfacilitating precise OSA diagnosis.\n  The method was validated on three publicly available sleep apnea databases:\nthe Apnea-ECG database, the UCDDB dataset, and the MIT-BIH Polysomnographic\ndatabase. Results showed an overall OSA detection accuracy of 0.978,\nhighlighting the model's robustness. Respiratory event classification achieved\nan accuracy of 0.969 and an area under the receiver operating characteristic\ncurve (ROC-AUC) of 0.98. For sleep stage classification, in UCDDB dataset, the\nROC-AUC exceeded 0.85 across all stages, with recall for Sleep reaching 0.906\nand specificity for REM and Wake states at 0.956 and 0.937, respectively.\n  This study underscores the potential of integrating lightweight neural\nnetworks with multi-signal analysis for accurate, portable, and cost-effective\nOSA screening, paving the way for broader adoption in home-based and wearable\nhealth monitoring systems.\n","authors":["Hui Pan","Yanxuan Yu","Jilun Ye","Xu Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.19967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11482v2","updated":"2024-12-28T01:23:49Z","published":"2024-12-16T06:50:02Z","title":"Probabilistic GOSPA: A Metric for Performance Evaluation of Multi-Object\n  Filters with Uncertainties","summary":"  This correspondence presents a probabilistic generalization of the\nGeneralized Optimal Sub-Pattern Assignment (GOSPA) metric, termed P-GOSPA. The\nGOSPA metric is widely used to evaluate the distance between finite sets,\nparticularly in multi-object estimation applications. The P-GOSPA extends GOSPA\ninto the space of multi-Bernoulli densities, incorporating inherent uncertainty\nin probabilistic multi-object representations. Additionally, P-GOSPA retains\nthe interpretability of GOSPA, such as its decomposition into localization,\nmissed detection, and false detection errors in a sound and meaningful manner.\nExamples and simulations are provided to demonstrate the efficacy of the\nproposed P-GOSPA metric.\n","authors":["Yuxuan Xia","√Ångel F. Garc√≠a-Fern√°ndez","Johan Karlsson","Ting Yuan","Kuo-Chu Chang","Lennart Svensson"],"pdf_url":"https://arxiv.org/pdf/2412.11482v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20170v1","updated":"2024-12-28T14:58:46Z","published":"2024-12-28T14:58:46Z","title":"Real-time Calibration Model for Low-cost Sensor in Fine-grained Time\n  series","summary":"  Precise measurements from sensors are crucial, but data is usually collected\nfrom low-cost, low-tech systems, which are often inaccurate. Thus, they require\nfurther calibrations. To that end, we first identify three requirements for\neffective calibration under practical low-tech sensor conditions. Based on the\nrequirements, we develop a model called TESLA, Transformer for effective sensor\ncalibration utilizing logarithmic-binned attention. TESLA uses a\nhigh-performance deep learning model, Transformers, to calibrate and capture\nnon-linear components. At its core, it employs logarithmic binning to minimize\nattention complexity. TESLA achieves consistent real-time calibration, even\nwith longer sequences and finer-grained time series in hardware-constrained\nsystems. Experiments show that TESLA outperforms existing novel deep learning\nand newly crafted linear models in accuracy, calibration speed, and energy\nefficiency.\n","authors":["Seokho Ahn","Hyungjin Kim","Sungbok Shin","Young-Duk Seo"],"pdf_url":"https://arxiv.org/pdf/2412.20170v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.20041v1","updated":"2024-12-28T06:12:37Z","published":"2024-12-28T06:12:37Z","title":"On Random Sampling of Diffused Graph Signals with Sparse Inputs on\n  Vertex Domain","summary":"  The sampling of graph signals has recently drawn much attention due to the\nwide applications of graph signal processing. While a lot of efficient methods\nand interesting results have been reported to the sampling of band-limited or\nsmooth graph signals, few research has been devoted to non-smooth graph\nsignals, especially to sparse graph signals, which are also of importance in\nmany practical applications. This paper addresses the random sampling of\nnon-smooth graph signals generated by diffusion of sparse inputs. We aim to\npresent a solid theoretical analysis on the random sampling of diffused sparse\ngraph signals, which can be parallel to that of band-limited graph signals, and\nthus present a sufficient condition to the number of samples ensuring the\nunique recovery for uniform random sampling. Then, we focus on two classes of\nwidely used binary graph models, and give explicit and tighter estimations on\nthe sampling numbers ensuring unique recovery. We also propose an adaptive\nvariable-density sampling strategy to provide a better performance than uniform\nrandom sampling. Finally, simulation experiments are presented to validate the\neffectiveness of the theoretical results.\n","authors":["Yingcheng Lai","Li Chai","Jinming Xu"],"pdf_url":"https://arxiv.org/pdf/2412.20041v1.pdf","comment":"14 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1612.09565 by other authors"}]},"2024-12-31T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2412.20798v2","updated":"2024-12-31T16:13:54Z","published":"2024-12-30T08:43:28Z","title":"A Tale of Two Imperatives: Privacy and Explainability","summary":"  Deep learning's preponderance across scientific domains has reshaped\nhigh-stakes decision-making, making it essential to follow rigorous operational\nframeworks that include both Right-to-Privacy (RTP) and Right-to-Explanation\n(RTE). This paper examines the complexities of combining these two\nrequirements. For RTP, we focus on `Differential privacy' (DP), which is\nconsidered the current \\textit{gold standard} for privacy-preserving machine\nlearning due to its strong quantitative guarantee of privacy. For RTE, we focus\non post-hoc explainers: they are the \\textit{go-to} option for model auditing\nas they operate independently of model training. We formally investigate DP\nmodels and various commonly-used post-hoc explainers: how to evaluate these\nexplainers subject to RTP, and analyze the intrinsic interactions between DP\nmodels and these explainers. Furthermore, our work throws light on how RTP and\nRTE can be effectively combined in high-stakes applications. Our study\nconcludes by outlining an industrial software pipeline, with the example of a\nwildly used use-case, that respects both RTP and RTE requirements.\n","authors":["Supriya Manna","Niladri Sett"],"pdf_url":"https://arxiv.org/pdf/2412.20798v2.pdf","comment":"45 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.20787v2","updated":"2024-12-31T06:43:12Z","published":"2024-12-30T08:11:54Z","title":"SecBench: A Comprehensive Multi-Dimensional Benchmarking Dataset for\n  LLMs in Cybersecurity","summary":"  Evaluating Large Language Models (LLMs) is crucial for understanding their\ncapabilities and limitations across various applications, including natural\nlanguage processing and code generation. Existing benchmarks like MMLU, C-Eval,\nand HumanEval assess general LLM performance but lack focus on specific expert\ndomains such as cybersecurity. Previous attempts to create cybersecurity\ndatasets have faced limitations, including insufficient data volume and a\nreliance on multiple-choice questions (MCQs). To address these gaps, we propose\nSecBench, a multi-dimensional benchmarking dataset designed to evaluate LLMs in\nthe cybersecurity domain. SecBench includes questions in various formats (MCQs\nand short-answer questions (SAQs)), at different capability levels (Knowledge\nRetention and Logical Reasoning), in multiple languages (Chinese and English),\nand across various sub-domains. The dataset was constructed by collecting\nhigh-quality data from open sources and organizing a Cybersecurity Question\nDesign Contest, resulting in 44,823 MCQs and 3,087 SAQs. Particularly, we used\nthe powerful while cost-effective LLMs to (1). label the data and (2).\nconstructing a grading agent for automatic evaluation of SAQs. Benchmarking\nresults on 13 SOTA LLMs demonstrate the usability of SecBench, which is\narguably the largest and most comprehensive benchmark dataset for LLMs in\ncybersecurity. More information about SecBench can be found at our website, and\nthe dataset can be accessed via the artifact link.\n","authors":["Pengfei Jing","Mengyun Tang","Xiaorong Shi","Xing Zheng","Sen Nie","Shi Wu","Yong Yang","Xiapu Luo"],"pdf_url":"https://arxiv.org/pdf/2412.20787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20447v2","updated":"2024-12-31T13:36:16Z","published":"2024-12-29T12:09:19Z","title":"Cool, But What About Oracles? An Oracle-Based Perspective on Blockchain\n  Integration in the Accounting Field","summary":"  The Bitcoin Network is a sophisticated accounting system that allows its\nunderlying cryptocurrency to be trusted even in the absence of a reliable\nfinancial authority. Given its undeniable success, the technology, generally\nreferred to as blockchain, has also been proposed as a means to improve legacy\naccounting systems. Accounting for real-world data, however, requires the\nintervention of a third party known as an Oracle, which, having not the same\ncharacteristics as a blockchain, could potentially reduce the expected\nintegration benefit. Through a systematic review of the literature, this study\naims to investigate whether the papers concerning blockchain integration in\naccounting consider and address the limitations posed by oracles. A broad\noverview of the limitations that emerged in the literature is provided and\ndistinguished according to the specific accounting integration. Results support\nthe view that although research on the subject counts numerous articles, actual\nstudies considering oracle limitations are lacking. Interestingly, despite the\nscarce production of papers addressing oracles in various accounting sectors,\nreporting for ESG already shows interesting workarounds for oracle limitations,\nwith permissioned chains envisioned as a valid support for the safe storage of\nsustainability data.\n","authors":["Giulio Caldarelli"],"pdf_url":"https://arxiv.org/pdf/2412.20447v2.pdf","comment":"This manuscript is not Proofread. Some tables and figures, as well as\n  paragraph content may be subject to change in the journal version"},{"id":"http://arxiv.org/abs/2407.16928v2","updated":"2024-12-31T16:29:28Z","published":"2024-07-24T01:33:57Z","title":"From Sands to Mansions: Simulating Full Attack Chain with LLM-Organized\n  Knowledge","summary":"  Adversarial dynamics are intrinsic to the nature of offense and defense in\ncyberspace, with both attackers and defenders continuously evolving their\ntechnologies. Given the wide array of security products available, users often\nface challenges in selecting the most effective solutions. Furthermore,\ntraditional benchmarks based on single-point attacks are increasingly\ninadequate, failing to accurately reflect the full range of attacker\ncapabilities and falling short in properly evaluating the effectiveness of\ndefense products. Automated multi-stage attack simulations offer a promising\napproach to enhance system evaluation efficiency and aid in analyzing the\neffectiveness of detection systems. However, simulating a full attack chain is\ncomplex and requires significant time and expertise from security\nprofessionals, facing several challenges, including limited coverage of attack\ntechniques, a high level of required expertise, and a lack of execution detail.\nIn this paper, we model automatic attack simulation as a planning problem. By\nusing the Planning Domain Definition Language (PDDL) to formally describe the\nattack simulation problem, and combining domain knowledge of both the problem\nand the domain space, we enable the planning of attack paths through\nstandardized, domain-independent planning algorithms. We explore the potential\nof Large Language Models (LLMs) to summarize and analyze knowledge from\nexisting attack documentation and reports, facilitating automated attack\nplanning. We introduce Aurora, a system that autonomously simulates full attack\nchains based on external attack tools and threat intelligence reports.\n","authors":["Lingzhi Wang","Zhenyuan Li","Zonghan Guo","Yi Jiang","Kyle Jung","Kedar Thiagarajan","Jiahui Wang","Zhengkai Wang","Emily Wei","Xiangmin Shen","Yan Chen"],"pdf_url":"https://arxiv.org/pdf/2407.16928v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05496v3","updated":"2024-12-31T12:18:37Z","published":"2023-12-09T07:51:01Z","title":"Implicit Steganography Beyond the Constraints of Modality","summary":"  Cross-modal steganography is committed to hiding secret information of one\nmodality in another modality. Despite the advancement in the field of\nsteganography by the introduction of deep learning, cross-modal steganography\nstill remains to be a challenge to the field. The incompatibility between\ndifferent modalities not only complicate the hiding process but also results in\nincreased vulnerability to detection. To rectify these limitations, we present\nINRSteg, an innovative cross-modal steganography framework based on Implicit\nNeural Representations (INRs). We introduce a novel network allocating\nframework with a masked parameter update which facilitates hiding multiple data\nand enables cross modality across image, audio, video and 3D shape. Moreover,\nwe eliminate the necessity of training a deep neural network and therefore\nsubstantially reduce the memory and computational cost and avoid domain\nadaptation issues. To the best of our knowledge, in the field of steganography,\nthis is the first to introduce diverse modalities to both the secret and cover\ndata. Detailed experiments in extreme modality settings demonstrate the\nflexibility, security, and robustness of INRSteg.\n","authors":["Sojeong Song","Seoyun Yang","Chang D. Yoo","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2312.05496v3.pdf","comment":"25 pages, Accepted at European Conference on Computer Vision (ECCV)"},{"id":"http://arxiv.org/abs/2402.00356v3","updated":"2024-12-31T05:24:50Z","published":"2024-02-01T05:55:43Z","title":"Securing Cloud-Based Internet of Things: Challenges and Mitigations","summary":"  The Internet of Things (IoT) has seen remarkable advancements in recent\nyears, leading to a paradigm shift in the digital landscape. However, these\ntechnological strides have introduced new challenges, particularly in\ncybersecurity. IoT devices, inherently connected to the internet, are\nsusceptible to various forms of attacks. Moreover, IoT services often handle\nsensitive user data, which could be exploited by malicious actors or\nunauthorized service providers. As IoT ecosystems expand, the convergence of\ntraditional and cloud-based systems presents unique security threats in the\nabsence of uniform regulations. Cloud-based IoT systems, enabled by\nPlatform-as-a-Service (PaaS) and Infrastructure-as-a-Service (IaaS) models,\noffer flexibility and scalability but also pose additional security risks. The\nintricate interaction between these systems and traditional IoT devices demands\ncomprehensive strategies to protect data integrity and user privacy. This paper\nhighlights the pressing security concerns associated with the widespread\nadoption of IoT devices and services. We propose viable solutions to bridge the\nexisting security gaps while anticipating and preparing for future challenges.\nThis paper provides a detailed survey of the key security challenges that IoT\nservices are currently facing. We also suggest proactive strategies to mitigate\nthese risks, thereby strengthening the overall security of IoT devices and\nservices.\n","authors":["Nivedita Singh","Rajkumar Buyya","Hyoungshich Kim"],"pdf_url":"https://arxiv.org/pdf/2402.00356v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11441v2","updated":"2024-12-31T05:07:06Z","published":"2024-12-16T04:47:55Z","title":"UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion\n  Models","summary":"  Recent studies show that diffusion models (DMs) are vulnerable to backdoor\nattacks. Existing backdoor attacks impose unconcealed triggers (e.g., a gray\nbox and eyeglasses) that contain evident patterns, rendering remarkable attack\neffects yet easy detection upon human inspection and defensive algorithms.\nWhile it is possible to improve stealthiness by reducing the strength of the\nbackdoor, doing so can significantly compromise its generality and\neffectiveness. In this paper, we propose UIBDiffusion, the universal\nimperceptible backdoor attack for diffusion models, which allows us to achieve\nsuperior attack and generation performance while evading state-of-the-art\ndefenses. We propose a novel trigger generation approach based on universal\nadversarial perturbations (UAPs) and reveal that such perturbations, which are\ninitially devised for fooling pre-trained discriminative models, can be adapted\nas potent imperceptible backdoor triggers for DMs. We evaluate UIBDiffusion on\nmultiple types of DMs with different kinds of samplers across various datasets\nand targets. Experimental results demonstrate that UIBDiffusion brings three\nadvantages: 1) Universality, the imperceptible trigger is universal (i.e.,\nimage and model agnostic) where a single trigger is effective to any images and\nall diffusion models with different samplers; 2) Utility, it achieves\ncomparable generation quality (e.g., FID) and even better attack success rate\n(i.e., ASR) at low poison rates compared to the prior works; and 3)\nUndetectability, UIBDiffusion is plausible to human perception and can bypass\nElijah and TERD, the SOTA defenses against backdoors for DMs. We will release\nour backdoor triggers and code.\n","authors":["Yuning Han","Bingyin Zhao","Rui Chu","Feng Luo","Biplab Sikdar","Yingjie Lao"],"pdf_url":"https://arxiv.org/pdf/2412.11441v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03305v2","updated":"2024-12-31T02:28:23Z","published":"2024-11-05T18:00:29Z","title":"Quantum One-Time Protection of any Randomized Algorithm","summary":"  The meteoric rise in power and popularity of machine learning models\ndependent on valuable training data has reignited a basic tension between the\npower of running a program locally and the risk of exposing details of that\nprogram to the user. At the same time, fundamental properties of quantum states\noffer new solutions to data and program security that can require strikingly\nfew quantum resources to exploit, and offer advantages outside of mere\ncomputational run time. In this work, we demonstrate such a solution with\nquantum one-time tokens.\n  A quantum one-time token is a quantum state that permits a certain program to\nbe evaluated exactly once. One-time security guarantees, roughly, that the\ntoken cannot be used to evaluate the program more than once. We propose a\nscheme for building quantum one-time tokens for any randomized classical\nprogram, which include generative AI models. We prove that the scheme satisfies\nan interesting definition of one-time security as long as outputs of the\nclassical algorithm have high enough min-entropy, in a black box model.\n  Importantly, the classical program being protected does not need to be\nimplemented coherently on a quantum computer. In fact, the size and complexity\nof the quantum one-time token is independent of the program being protected,\nand additional quantum resources serve only to increase the security of the\nprotocol. Due to this flexibility in adjusting the security, we believe that\nour proposal is parsimonious enough to serve as a promising candidate for a\nnear-term useful demonstration of quantum computing in either the NISQ or early\nfault tolerant regime.\n","authors":["Sam Gunn","Ramis Movassagh"],"pdf_url":"https://arxiv.org/pdf/2411.03305v2.pdf","comment":"Update: Resolved a bug where we used an insufficiently-strong\n  definition of one-time authentication. See the remark on page 4"},{"id":"http://arxiv.org/abs/2411.18648v2","updated":"2024-12-31T02:11:04Z","published":"2024-11-26T22:50:53Z","title":"MADE: Graph Backdoor Defense with Masked Unlearning","summary":"  Graph Neural Networks (GNNs) have garnered significant attention from\nresearchers due to their outstanding performance in handling graph-related\ntasks, such as social network analysis, protein design, and so on. Despite\ntheir widespread application, recent research has demonstrated that GNNs are\nvulnerable to backdoor attacks, implemented by injecting triggers into the\ntraining datasets. Trained on the poisoned data, GNNs will predict target\nlabels when attaching trigger patterns to inputs. This vulnerability poses\nsignificant security risks for applications of GNNs in sensitive domains, such\nas drug discovery. While there has been extensive research into backdoor\ndefenses for images, strategies to safeguard GNNs against such attacks remain\nunderdeveloped. Furthermore, we point out that conventional backdoor defense\nmethods designed for images cannot work well when directly implemented on graph\ndata. In this paper, we first analyze the key difference between image backdoor\nand graph backdoor attacks. Then we tackle the graph defense problem by\npresenting a novel approach called MADE, which devises an adversarial mask\ngeneration mechanism that selectively preserves clean sub-graphs and further\nleverages masks on edge weights to eliminate the influence of triggers\neffectively. Extensive experiments across various graph classification tasks\ndemonstrate the effectiveness of MADE in significantly reducing the attack\nsuccess rate (ASR) while maintaining a high classification accuracy.\n","authors":["Xiao Lin","Mingjie Li","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2411.18648v2.pdf","comment":"15 pages, 10 figures"}],"Information Theory":[{"id":"http://arxiv.org/abs/2412.20241v2","updated":"2024-12-31T14:08:42Z","published":"2024-12-28T18:39:05Z","title":"A Hybrid Quantum-Classical Autoencoder Framework for End-to-End\n  Communication Systems","summary":"  This paper investigates the application of quantum machine learning to\nEnd-to-End (E2E) communication systems in wireless fading scenarios. We\nintroduce a novel hybrid quantum-classical autoencoder architecture that\ncombines parameterized quantum circuits with classical deep neural networks\n(DNNs). Specifically, we propose a hybrid quantum-classical autoencoder (QAE)\nframework to optimize the E2E communication system. Our results demonstrate the\nfeasibility of the proposed hybrid system, and reveal that it is the first work\nthat can achieve comparable block error rate (BLER) performance to classical\nDNN-based and conventional channel coding schemes, while significantly reducing\nthe number of trainable parameters. Additionally, the proposed QAE exhibits\nsteady and superior BLER convergence over the classical autoencoder baseline.\n","authors":["Bolun Zhang","Gan Zheng","Nguyen Van Huynh"],"pdf_url":"https://arxiv.org/pdf/2412.20241v2.pdf","comment":"This paper has been accepted in IEEE Wireless Communications Letters"},{"id":"http://arxiv.org/abs/2412.19438v2","updated":"2024-12-31T03:55:54Z","published":"2024-12-27T04:06:32Z","title":"The Rendezvous Between Extreme Value Theory and Next-generation Networks","summary":"  Promising technologies such as massive multiple-input and multiple-output,\nreconfigurable intelligent reflecting surfaces, non-terrestrial networks,\nmillimetre wave communication, ultra-reliable lowlatency communication are\nenvisioned as the enablers for next-generation (NG) networks. In contrast to\nconventional communication systems meeting specific average performance\nrequirements, NG systems are expected to meet quality-of-service requirements\nin extreme scenarios as well. In this regard, extreme value theory (EVT)\nprovides a powerful framework for the design of communication systems. In this\npaper, we provide a comprehensive survey of advances in communication that\nutilize EVT to characterize the extreme order statistics of interest. We first\ngive an overview of the history of EVT and then elaborate on the fundamental\ntheorems. Subsequently, we discuss different problems of interest in NG\ncommunication systems and how EVT can be utilized for their analysis. We\nfinally point out the open challenges and future directions of EVT in NG\ncommunication systems.\n","authors":["Srinivas Sagar","Athira Subhash","Chen-Feng Liu","Ahmed Elzanaty","Yazan H. Al-Badarneh","Sheetal Kalyani","Mohamed-Slim Alouini","Mehdi Bennis","Lajos Hanzo"],"pdf_url":"https://arxiv.org/pdf/2412.19438v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11857v4","updated":"2024-12-31T12:20:07Z","published":"2024-11-01T20:20:05Z","title":"Radiance Field Delta Video Compression in Edge-Enabled Vehicular\n  Metaverse","summary":"  Connected and autonomous vehicles (CAVs) offload computationally intensive\ntasks to multi-access edge computing (MEC) servers via\nvehicle-to-infrastructure (V2I) communication, enabling applications within the\nvehicular metaverse, which transforms physical environment into the digital\nspace enabling advanced analysis or predictive modeling. A core challenge is\nphysical-to-virtual (P2V) synchronization through digital twins (DTs), reliant\non MEC networks and ultra-reliable low-latency communication (URLLC). To\naddress this, we introduce radiance field (RF) delta video compression (RFDVC),\nwhich uses RF-encoder and RF-decoder architecture using distributed RFs as DTs\nstoring photorealistic 3D urban scenes in compressed form. This method extracts\ndifferences between CAV-frame capturing actual traffic and RF-frame capturing\nempty scene from the same camera pose in batches encoded and transmitted over\nthe MEC network. Experiments show data savings up to 71% against H.264 codec\nand 44% against H.265 codec under different conditions as lighting changes, and\nrain. RFDVC also demonstrates resilience to transmission errors, achieving up\nto +0.29 structural similarity index measure (SSIM) improvement at block error\nrate (BLER) = 0.35 in non-rainy and +0.25 at BLER = 0.2 in rainy conditions,\nensuring superior visual quality compared to standard video coding (VC) methods\nacross various conditions.\n","authors":["Mat√∫≈° Dopiriak","Eugen ≈†lapak","Juraj Gazda","Devendra S. Gurjar","Mohammad Abdullah Al Faruque","Marco Levorato"],"pdf_url":"https://arxiv.org/pdf/2411.11857v4.pdf","comment":"I. We changed the template. II. We removed biography section"},{"id":"http://arxiv.org/abs/2404.17973v2","updated":"2024-12-31T05:42:23Z","published":"2024-04-27T18:24:34Z","title":"Over-the-Air Fusion of Sparse Spatial Features for Integrated Sensing\n  and Edge AI over Broadband Channels","summary":"  The 6G mobile networks feature two new usage scenarios -- distributed sensing\nand edge artificial intelligence (AI). Their natural integration, termed\nintegrated sensing and edge AI (ISEA), promises to create a platform that\nenables intelligent environment perception for wide-ranging applications. A\nbasic operation in ISEA is for a fusion center to acquire and fuse features of\nspatial sensing data distributed at many edge devices (known as agents), which\nis confronted by a communication bottleneck due to multiple access over hostile\nwireless channels. To address this issue, we propose a novel framework, called\nSpatial Over-the-Air Fusion (Spatial AirFusion), which exploits radio waveform\nsuperposition to aggregate spatially sparse features over the air and thereby\nenables simultaneous access. The framework supports simultaneous aggregation\nover multiple voxels, which partition the 3D sensing region, and across\nmultiple subcarriers. It exploits both spatial feature sparsity with channel\ndiversity to pair voxel-level aggregation tasks and subcarriers to maximize the\nminimum receive signal-to-noise ratio among voxels. Optimally solving the\nresultant mixed-integer problem of Voxel-Carrier Pairing and Power Allocation\n(VoCa-PPA) is a focus of this work. The proposed approach hinges on derivations\nof optimal power allocation as a closed-form function of voxel-carrier pairing\nand a useful property of VoCa-PPA that allows dramatic solution space\nreduction. Both a low-complexity greedy algorithm and an optimal tree-search\nalgorithm are then designed for VoCa-PPA. The latter is accelerated with a\ncustomised compact search tree, node pruning and agent ordering. Extensive\nsimulations using real datasets demonstrate that Spatial AirFusion\nsignificantly reduces computation errors and improves sensing accuracy compared\nwith conventional over-the-air computation without awareness of spatial\nsparsity.\n","authors":["Zhiyan Liu","Qiao Lan","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2404.17973v2.pdf","comment":"Accepted by IEEE Transactions on Wireless Communications"},{"id":"http://arxiv.org/abs/2301.10414v3","updated":"2024-12-31T04:31:54Z","published":"2023-01-25T05:34:21Z","title":"Towards a Unification of Logic and Information Theory","summary":"  Today, the vast majority of the world's digital information is represented\nusing the fundamental assumption, introduced by Claude Shannon in 1948, that\n``...the semantic aspects of communication are irrelevant to the engineering\nproblem (of the design of communication systems)...''. Consider, nonetheless,\nthe observation that we often combine a message with other information in order\nto deduce new facts, thereby expanding the value of such a message. It is\nnoteworthy that to-date, no rigorous theory of communication has been put forth\nwhich postulates the existence of deductive capabilities on the receiver's\nside.\n  The purpose of this paper is to present such a theory. We formally model such\ndeductive capabilities using logic reasoning, and present a rigorous theory\nwhich covers the following generic scenario: Alice and Bob each have knowledge\nof some logic sentence, and they wish to communicate as efficiently as possible\nwith the shared goal that, following their communication, Bob should be able to\ndeduce a particular logic sentence that Alice knows to be true, but that Bob\ncurrently cannot prove. Many variants of this general setup are considered in\nthis article; in all cases we are able to provide sharp upper and lower bounds.\nOur contribution includes the identification of the most fundamental\nrequirements that we place on a logic and associated logical language for all\nof our results to apply. Practical algorithms that are in some cases\nasymptotically optimal are provided, and we illustrate the potential practical\nvalue of the design of communication systems that incorporate the assumption of\ndeductive capabilities at the receiver using experimental results that suggest\nsignificant possible gains compared to classical systems.\n","authors":["Luis A. Lastras","Barry Trager","Jonathan Lenchner","Wojtek Szpankowski","Chai Wah Wu","Mark Squillante","Alex Gray"],"pdf_url":"https://arxiv.org/pdf/2301.10414v3.pdf","comment":null}],"Signal Processing":[{"id":"http://arxiv.org/abs/2412.20241v2","updated":"2024-12-31T14:08:42Z","published":"2024-12-28T18:39:05Z","title":"A Hybrid Quantum-Classical Autoencoder Framework for End-to-End\n  Communication Systems","summary":"  This paper investigates the application of quantum machine learning to\nEnd-to-End (E2E) communication systems in wireless fading scenarios. We\nintroduce a novel hybrid quantum-classical autoencoder architecture that\ncombines parameterized quantum circuits with classical deep neural networks\n(DNNs). Specifically, we propose a hybrid quantum-classical autoencoder (QAE)\nframework to optimize the E2E communication system. Our results demonstrate the\nfeasibility of the proposed hybrid system, and reveal that it is the first work\nthat can achieve comparable block error rate (BLER) performance to classical\nDNN-based and conventional channel coding schemes, while significantly reducing\nthe number of trainable parameters. Additionally, the proposed QAE exhibits\nsteady and superior BLER convergence over the classical autoencoder baseline.\n","authors":["Bolun Zhang","Gan Zheng","Nguyen Van Huynh"],"pdf_url":"https://arxiv.org/pdf/2412.20241v2.pdf","comment":"This paper has been accepted in IEEE Wireless Communications Letters"},{"id":"http://arxiv.org/abs/2412.18122v3","updated":"2024-12-31T12:17:33Z","published":"2024-12-24T03:13:45Z","title":"FOGNA: An effective Sum-Difference Co-Array Design Based on Fourth-Order\n  Cumulants","summary":"  Array structures based on the fourth-order difference co-array (FODCA)\nprovide more degrees of freedom (DOF). However, since the growth of DOF is\nlimited by a single case of fourth-order cumulant in FODCA, this paper aims to\ndesign a sparse linear array (SLA) with higher DOF via exploring different\ncases of fourth-order cumulants. This paper presents a mathematical framework\nbased on fourth-order cumulant to devise a fourth-order extend co-array\n(FOECA), which is equivalent to FODCA. A novel SLA, namely fourth-order\ngeneralized nested array (FOGNA), is proposed based on FOECA to provide\nclosed-form expressions for the sensor locations and enhance DOF to resolve\nmore signal sources in direction of arrival (DOA) estimation. FOGNA is\nconsisted of three subarrays, where the first is a concatenated nested array\nand the other two subarrays are SLA with big inter-spacing between sensors.\nWhen the total physical sensors of FOGNA are given, the number of sensors in\neach subarray is determined by the designed method, which can obtain the\nmaximum DOF under the proposed array structure and derive closed-form\nexpressions for the sensor locations of FOGNA. The proposed array structure not\nonly achieves higher DOF than those of existing FODCAs but also reduces mutual\ncoupling effects. Numerical simulations are conducted to verify the superiority\nof FOGNA on DOA estimation performance and enhanced DOF over other existing\nFODCAs.\n","authors":["Si Wang","Guoqiang Xiao"],"pdf_url":"https://arxiv.org/pdf/2412.18122v3.pdf","comment":"16 pages, 29 figures"},{"id":"http://arxiv.org/abs/2411.12469v3","updated":"2024-12-31T15:52:47Z","published":"2024-11-19T12:51:17Z","title":"AI Flow at the Network Edge","summary":"  Recent advancements in large language models (LLMs) and their multimodal\nvariants have led to remarkable progress across various domains, demonstrating\nimpressive capabilities and unprecedented potential. In the era of ubiquitous\nconnectivity, leveraging communication networks to distribute intelligence is a\ntransformative concept, envisioning AI-powered services accessible at the\nnetwork edge. However, pushing large models from the cloud to\nresource-constrained environments faces critical challenges. Model inference on\nlow-end devices leads to excessive latency and performance bottlenecks, while\nraw data transmission over limited bandwidth networks causes high communication\noverhead. This article presents AI Flow, a framework that streamlines the\ninference process by jointly leveraging the heterogeneous resources available\nacross devices, edge nodes, and cloud servers, making intelligence flow across\nnetworks. To facilitate cooperation among multiple computational nodes, the\nproposed framework explores a paradigm shift in the design of communication\nnetwork systems from transmitting information flow to intelligence flow, where\nthe goal of communications is task-oriented and folded into the inference\nprocess. Experimental results demonstrate the effectiveness of the proposed\nframework through an image captioning use case, showcasing the ability to\nreduce response latency while maintaining high-quality captions. This article\nserves as a position paper for identifying the motivation, challenges, and\nprinciples of AI Flow.\n","authors":["Jiawei Shao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2411.12469v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11350v2","updated":"2024-12-31T23:02:54Z","published":"2024-04-17T13:08:26Z","title":"Calibrating Bayesian Learning via Regularization, Confidence\n  Minimization, and Selective Inference","summary":"  The application of artificial intelligence (AI) models in fields such as\nengineering is limited by the known difficulty of quantifying the reliability\nof an AI's decision. A well-calibrated AI model must correctly report its\naccuracy on in-distribution (ID) inputs, while also enabling the detection of\nout-of-distribution (OOD) inputs. A conventional approach to improve\ncalibration is the application of Bayesian ensembling. However, owing to\ncomputational limitations and model misspecification, practical ensembling\nstrategies do not necessarily enhance calibration. This paper proposes an\nextension of variational inference (VI)-based Bayesian learning that integrates\ncalibration regularization for improved ID performance, confidence minimization\nfor OOD detection, and selective calibration to ensure a synergistic use of\ncalibration regularization and confidence minimization. The scheme is\nconstructed successively by first introducing calibration-regularized Bayesian\nlearning (CBNN), then incorporating out-of-distribution confidence minimization\n(OCM) to yield CBNN-OCM, and finally integrating also selective calibration to\nproduce selective CBNN-OCM (SCBNN-OCM). Selective calibration rejects inputs\nfor which the calibration performance is expected to be insufficient. Numerical\nresults illustrate the trade-offs between ID accuracy, ID calibration, and OOD\ncalibration attained by both frequentist and Bayesian learning methods. Among\nthe main conclusions, SCBNN-OCM is seen to achieve best ID and OOD performance\nas compared to existing state-of-the-art approaches at the cost of rejecting a\nsufficiently large number of inputs.\n","authors":["Jiayi Huang","Sangwoo Park","Osvaldo Simeone"],"pdf_url":"https://arxiv.org/pdf/2404.11350v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2411.11857v4","updated":"2024-12-31T12:20:07Z","published":"2024-11-01T20:20:05Z","title":"Radiance Field Delta Video Compression in Edge-Enabled Vehicular\n  Metaverse","summary":"  Connected and autonomous vehicles (CAVs) offload computationally intensive\ntasks to multi-access edge computing (MEC) servers via\nvehicle-to-infrastructure (V2I) communication, enabling applications within the\nvehicular metaverse, which transforms physical environment into the digital\nspace enabling advanced analysis or predictive modeling. A core challenge is\nphysical-to-virtual (P2V) synchronization through digital twins (DTs), reliant\non MEC networks and ultra-reliable low-latency communication (URLLC). To\naddress this, we introduce radiance field (RF) delta video compression (RFDVC),\nwhich uses RF-encoder and RF-decoder architecture using distributed RFs as DTs\nstoring photorealistic 3D urban scenes in compressed form. This method extracts\ndifferences between CAV-frame capturing actual traffic and RF-frame capturing\nempty scene from the same camera pose in batches encoded and transmitted over\nthe MEC network. Experiments show data savings up to 71% against H.264 codec\nand 44% against H.265 codec under different conditions as lighting changes, and\nrain. RFDVC also demonstrates resilience to transmission errors, achieving up\nto +0.29 structural similarity index measure (SSIM) improvement at block error\nrate (BLER) = 0.35 in non-rainy and +0.25 at BLER = 0.2 in rainy conditions,\nensuring superior visual quality compared to standard video coding (VC) methods\nacross various conditions.\n","authors":["Mat√∫≈° Dopiriak","Eugen ≈†lapak","Juraj Gazda","Devendra S. Gurjar","Mohammad Abdullah Al Faruque","Marco Levorato"],"pdf_url":"https://arxiv.org/pdf/2411.11857v4.pdf","comment":"I. We changed the template. II. We removed biography section"},{"id":"http://arxiv.org/abs/2404.17973v2","updated":"2024-12-31T05:42:23Z","published":"2024-04-27T18:24:34Z","title":"Over-the-Air Fusion of Sparse Spatial Features for Integrated Sensing\n  and Edge AI over Broadband Channels","summary":"  The 6G mobile networks feature two new usage scenarios -- distributed sensing\nand edge artificial intelligence (AI). Their natural integration, termed\nintegrated sensing and edge AI (ISEA), promises to create a platform that\nenables intelligent environment perception for wide-ranging applications. A\nbasic operation in ISEA is for a fusion center to acquire and fuse features of\nspatial sensing data distributed at many edge devices (known as agents), which\nis confronted by a communication bottleneck due to multiple access over hostile\nwireless channels. To address this issue, we propose a novel framework, called\nSpatial Over-the-Air Fusion (Spatial AirFusion), which exploits radio waveform\nsuperposition to aggregate spatially sparse features over the air and thereby\nenables simultaneous access. The framework supports simultaneous aggregation\nover multiple voxels, which partition the 3D sensing region, and across\nmultiple subcarriers. It exploits both spatial feature sparsity with channel\ndiversity to pair voxel-level aggregation tasks and subcarriers to maximize the\nminimum receive signal-to-noise ratio among voxels. Optimally solving the\nresultant mixed-integer problem of Voxel-Carrier Pairing and Power Allocation\n(VoCa-PPA) is a focus of this work. The proposed approach hinges on derivations\nof optimal power allocation as a closed-form function of voxel-carrier pairing\nand a useful property of VoCa-PPA that allows dramatic solution space\nreduction. Both a low-complexity greedy algorithm and an optimal tree-search\nalgorithm are then designed for VoCa-PPA. The latter is accelerated with a\ncustomised compact search tree, node pruning and agent ordering. Extensive\nsimulations using real datasets demonstrate that Spatial AirFusion\nsignificantly reduces computation errors and improves sensing accuracy compared\nwith conventional over-the-air computation without awareness of spatial\nsparsity.\n","authors":["Zhiyan Liu","Qiao Lan","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2404.17973v2.pdf","comment":"Accepted by IEEE Transactions on Wireless Communications"},{"id":"http://arxiv.org/abs/2407.10393v3","updated":"2024-12-31T03:13:10Z","published":"2024-07-15T02:10:20Z","title":"Movable Antenna-Aided Secure Full-Duplex Multi-User Communications","summary":"  In this paper, we investigate physical layer security (PLS) for full-duplex\n(FD) multi-user systems. We consider a base station (BS) that operates in FD\nmode and transmits artificial noise (AN) to simultaneously protect uplink (UL)\nand downlink (DL) transmissions. Conventional fixed-position antennas (FPAs) at\nthe FD BS struggle to fully exploit spatial degrees of freedom (DoFs) to\nimprove signal reception and suppress interference. To overcome this\nlimitation, we propose a novel FD BS architecture equipped with multiple\ntransmit and receive movable antennas (MAs). The MAs introduce the DoFs in\nantenna position optimization, which can improve the performance of secure\ncommunication systems. To serve users and counter the cooperative interception\nof multiple eavesdroppers (Eves), we formulate a sum of secrecy rates (SSR)\nmaximization problem to jointly optimize the MA positions, the transmit,\nreceive, and AN beamformers at the BS, and the UL powers. We propose an\nalternating optimization (AO) algorithm, which decomposes the original problem\ninto three sub-problems, to solve the challenging non-convex optimization\nproblem with highly coupled variables. Specifically, we propose the\nmulti-velocity particle swarm optimization (MVPSO), which is an improved\nversion of the standard particle swarm optimization (PSO), to simultaneously\noptimize all MA positions. The transmit/AN beamformers and the UL powers are\nsolved by successive convex approximation (SCA). The optimal receive beamformer\nis derived as a closed-form solution. Simulation results demonstrate the\neffectiveness of the proposed algorithms and the advantages of MAs over\nconventional FPAs in enhancing the security of FD multi-user systems.\n","authors":["Jingze Ding","Zijian Zhou","Bingli Jiao"],"pdf_url":"https://arxiv.org/pdf/2407.10393v3.pdf","comment":"This paper has been accepted by IEEE Transactions on Wireless\n  Communications"}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2112.03543v3","updated":"2024-12-31T10:15:21Z","published":"2021-12-07T07:37:46Z","title":"Phase transition of the 3-majority opinion dynamics with noisy\n  interactions","summary":"  Communication noise is a common feature in several real-world scenarios where\nsystems of agents need to communicate in order to pursue some collective task.\nIn particular, many biologically inspired systems that try to achieve\nagreements on some opinion must implement resilient dynamics that are not\nstrongly affected by noisy communications. In this work, we study the popular\n3-Majority dynamics, an opinion dynamics which has been proved to be an\nefficient protocol for the majority consensus problem, in which we introduce a\nsimple feature of uniform communication noise, following (d'Amore et al. 2020).\nWe prove that in the fully connected communication network of n agents and in\nthe binary opinion case, the process induced by the 3-Majority dynamics\nexhibits a phase transition. For a noise probability $p < 1/3$, the dynamics\nreaches in logarithmic time an almost-consensus metastable phase which lasts\nfor a polynomial number of rounds with high probability. Furthermore, departing\nfrom previous analyses, we further characterize this phase by showing that\nthere exists an attractive equilibrium value $s_{\\text{eq}} \\in [n]$ for the\nbias of the system, i.e. the difference between the majority community size and\nthe minority one. Moreover, the agreement opinion turns out to be the initial\nmajority one if the bias towards it is of magnitude $\\Omega(\\sqrt{n\\log n})$ in\nthe initial configuration. If, instead, $p > 1/3$, no form of consensus is\npossible, and any information regarding the initial majority opinion is lost in\nlogarithmic time with high probability. Despite more communications per-round\nare allowed, the 3-Majority dynamics surprisingly turns out to be less\nresilient to noise than the Undecided-State dynamics (d'Amore et al. 2020),\nwhose noise threshold value is $p = 1/2$.\n","authors":["Francesco d'Amore","Isabella Ziccardi"],"pdf_url":"https://arxiv.org/pdf/2112.03543v3.pdf","comment":"Journal version"}]},"2025-01-02T00:00:00Z":{"Cryptography and Security":[{"id":"http://arxiv.org/abs/2412.18990v2","updated":"2025-01-02T09:34:09Z","published":"2024-12-25T21:58:52Z","title":"Detection and classification of DDoS flooding attacks by machine\n  learning method","summary":"  This study focuses on a method for detecting and classifying distributed\ndenial of service (DDoS) attacks, such as SYN Flooding, ACK Flooding, HTTP\nFlooding, and UDP Flooding, using neural networks. Machine learning,\nparticularly neural networks, is highly effective in detecting malicious\ntraffic. A dataset containing normal traffic and various DDoS attacks was used\nto train a neural network model with a 24-106-5 architecture. The model\nachieved high Accuracy (99.35%), Precision (99.32%), Recall (99.54%), and\nF-score (0.99) in the classification task. All major attack types were\ncorrectly identified. The model was also further tested in the lab using\nvirtual infrastructures to generate normal and DDoS traffic. The results showed\nthat the model can accurately classify attacks under near-real-world\nconditions, demonstrating 95.05% accuracy and balanced F-score scores for all\nattack types. This confirms that neural networks are an effective tool for\ndetecting DDoS attacks in modern information security systems.\n","authors":["Dmytro Tymoshchuk","Oleh Yasniy","Mykola Mytnyk","Nataliya Zagorodna","Vitaliy Tymoshchuk"],"pdf_url":"https://arxiv.org/pdf/2412.18990v2.pdf","comment":"Paper Submitted to BAIT 2024 CEUR-WS, see\n  https://ceur-ws.org/Vol-3842/paper11.pdf"},{"id":"http://arxiv.org/abs/2408.11006v4","updated":"2025-01-02T13:11:53Z","published":"2024-08-20T17:00:04Z","title":"Security Attacks on LLM-based Code Completion Tools","summary":"  The rapid development of large language models (LLMs) has significantly\nadvanced code completion capabilities, giving rise to a new generation of\nLLM-based Code Completion Tools (LCCTs). Unlike general-purpose LLMs, these\ntools possess unique workflows, integrating multiple information sources as\ninput and prioritizing code suggestions over natural language interaction,\nwhich introduces distinct security challenges. Additionally, LCCTs often rely\non proprietary code datasets for training, raising concerns about the potential\nexposure of sensitive data. This paper exploits these distinct characteristics\nof LCCTs to develop targeted attack methodologies on two critical security\nrisks: jailbreaking and training data extraction attacks. Our experimental\nresults expose significant vulnerabilities within LCCTs, including a 99.4%\nsuccess rate in jailbreaking attacks on GitHub Copilot and a 46.3% success rate\non Amazon Q. Furthermore, We successfully extracted sensitive user data from\nGitHub Copilot, including 54 real email addresses and 314 physical addresses\nassociated with GitHub usernames. Our study also demonstrates that these\ncode-based attack methods are effective against general-purpose LLMs, such as\nthe GPT series, highlighting a broader security misalignment in the handling of\ncode by modern LLMs. These findings underscore critical security challenges\nassociated with LCCTs and suggest essential directions for strengthening their\nsecurity frameworks. The example code and attack samples from our research are\nprovided at https://github.com/Sensente/Security-Attacks-on-LCCTs.\n","authors":["Wen Cheng","Ke Sun","Xinyu Zhang","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.11006v4.pdf","comment":"Paper accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13374v2","updated":"2025-01-02T14:12:28Z","published":"2024-12-17T23:14:46Z","title":"Accelerating Hardware Verification with Graph Models","summary":"  The increasing complexity of modern processor and IP designs presents\nsignificant challenges in identifying and mitigating hardware flaws early in\nthe IC design cycle. Traditional hardware fuzzing techniques, inspired by\nsoftware testing, have shown promise but face scalability issues, especially at\nthe gate-level netlist where bugs introduced during synthesis are often missed\nby RTL-level verification due to longer simulation times.\n  To address this, we introduce GraphFuzz, a graph-based hardware fuzzer\ndesigned for gate-level netlist verification. In this approach, hardware\ndesigns are modeled as graph nodes, with gate behaviors encoded as features. By\nleveraging graph learning algorithms, GraphFuzz efficiently detects hardware\nvulnerabilities by analyzing node patterns. Our evaluation across benchmark\ncircuits and open-source processors demonstrates an average prediction accuracy\nof 80% and bug detection accuracy of 70%, highlighting the potential of\ngraph-based methods for enhancing hardware verification.\n","authors":["Raghul Saravanan","Sreenitha Kasarapu","Sai Manoj Pudukotai Dinakarrao"],"pdf_url":"https://arxiv.org/pdf/2412.13374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19530v2","updated":"2025-01-02T13:54:17Z","published":"2024-03-28T16:06:06Z","title":"Detecting Financial Bots on the Ethereum Blockchain","summary":"  The integration of bots in Distributed Ledger Technologies (DLTs) fosters\nefficiency and automation. However, their use is also associated with predatory\ntrading and market manipulation, and can pose threats to system integrity. It\nis therefore essential to understand the extent of bot deployment in DLTs;\ndespite this, current detection systems are predominantly rule-based and lack\nflexibility. In this study, we present a novel approach that utilizes machine\nlearning for the detection of financial bots on the Ethereum platform. First,\nwe systematize existing scientific literature and collect anecdotal evidence to\nestablish a taxonomy for financial bots, comprising 7 categories and 24\nsubcategories. Next, we create a ground-truth dataset consisting of 133 human\nand 137 bot addresses. Third, we employ both unsupervised and supervised\nmachine learning algorithms to detect bots deployed on Ethereum. The\nhighest-performing clustering algorithm is a Gaussian Mixture Model with an\naverage cluster purity of 82.6%, while the highest-performing model for binary\nclassification is a Random Forest with an accuracy of 83%. Our machine\nlearning-based detection mechanism contributes to understanding the Ethereum\necosystem dynamics by providing additional insights into the current bot\nlandscape.\n","authors":["Thomas Niedermayer","Pietro Saggese","Bernhard Haslhofer"],"pdf_url":"https://arxiv.org/pdf/2403.19530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14832v2","updated":"2025-01-02T11:24:40Z","published":"2024-12-19T13:20:32Z","title":"Federated Heavy Hitter Analytics with Local Differential Privacy","summary":"  Federated heavy hitter analytics enables service providers to better\nunderstand the preferences of cross-party users by analyzing the most frequent\nitems. As with federated learning, it faces challenges of privacy concerns,\nstatistical heterogeneity, and expensive communication. Local differential\nprivacy (LDP), as the de facto standard for privacy-preserving data collection,\nsolves the privacy challenge by letting each user perturb her data locally and\nreport the sanitized version. However, in federated settings, applying LDP\ncomplicates the other two challenges, due to the deteriorated utility by the\ninjected LDP noise or increasing communication/computation costs by\nperturbation mechanism. To tackle these problems, we propose a novel\ntarget-aligning prefix tree mechanism satisfying $\\epsilon$-LDP, for federated\nheavy hitter analytics. In particular, we propose an adaptive extension\nstrategy to address the inconsistencies between covering necessary prefixes and\nestimating heavy hitters within a party to enhance the utility. We also present\na consensus-based pruning strategy that utilizes noisy prior knowledge from\nother parties to further align the inconsistency between finding heavy hitters\nin each party and providing reasonable frequency information to identify the\nglobal ones. To the best of our knowledge, our study is the first solution to\nthe federated heavy hitter analytics in a cross-party setting while satisfying\nthe stringent $\\epsilon$-LDP. Comprehensive experiments on both real-world and\nsynthetic datasets confirm the effectiveness of our proposed mechanism.\n","authors":["Yuemin Zhang","Qingqing Ye","Haibo Hu"],"pdf_url":"https://arxiv.org/pdf/2412.14832v2.pdf","comment":"Accepted by SIGMOD 2025"},{"id":"http://arxiv.org/abs/2411.02974v3","updated":"2025-01-02T02:37:12Z","published":"2024-11-05T10:21:21Z","title":"Region-Guided Attack on the Segment Anything Model (SAM)","summary":"  The Segment Anything Model (SAM) is a cornerstone of image segmentation,\ndemonstrating exceptional performance across various applications, particularly\nin autonomous driving and medical imaging, where precise segmentation is\ncrucial. However, SAM is vulnerable to adversarial attacks that can\nsignificantly impair its functionality through minor input perturbations.\nTraditional techniques, such as FGSM and PGD, are often ineffective in\nsegmentation tasks due to their reliance on global perturbations that overlook\nspatial nuances. Recent methods like Attack-SAM-K and UAD have begun to address\nthese challenges, but they frequently depend on external cues and do not fully\nleverage the structural interdependencies within segmentation processes. This\nlimitation underscores the need for a novel adversarial strategy that exploits\nthe unique characteristics of segmentation tasks. In response, we introduce the\nRegion-Guided Attack (RGA), designed specifically for SAM. RGA utilizes a\nRegion-Guided Map (RGM) to manipulate segmented regions, enabling targeted\nperturbations that fragment large segments and expand smaller ones, resulting\nin erroneous outputs from SAM. Our experiments demonstrate that RGA achieves\nhigh success rates in both white-box and black-box scenarios, emphasizing the\nneed for robust defenses against such sophisticated attacks. RGA not only\nreveals SAM's vulnerabilities but also lays the groundwork for developing more\nresilient defenses against adversarial threats in image segmentation.\n","authors":["Xiaoliang Liu","Furao Shen","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.02974v3.pdf","comment":null}],"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2412.20151v2","updated":"2025-01-02T03:59:50Z","published":"2024-12-28T13:32:36Z","title":"Contention-Aware Microservice Deployment in Collaborative Mobile Edge\n  Networks","summary":"  As an emerging computing paradigm, mobile edge computing (MEC) provides\nprocessing capabilities at the network edge, aiming to reduce latency and\nimprove user experience. Meanwhile, the advancement of containerization\ntechnology facilitates the deployment of microservice-based applications via\nedge node collaboration, ensuring highly efficient service delivery. However,\nexisting research overlooks the resource contention among microservices in MEC.\nThis neglect potentially results in inadequate resources for microservices\nconstituting latency-sensitive applications, leading to increased response time\nand ultimately compromising quality of service (QoS). To solve this problem, we\npropose the Contention-Aware Multi-Application Microservice Deployment (CAMD)\nalgorithm for collaborative MEC, balancing rapid response for applications with\nlow-latency requirements and overall processing efficiency. The CAMD algorithm\ndecomposes the overall deployment problem into manageable sub-problems, each\nfocusing on a single microservice, then employs a heuristic approach to\noptimize these sub-problems, and ultimately arrives at an optimized deployment\nscheme through an iterative process. Finally, the superiority of the proposed\nalgorithm is evidenced through intensive experiments and comparison with\nbaseline algorithms.\n","authors":["Xinlei Ge","Yang Li","Xing Zhang","Yukun Sun","Yunji Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.20151v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19442v2","updated":"2025-01-02T03:40:15Z","published":"2024-12-27T04:17:57Z","title":"A Survey on Large Language Model Acceleration based on KV Cache\n  Management","summary":"  Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.\n","authors":["Haoyang Li","Yiming Li","Anxin Tian","Tianhao Tang","Zhanchao Xu","Xuejia Chen","Nicole Hu","Wei Dong","Qing Li","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2412.19442v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16888v2","updated":"2025-01-02T06:55:38Z","published":"2024-12-22T06:51:33Z","title":"Rethinking Performance Analysis for Configurable Software Systems: A\n  Case Study from a Fitness Landscape Perspective","summary":"  Modern software systems are often highly configurable to tailor varied\nrequirements from diverse stakeholders. Understanding the mapping between\nconfigurations and the desired performance attributes plays a fundamental role\nin advancing the controllability and tuning of the underlying system, yet has\nlong been a dark hole of knowledge due to its black-box nature. While there\nhave been previous efforts in performance analysis for these systems, they\nanalyze the configurations as isolated data points without considering their\ninherent spatial relationships. This renders them incapable of interrogating\nmany important aspects of the configuration space like local optima. In this\nwork, we advocate a novel perspective to rethink performance analysis --\nmodeling the configuration space as a structured ``landscape''. To support this\nproposition, we designed \\our, an open-source, graph data mining empowered\nfitness landscape analysis (FLA) framework. By applying this framework to $86$M\nbenchmarked configurations from $32$ running workloads of $3$ real-world\nsystems, we arrived at $6$ main findings, which together constitute a holistic\npicture of the landscape topography, with thorough discussions about their\nimplications on both configuration tuning and performance modeling.\n","authors":["Mingyu Huang","Peili Mao","Ke Li"],"pdf_url":"https://arxiv.org/pdf/2412.16888v2.pdf","comment":"23 pages, 8 figures, accepted as a conference paper at ISSTA 2025"}],"Information Theory":[{"id":"http://arxiv.org/abs/2409.16293v2","updated":"2025-01-02T18:01:44Z","published":"2024-09-08T23:14:11Z","title":"Optimization of Excitation Waveforms for Maximum Instantaneous Field\n  Intensity","summary":"  This paper introduces a computational approach to identify performance\nconstraints in the time-domain based on optimizing the excitation waveform. The\nmethod builds on an optimization algorithm that has been employed for decades\nto establish fundamental limits in the frequency domain and this paper\nshowcases its first comprehensive application to time-domain pulses. The method\nis applied to arbitrarily polarized multiport antennas and arrays. The\ndemonstration performed is based on finding an antenna's maximum peak radiation\nintensity in a given direction and time with limited total input energy\navailable. To highlight the generality of the approach, an analysis on finding\noptimal illumination for antiferromagnetic memory switching is conducted.\n","authors":["Jakub Liska","Lukas Jelinek","Miloslav Capek"],"pdf_url":"https://arxiv.org/pdf/2409.16293v2.pdf","comment":"12 pages, 16 figures, IEEE TAP paper"},{"id":"http://arxiv.org/abs/2411.01446v2","updated":"2025-01-02T08:23:35Z","published":"2024-11-03T06:07:31Z","title":"Protocol Design for Irregular Repetition Slotted ALOHA With Energy\n  Harvesting to Maintain Information Freshness","summary":"  We investigate an internet-of-things system where energy-harvesting devices\nsend status updates to a common receiver using the irregular repetition slotted\nALOHA (IRSA) protocol. Energy shortages in these devices may lead to\ntransmission failures that are unknown to the receiver, disrupting the decoding\nprocess. To address this issue, we propose a method for the receiver to\nperfectly identify such failures. Furthermore, we optimize the degree\ndistribution of the protocol to enhance the freshness of the status updates.\nOur optimized degree distribution mitigates the adverse effects of potential\ntransmission failures. Numerical results demonstrate that, despite\nenergy-harvesting constraints, IRSA can achieve a level of information\nfreshness comparable to systems with unlimited energy.\n","authors":["Khac-Hoang Ngo","Diep N. Nguyen","Thai-Mai Dinh Thi"],"pdf_url":"https://arxiv.org/pdf/2411.01446v2.pdf","comment":"accepted to IEEE WCNC 2025"}],"Signal Processing":[{"id":"http://arxiv.org/abs/2409.13833v2","updated":"2025-01-02T15:24:28Z","published":"2024-09-20T18:19:47Z","title":"Transfer Learning and Double U-Net Empowered Wave Propagation Model in\n  Complex Indoor Environment","summary":"  A Machine Learning (ML) network based on transfer learning and transformer\nnetworks is applied to wave propagation models for complex indoor settings.\nThis network is designed to predict signal propagation in environments with a\nvariety of objects, effectively simulating the diverse range of furniture\ntypically found in indoor spaces. We propose Attention U-Net with Efficient\nNetworks as the backbone, to process images encoded with the essential\ninformation of the indoor environment. The indoor environment is defined by its\nfundamental structure, such as the arrangement of walls, windows, and doorways,\nalongside varying configurations of furniture placement. An innovative\nalgorithm is introduced to generate a 3D environment from a 2D floorplan, which\nis crucial for efficient collection of data for training. The model is\nevaluated by comparing the predicted signal coverage map with ray tracing (RT)\nsimulations. The prediction results show a root mean square error of less than\n6 dB across all tested scenarios, with significant improvements observed when\nusing a Double U-Net structure compared to a single U-Net model.\n","authors":["Ziheng Fu","Swagato Mukherjee","Michael T. Lanagan","Prasenjit Mitra","Tarun Chawla","Ram M. Narayanan"],"pdf_url":"https://arxiv.org/pdf/2409.13833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07259v2","updated":"2025-01-02T14:57:03Z","published":"2024-02-11T18:04:06Z","title":"RIS-Augmented Millimeter-Wave MIMO Systems for Passive Drone Detection","summary":"  In the past decade, the number of amateur drones is increasing, and this\ntrend is expected to continue in the future. The security issues brought by\nabuse and misconduct of drones become more and more severe and may incur a\nnegative impact to the society. In this paper, we leverage existing cellular\nmultiple-input multiple-output (MIMO) base station (BS) infrastructure,\noperating at millimeter wave (mmWave) frequency bands, for drone detection in a\ndevice-free manner with the aid of one reconfigurable intelligent surface\n(RIS), deployed in the proximity of the BS. We theoretically examine the\nfeasibility of drone detection with the aid of the generalized likelihood ratio\ntest (GLRT) and validate via simulations that, the optimized deployment of an\nRIS can bring added benefits compared to RIS-free systems. In addition, the\neffect of RIS training beams, training overhead, and radar cross section, is\ninvestigated in order to offer theoretical design guidance for the proposed\ncellular RIS-based passive drone detection system.\n","authors":["Jiguang He","Aymen Fakhreddine","George C. Alexandropoulos"],"pdf_url":"https://arxiv.org/pdf/2402.07259v2.pdf","comment":"6 pages, 6 figures, accepted by IEEE PIMRC 2024"},{"id":"http://arxiv.org/abs/2404.14879v2","updated":"2025-01-02T14:52:24Z","published":"2024-04-23T10:06:32Z","title":"Device-Free 3D Drone Localization in RIS-Assisted mmWave MIMO Networks","summary":"  In this paper, we investigate the potential of reconfigurable intelligent\nsurfaces (RISs) in facilitating passive/device-free three-dimensional (3D)\ndrone localization within existing cellular infrastructure operating at\nmillimeter-wave (mmWave) frequencies and employing multiple antennas at the\ntransceivers. The developed localization system operates in the bi-static mode\nwithout requiring direct communication between the drone and the base station.\nWe analyze the theoretical performance limits via Fisher information analysis\nand Cram\\'er Rao lower bounds (CRLBs). Furthermore, we develop a low-complexity\nyet effective drone localization algorithm based on coordinate gradient descent\nand examine the impact of factors such as radar cross section (RCS) of the\ndrone and training overhead on system performance. It is demonstrated that\nintegrating RIS yields significant benefits over its RIS-free counterpart, as\nevidenced by both theoretical analyses and numerical simulations.\n","authors":["Jiguang He","Charles Vanwynsberghe","Hui Chen","Chongwen Huang","Aymen Fakhreddine"],"pdf_url":"https://arxiv.org/pdf/2404.14879v2.pdf","comment":"6 pages, 5 figures, accepted by IEEE GLOBECOM 2024"}]},"2025-01-01T00:00:00Z":{"Distributed, Parallel, and Cluster Computing":[{"id":"http://arxiv.org/abs/2412.19706v2","updated":"2025-01-01T11:29:49Z","published":"2024-12-27T16:00:24Z","title":"Geometric Freeze-Tag Problem","summary":"  We study the Freeze-Tag Problem (FTP), introduced by Arkin et al. (SODA'02),\nwhere the objective is to activate a group of n robots, starting from a single\ninitially active robot. Robots are positioned in $\\mathbb{R}^d$, and once\nactivated, they move at a constant speed to wake up others. The goal is to\nminimize the time required to activate the last robot, known as the makespan.\nWe establish new upper bounds for the makespan under the $l_1$ and $l_2$ norms\nin $\\mathbb{R}^2$ and $\\mathbb{R}^3$. Specifically, we improve the previous\nupper bound for $(\\mathbb{R}^2, l_2)$ from $7.07r$ (Bonichon et al., DISC'24)\nto $5.064r$. For $(\\mathbb{R}^3, l_1)$, we derive a makespan bound of $13r$,\nwhich translates to $22.52r$ for $(\\mathbb{R}^3, l_2)$. Here, $r$ denotes the\nmaximum distance of any robot from the initially active robot under the given\nnorm. To our knowledge, these are the first makespan bounds for FTP in\n$\\mathbb{R}^3$. Additionally, we show that the maximum makespan for $n$ robots\nis not necessarily achieved when robots are equally distributed along the\nboundary in $(\\mathbb{R}^2, l_2)$. We further investigate FTP in\n$(\\mathbb{R}^3, l_2)$ for specific configurations where robots lie on a\nboundary, providing insights into practical scenarios.\n","authors":["Sharareh Alipour","Kajal Baghestani","Mahdis Mirzaei","Soroush Sahraei"],"pdf_url":"https://arxiv.org/pdf/2412.19706v2.pdf","comment":null}],"Cryptography and Security":[{"id":"http://arxiv.org/abs/2407.17619v2","updated":"2025-01-01T20:31:16Z","published":"2024-07-24T20:15:07Z","title":"The Power of Graph Sparsification in the Continual Release Model","summary":"  The graph continual release model of differential privacy seeks to produce\ndifferentially private solutions to graph problems under a stream of edge\nupdates where new private solutions are released after each update. Thus far,\npreviously known edge-differentially private algorithms for most graph problems\nincluding densest subgraph and matchings in the continual release setting only\noutput real-value estimates (not vertex subset solutions) and do not use\nsublinear space. Instead, they rely on computing exact graph statistics on the\ninput [FHO21,SLMVC18]. In this paper, we leverage sparsification to address the\nabove shortcomings for edge-insertion streams. Our edge-differentially private\nalgorithms use sublinear space with respect to the number of edges in the graph\nwhile some also achieve sublinear space in the number of vertices in the graph.\nIn addition, for the densest subgraph problem, we also output\nedge-differentially private vertex subset solutions; no previous graph\nalgorithms in the continual release model output such subsets.\n  We make novel use of assorted sparsification techniques from the non-private\nstreaming and static graph algorithms literature to achieve new results in the\nsublinear space, continual release setting. This includes algorithms for\ndensest subgraph, maximum matching, as well as the first continual release\n$k$-core decomposition algorithm. We conclude with polynomial additive error\nlower bounds for edge-privacy in the fully dynamic setting.\n","authors":["Alessandro Epasto","Quanquan C. Liu","Tamalika Mukherjee","Felix Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.17619v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16277v7","updated":"2025-01-01T18:50:20Z","published":"2024-01-29T16:32:36Z","title":"SECOMP: Formally Secure Compilation of Compartmentalized C Programs","summary":"  Undefined behavior in C often causes devastating security vulnerabilities.\nOne practical mitigation is compartmentalization, which allows developers to\nstructure large programs into mutually distrustful compartments with clearly\nspecified privileges and interactions. In this paper we introduce SECOMP, a\ncompiler for compartmentalized C code that comes with machine-checked proofs\nguaranteeing that the scope of undefined behavior is restricted to the\ncompartments that encounter it and become dynamically compromised. These\nguarantees are formalized as the preservation of safety properties against\nadversarial contexts, a secure compilation criterion similar to full\nabstraction, and this is the first time such a strong criterion is proven for a\nmainstream programming language. To achieve this we extend the languages of the\nCompCert verified C compiler with isolated compartments that can only interact\nvia procedure calls and returns, as specified by cross-compartment interfaces.\nWe adapt the passes and optimizations of CompCert as well as their correctness\nproofs to this compartment-aware setting. We then use compiler correctness as\nan ingredient in a larger secure compilation proof that involves several proof\nengineering novelties, needed to scale formally secure compilation up to a C\ncompiler.\n","authors":["J√©r√©my Thibault","Roberto Blanco","Dongjae Lee","Sven Argo","Arthur Azevedo de Amorim","A√Øna Linn Georges","Catalin Hritcu","Andrew Tolmach"],"pdf_url":"https://arxiv.org/pdf/2401.16277v7.pdf","comment":"CCS'24 version, slightly updated and extended with appendices and a\n  few more references"},{"id":"http://arxiv.org/abs/2404.00062v4","updated":"2025-01-01T10:52:04Z","published":"2024-03-27T10:00:35Z","title":"Modelling the Impact of Quantum Circuit Imperfections on Networks and\n  Computer Applications","summary":"  Post Quantum and Quantum Cryptography schemes are feasible quantum computer\napplications for 7G networks. These schemes could possibly replace existing\nschemes. These algorithms have been compromised by advances in quantum search\nalgorithms run on quantum computers like Shor algorithm. Shor algorithm is a\nquantum algorithm for finding the prime factors of an integer which is the\nbasis of existing algorithm. This has become an available quantum computer\napplication putting the use of ESA algorithm at risk. Our recent paper provides\na detailed survey of the work on post quantum and quantum cryptography\nalgorithms with focus on their applicability in 7G networks.\n  Since the paper focuses on the cryptography algorithms as a follow up, in\nthis paper, we provide a new framework for quantum network optimization and\nsurvey in detail the work on enabling technologies (quantum hardware) for the\npractical implementation of these algorithms including the most important\nsegments of quantum hardware in 7G. As always in engineering practice practical\nsolutions are a compromise between the performance and complexity of the\nimplementation. For this reason, as the main contribution, the paper presents a\nnetwork and computer applications optimization framework that includes\nimplementation imperfections. The tools should be useful in optimizing future\ngeneration practical computer system design. After that a comprehensive survey\nof the existing work on quantum hardware is presented pointing out the sources\nof these imperfections. This enables us to make a fair assessment of how much\ninvestment into quantum hardware improvements contributes to the performance\nenhancement of the overall system. In this way a decision can be made on proper\npartitioning between the investment in hardware and system level complexity.\n","authors":["Savo Glisic"],"pdf_url":"https://arxiv.org/pdf/2404.00062v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12842v2","updated":"2025-01-01T06:08:54Z","published":"2024-06-18T17:57:46Z","title":"A Characterization of Semi-Involutory MDS Matrices","summary":"  In symmetric cryptography, maximum distance separable (MDS) matrices with\ncomputationally simple inverses have wide applications. Many block ciphers like\nAES, SQUARE, SHARK, and hash functions like PHOTON use an MDS matrix in the\ndiffusion layer. In this article, we first characterize all $3 \\times 3$\nirreducible semi-involutory matrices over the finite field of characteristic\n$2$. Using this matrix characterization, we provide a necessary and sufficient\ncondition to construct MDS semi-involutory matrices using only their diagonal\nentries and the entries of an associated diagonal matrix. Finally, we count the\nnumber of $3 \\times 3$ semi-involutory MDS matrices over any finite field of\ncharacteristic $2$.\n","authors":["Tapas Chatterjee","Ayantika Laha"],"pdf_url":"https://arxiv.org/pdf/2406.12842v2.pdf","comment":"19 pages"}],"Information Theory":[{"id":"http://arxiv.org/abs/2309.13691v7","updated":"2025-01-01T19:37:20Z","published":"2023-09-24T16:46:47Z","title":"On Simultaneous Information and Energy Transmission through Quantum\n  Channels","summary":"  The optimal rate at which information can be sent through a quantum channel\nwhen the transmitted signal must simultaneously carry some minimum amount of\nenergy is characterized. To do so, we introduce the quantum-classical analogue\nof the capacity-power function and generalize results in classical information\ntheory for transmitting classical information through noisy channels. We show\nthat the capacity-power function for a classical-quantum channel, for both\nunassisted and private protocol, is concave and also prove additivity for\nunentangled and uncorrelated ensembles of input signals for such channels. This\nimplies we do not need regularized formulas for calculation. We show these\nproperties also hold for all noiseless channels when we restrict the set of\ninput states to be pure quantum states. For general channels, we find that the\ncapacity-power function is piece-wise concave. We give an elegant visual proof\nfor this supported by numerical simulations. We connect channel capacity and\nproperties of random quantum states. In particular, we obtain analytical\nexpressions for the capacity-power function for the case of noiseless channels\nusing properties of random quantum states under an energy constraint and\nconcentration phenomena in large Hilbert spaces.\n","authors":["Bishal Kumar Das","Lav R. Varshney","Vaibhav Madhok"],"pdf_url":"https://arxiv.org/pdf/2309.13691v7.pdf","comment":"16 pages, 18 figures"}],"Signal Processing":[{"id":"http://arxiv.org/abs/2406.18306v5","updated":"2025-01-01T13:17:53Z","published":"2024-06-26T12:45:48Z","title":"Neural Network-Based IRS Assisted NLoS DoA Estimation","summary":"  Direction-of-Arrival (DoA) estimation in challenging Non-Line-of-Sight (NLoS)\nenvironments is crucial for various wireless applications. This paper presents\na novel neural network architecture to address this challenge using an\nIntelligent Reflecting Surface (IRS). The key innovation is the introduction of\na dedicated, learnable Neural Network (NN)-based IRS layer integrated within a\ncarefully designed network structure. Unlike conventional neural network\nlayers, this specialized layer incorporates sinusoidal weight constraints,\nwhere the phase arguments of these sinusoids are learned during training to\ndirectly emulate the phase shifts of the IRS elements. This allows the network\nto autonomously optimize the IRS configuration for enhanced DoA estimation,\neliminating the need for separate IRS control algorithms. We adapt standard\nbackpropagation to accommodate these constraints, ensuring effective training.\nNumerical simulations, conducted under various conditions and noise levels,\ndemonstrate the superior performance of the proposed approach compared to\ntraditional methods, highlighting its potential for significantly improved DoA\nestimation accuracy in complex IRS-assisted wireless systems.\n","authors":["Yasin Azhdari","Mahmoud Farhang"],"pdf_url":"https://arxiv.org/pdf/2406.18306v5.pdf","comment":null}]}}